{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "DCoxpqkc7c4A"
   },
   "outputs": [],
   "source": [
    "#Importing libraries\n",
    "import yfinance as yf\n",
    "import talib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn\n",
    "import xgboost\n",
    "import dateutil\n",
    "import scipy\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from scipy.stats import linregress\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.max_rows\", 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cUNvVYH7c4B"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "q4U36sNk7c4C"
   },
   "outputs": [],
   "source": [
    "# I chose Intel asset to predict open and close\n",
    "focus_asset=[\"INTC\"]\n",
    "# And then I added sector assets data \n",
    "peer_data_assets=[\"AMD\", \"NVDA\", \"QCOM\",\"TXN\",\"MU\",\"AVGO\",\"SSNLF\",\"TSM\",\"MSFT\",\"AAPL\",\"NVDA\",\"AVGO\",\"ORCL\",\"ADBE\",\n",
    "                \"QCOM\",\"CSCO\",\"AVGO\",\"NXPI\",\"MU\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4uyp8V2K7c4D"
   },
   "outputs": [],
   "source": [
    "# Then indexes\n",
    "indexes_=[\"^GSPC\",\n",
    "\"^DJI\",\n",
    "\"^IXIC\",\n",
    "\"^NYA\",\n",
    "\"^XAX\",\n",
    "\"^BUK100P\",\n",
    "\"^RUT\",\n",
    "\"^VIX\",\n",
    "\"^FTSE\",\n",
    "\"^GDAXI\",\n",
    "\"^FCHI\",\n",
    "\"^STOXX50E\",\n",
    "\"^N100\",\n",
    "\"^BFX\",\n",
    "\"IMOEX.ME\",\n",
    "\"^N225\",\n",
    "\"^HSI\",\n",
    "\"000001.SS\",\n",
    "\"399001.SZ\",\n",
    "\"^STI\",\n",
    "\"^AXJO\",\n",
    "\"^AORD\",\n",
    "\"^BSESN\",\n",
    "\"^JKSE\",\n",
    "\"^KLSE\",\n",
    "\"^NZ50\",\n",
    "\"^KS11\",\n",
    "\"^TWII\",\n",
    "\"^GSPTSE\",\n",
    "\"^BVSP\",\n",
    "\"^MXX\",\n",
    "\"^MERV\",\n",
    "\"^TA125.TA\",\n",
    "\"^JN0U.JO\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "ldm5Ag4V7c4E"
   },
   "outputs": [],
   "source": [
    "for e in indexes_:\n",
    "    peer_data_assets.append(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "y4krgWpD7c4F",
    "outputId": "0f9a3a03-1a49-4f28-e673-e39840de27c0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 604 rows of data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "START_DATE = '2022-01-01'\n",
    "END_DATE = '2024-05-30' \n",
    "n_assets = len(focus_asset)\n",
    "\n",
    "intel = yf.download(focus_asset, start=START_DATE,\n",
    "                        end=END_DATE,interval=\"1d\")\n",
    "print(f'Downloaded {intel.shape[0]} rows of data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r_nm7iIM7c4G",
    "outputId": "ce6968d2-a113-448c-e4cd-26465a6138ad"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  48 of 48 completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 736 rows of data.\n"
     ]
    }
   ],
   "source": [
    "n_assets = len(peer_data_assets)\n",
    "\n",
    "peers = yf.download(peer_data_assets, start=START_DATE,\n",
    "                        end=END_DATE,interval=\"1d\")['Adj Close']\n",
    "print(f'Downloaded {peers.shape[0]} rows of data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### I calculated returns of peers and focus assets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "amsvSYat7c4G"
   },
   "outputs": [],
   "source": [
    "for i in peers.columns:\n",
    "    peers[\"Return_\"+i]=peers[i].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 648
    },
    "id": "O_NJqgxm7c4G",
    "outputId": "86d6c189-81fd-4748-ef20-c21ee3c388cc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Ticker</th>\n",
       "      <th>000001.SS</th>\n",
       "      <th>399001.SZ</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AVGO</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>IMOEX.ME</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>MU</th>\n",
       "      <th>NVDA</th>\n",
       "      <th>NXPI</th>\n",
       "      <th>ORCL</th>\n",
       "      <th>QCOM</th>\n",
       "      <th>SSNLF</th>\n",
       "      <th>TSM</th>\n",
       "      <th>TXN</th>\n",
       "      <th>^AORD</th>\n",
       "      <th>^AXJO</th>\n",
       "      <th>^BFX</th>\n",
       "      <th>^BSESN</th>\n",
       "      <th>^BUK100P</th>\n",
       "      <th>^BVSP</th>\n",
       "      <th>^DJI</th>\n",
       "      <th>^FCHI</th>\n",
       "      <th>^FTSE</th>\n",
       "      <th>^GDAXI</th>\n",
       "      <th>^GSPC</th>\n",
       "      <th>^GSPTSE</th>\n",
       "      <th>^HSI</th>\n",
       "      <th>^IXIC</th>\n",
       "      <th>^JKSE</th>\n",
       "      <th>^JN0U.JO</th>\n",
       "      <th>^KLSE</th>\n",
       "      <th>^KS11</th>\n",
       "      <th>^MERV</th>\n",
       "      <th>^MXX</th>\n",
       "      <th>^N100</th>\n",
       "      <th>^N225</th>\n",
       "      <th>^NYA</th>\n",
       "      <th>^NZ50</th>\n",
       "      <th>^RUT</th>\n",
       "      <th>^STI</th>\n",
       "      <th>^STOXX50E</th>\n",
       "      <th>^TA125.TA</th>\n",
       "      <th>^TWII</th>\n",
       "      <th>^VIX</th>\n",
       "      <th>^XAX</th>\n",
       "      <th>Return_000001.SS</th>\n",
       "      <th>Return_399001.SZ</th>\n",
       "      <th>Return_AAPL</th>\n",
       "      <th>Return_ADBE</th>\n",
       "      <th>Return_AMD</th>\n",
       "      <th>Return_AVGO</th>\n",
       "      <th>Return_CSCO</th>\n",
       "      <th>Return_IMOEX.ME</th>\n",
       "      <th>Return_MSFT</th>\n",
       "      <th>Return_MU</th>\n",
       "      <th>Return_NVDA</th>\n",
       "      <th>Return_NXPI</th>\n",
       "      <th>Return_ORCL</th>\n",
       "      <th>Return_QCOM</th>\n",
       "      <th>Return_SSNLF</th>\n",
       "      <th>Return_TSM</th>\n",
       "      <th>Return_TXN</th>\n",
       "      <th>Return_^AORD</th>\n",
       "      <th>Return_^AXJO</th>\n",
       "      <th>Return_^BFX</th>\n",
       "      <th>Return_^BSESN</th>\n",
       "      <th>Return_^BUK100P</th>\n",
       "      <th>Return_^BVSP</th>\n",
       "      <th>Return_^DJI</th>\n",
       "      <th>Return_^FCHI</th>\n",
       "      <th>Return_^FTSE</th>\n",
       "      <th>Return_^GDAXI</th>\n",
       "      <th>Return_^GSPC</th>\n",
       "      <th>Return_^GSPTSE</th>\n",
       "      <th>Return_^HSI</th>\n",
       "      <th>Return_^IXIC</th>\n",
       "      <th>Return_^JKSE</th>\n",
       "      <th>Return_^JN0U.JO</th>\n",
       "      <th>Return_^KLSE</th>\n",
       "      <th>Return_^KS11</th>\n",
       "      <th>Return_^MERV</th>\n",
       "      <th>Return_^MXX</th>\n",
       "      <th>Return_^N100</th>\n",
       "      <th>Return_^N225</th>\n",
       "      <th>Return_^NYA</th>\n",
       "      <th>Return_^NZ50</th>\n",
       "      <th>Return_^RUT</th>\n",
       "      <th>Return_^STI</th>\n",
       "      <th>Return_^STOXX50E</th>\n",
       "      <th>Return_^TA125.TA</th>\n",
       "      <th>Return_^TWII</th>\n",
       "      <th>Return_^VIX</th>\n",
       "      <th>Return_^XAX</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-02</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2073.090088</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179.273621</td>\n",
       "      <td>564.369995</td>\n",
       "      <td>150.240005</td>\n",
       "      <td>62.370541</td>\n",
       "      <td>57.983242</td>\n",
       "      <td>3852.500000</td>\n",
       "      <td>326.940796</td>\n",
       "      <td>94.188248</td>\n",
       "      <td>30.073059</td>\n",
       "      <td>219.249969</td>\n",
       "      <td>84.268791</td>\n",
       "      <td>175.001373</td>\n",
       "      <td>2.395472e+10</td>\n",
       "      <td>122.335548</td>\n",
       "      <td>175.891220</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4335.279785</td>\n",
       "      <td>59183.218750</td>\n",
       "      <td>732.270020</td>\n",
       "      <td>103922.0</td>\n",
       "      <td>36585.058594</td>\n",
       "      <td>7217.220215</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16020.730469</td>\n",
       "      <td>4796.560059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23274.750000</td>\n",
       "      <td>15832.799805</td>\n",
       "      <td>6665.308105</td>\n",
       "      <td>4532.049805</td>\n",
       "      <td>1549.050049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>85579.0</td>\n",
       "      <td>52941.011719</td>\n",
       "      <td>1372.060059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17226.099609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2272.560059</td>\n",
       "      <td>3134.250000</td>\n",
       "      <td>4331.819824</td>\n",
       "      <td>2086.850098</td>\n",
       "      <td>18270.509766</td>\n",
       "      <td>16.600000</td>\n",
       "      <td>3461.719971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>3632.330078</td>\n",
       "      <td>14791.309570</td>\n",
       "      <td>176.998352</td>\n",
       "      <td>554.000000</td>\n",
       "      <td>144.419998</td>\n",
       "      <td>63.085148</td>\n",
       "      <td>56.561138</td>\n",
       "      <td>3873.489990</td>\n",
       "      <td>321.334717</td>\n",
       "      <td>94.768616</td>\n",
       "      <td>29.243383</td>\n",
       "      <td>220.814590</td>\n",
       "      <td>85.169952</td>\n",
       "      <td>175.959976</td>\n",
       "      <td>2.395472e+10</td>\n",
       "      <td>126.704697</td>\n",
       "      <td>176.075790</td>\n",
       "      <td>7926.799805</td>\n",
       "      <td>7589.799805</td>\n",
       "      <td>4362.209961</td>\n",
       "      <td>59855.929688</td>\n",
       "      <td>743.950012</td>\n",
       "      <td>103514.0</td>\n",
       "      <td>36799.648438</td>\n",
       "      <td>7317.410156</td>\n",
       "      <td>7505.200195</td>\n",
       "      <td>16152.610352</td>\n",
       "      <td>4793.540039</td>\n",
       "      <td>21236.500000</td>\n",
       "      <td>23289.839844</td>\n",
       "      <td>15622.719727</td>\n",
       "      <td>6695.373047</td>\n",
       "      <td>4603.919922</td>\n",
       "      <td>1541.900024</td>\n",
       "      <td>2989.239990</td>\n",
       "      <td>85286.0</td>\n",
       "      <td>53016.781250</td>\n",
       "      <td>1383.489990</td>\n",
       "      <td>29301.789062</td>\n",
       "      <td>17336.759766</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2268.870117</td>\n",
       "      <td>3181.129883</td>\n",
       "      <td>4367.620117</td>\n",
       "      <td>2088.530029</td>\n",
       "      <td>18526.349609</td>\n",
       "      <td>16.910000</td>\n",
       "      <td>3496.050049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.012692</td>\n",
       "      <td>-0.018374</td>\n",
       "      <td>-0.038738</td>\n",
       "      <td>0.011457</td>\n",
       "      <td>-0.024526</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>-0.017147</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>0.015950</td>\n",
       "      <td>-0.003926</td>\n",
       "      <td>0.005866</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008232</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>-0.013269</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.015858</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003424</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.008330</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001624</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.014003</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>0.009917</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>3595.179932</td>\n",
       "      <td>14525.759766</td>\n",
       "      <td>172.290207</td>\n",
       "      <td>514.429993</td>\n",
       "      <td>136.149994</td>\n",
       "      <td>60.459896</td>\n",
       "      <td>55.665394</td>\n",
       "      <td>3815.050049</td>\n",
       "      <td>308.999420</td>\n",
       "      <td>92.860268</td>\n",
       "      <td>27.560068</td>\n",
       "      <td>212.944275</td>\n",
       "      <td>82.888268</td>\n",
       "      <td>175.273880</td>\n",
       "      <td>2.395472e+10</td>\n",
       "      <td>120.682884</td>\n",
       "      <td>172.402908</td>\n",
       "      <td>7899.600098</td>\n",
       "      <td>7565.799805</td>\n",
       "      <td>4344.620117</td>\n",
       "      <td>60223.148438</td>\n",
       "      <td>746.140015</td>\n",
       "      <td>101006.0</td>\n",
       "      <td>36407.109375</td>\n",
       "      <td>7376.370117</td>\n",
       "      <td>7516.899902</td>\n",
       "      <td>16271.750000</td>\n",
       "      <td>4700.580078</td>\n",
       "      <td>21039.699219</td>\n",
       "      <td>22907.250000</td>\n",
       "      <td>15100.169922</td>\n",
       "      <td>6662.298828</td>\n",
       "      <td>4673.850098</td>\n",
       "      <td>1547.949951</td>\n",
       "      <td>2953.969971</td>\n",
       "      <td>83836.0</td>\n",
       "      <td>53024.148438</td>\n",
       "      <td>1388.089966</td>\n",
       "      <td>29332.160156</td>\n",
       "      <td>17112.599609</td>\n",
       "      <td>13150.379883</td>\n",
       "      <td>2194.000000</td>\n",
       "      <td>3163.439941</td>\n",
       "      <td>4392.149902</td>\n",
       "      <td>2105.479980</td>\n",
       "      <td>18499.960938</td>\n",
       "      <td>19.730000</td>\n",
       "      <td>3469.100098</td>\n",
       "      <td>-0.010228</td>\n",
       "      <td>-0.017953</td>\n",
       "      <td>-0.026600</td>\n",
       "      <td>-0.071426</td>\n",
       "      <td>-0.057264</td>\n",
       "      <td>-0.041614</td>\n",
       "      <td>-0.015837</td>\n",
       "      <td>-0.015087</td>\n",
       "      <td>-0.038388</td>\n",
       "      <td>-0.020137</td>\n",
       "      <td>-0.057562</td>\n",
       "      <td>-0.035642</td>\n",
       "      <td>-0.026790</td>\n",
       "      <td>-0.003899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.047526</td>\n",
       "      <td>-0.020860</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>-0.003162</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>-0.024229</td>\n",
       "      <td>-0.010667</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.009267</td>\n",
       "      <td>-0.016427</td>\n",
       "      <td>-0.033448</td>\n",
       "      <td>-0.004940</td>\n",
       "      <td>0.015189</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>-0.017002</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.032999</td>\n",
       "      <td>-0.005561</td>\n",
       "      <td>0.005616</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>-0.001424</td>\n",
       "      <td>0.166765</td>\n",
       "      <td>-0.007709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>3586.080078</td>\n",
       "      <td>14429.509766</td>\n",
       "      <td>169.414108</td>\n",
       "      <td>514.119995</td>\n",
       "      <td>136.229996</td>\n",
       "      <td>59.898548</td>\n",
       "      <td>56.256390</td>\n",
       "      <td>3753.290039</td>\n",
       "      <td>306.557678</td>\n",
       "      <td>94.089890</td>\n",
       "      <td>28.133154</td>\n",
       "      <td>215.210541</td>\n",
       "      <td>83.080719</td>\n",
       "      <td>174.756989</td>\n",
       "      <td>2.395472e+10</td>\n",
       "      <td>122.022118</td>\n",
       "      <td>172.347549</td>\n",
       "      <td>7679.299805</td>\n",
       "      <td>7358.299805</td>\n",
       "      <td>4308.600098</td>\n",
       "      <td>59601.839844</td>\n",
       "      <td>738.669983</td>\n",
       "      <td>101561.0</td>\n",
       "      <td>36236.468750</td>\n",
       "      <td>7249.660156</td>\n",
       "      <td>7450.399902</td>\n",
       "      <td>16052.030273</td>\n",
       "      <td>4696.049805</td>\n",
       "      <td>21072.199219</td>\n",
       "      <td>23072.859375</td>\n",
       "      <td>15080.860352</td>\n",
       "      <td>6653.351074</td>\n",
       "      <td>4617.549805</td>\n",
       "      <td>1533.359985</td>\n",
       "      <td>2920.530029</td>\n",
       "      <td>83748.0</td>\n",
       "      <td>53055.308594</td>\n",
       "      <td>1365.170044</td>\n",
       "      <td>28487.869141</td>\n",
       "      <td>17156.519531</td>\n",
       "      <td>12983.009766</td>\n",
       "      <td>2206.370117</td>\n",
       "      <td>3184.300049</td>\n",
       "      <td>4324.810059</td>\n",
       "      <td>2088.489990</td>\n",
       "      <td>18367.919922</td>\n",
       "      <td>19.610001</td>\n",
       "      <td>3546.679932</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.006626</td>\n",
       "      <td>-0.016693</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.009285</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>-0.016189</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>-0.002949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>-0.000321</td>\n",
       "      <td>-0.027888</td>\n",
       "      <td>-0.027426</td>\n",
       "      <td>-0.008291</td>\n",
       "      <td>-0.010317</td>\n",
       "      <td>-0.010012</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>-0.004687</td>\n",
       "      <td>-0.017178</td>\n",
       "      <td>-0.008847</td>\n",
       "      <td>-0.013503</td>\n",
       "      <td>-0.000964</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>-0.001279</td>\n",
       "      <td>-0.001343</td>\n",
       "      <td>-0.012046</td>\n",
       "      <td>-0.009425</td>\n",
       "      <td>-0.011320</td>\n",
       "      <td>-0.001050</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.016512</td>\n",
       "      <td>-0.028784</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>-0.012727</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>0.006594</td>\n",
       "      <td>-0.015332</td>\n",
       "      <td>-0.008069</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>-0.006082</td>\n",
       "      <td>0.022363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-24</th>\n",
       "      <td>3088.871094</td>\n",
       "      <td>9424.580078</td>\n",
       "      <td>189.760345</td>\n",
       "      <td>475.429993</td>\n",
       "      <td>166.360001</td>\n",
       "      <td>140.338379</td>\n",
       "      <td>46.025688</td>\n",
       "      <td>3396.500000</td>\n",
       "      <td>429.386078</td>\n",
       "      <td>129.376846</td>\n",
       "      <td>106.451149</td>\n",
       "      <td>274.963867</td>\n",
       "      <td>122.563950</td>\n",
       "      <td>208.424332</td>\n",
       "      <td>-3.204001e+02</td>\n",
       "      <td>158.910294</td>\n",
       "      <td>197.891693</td>\n",
       "      <td>7999.200195</td>\n",
       "      <td>7727.600098</td>\n",
       "      <td>3969.919922</td>\n",
       "      <td>75410.390625</td>\n",
       "      <td>830.539978</td>\n",
       "      <td>124306.0</td>\n",
       "      <td>39069.589844</td>\n",
       "      <td>8094.970215</td>\n",
       "      <td>8317.599609</td>\n",
       "      <td>18693.369141</td>\n",
       "      <td>5304.720215</td>\n",
       "      <td>22320.900391</td>\n",
       "      <td>18608.939453</td>\n",
       "      <td>16920.789062</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4244.379883</td>\n",
       "      <td>1619.400024</td>\n",
       "      <td>2687.600098</td>\n",
       "      <td>1520809.0</td>\n",
       "      <td>55413.121094</td>\n",
       "      <td>1547.459961</td>\n",
       "      <td>38646.109375</td>\n",
       "      <td>18110.599609</td>\n",
       "      <td>11783.389648</td>\n",
       "      <td>2069.669922</td>\n",
       "      <td>3316.560059</td>\n",
       "      <td>5035.410156</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21565.339844</td>\n",
       "      <td>11.930000</td>\n",
       "      <td>4898.140137</td>\n",
       "      <td>-0.008829</td>\n",
       "      <td>-0.012268</td>\n",
       "      <td>0.016588</td>\n",
       "      <td>-0.016304</td>\n",
       "      <td>0.036963</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>-0.013446</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.025501</td>\n",
       "      <td>0.025723</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>-0.009509</td>\n",
       "      <td>0.042625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>-0.010380</td>\n",
       "      <td>-0.010779</td>\n",
       "      <td>-0.001464</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.002510</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.000908</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>-0.013767</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>-0.006003</td>\n",
       "      <td>-0.012569</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>-0.009036</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>-0.011690</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>-0.002209</td>\n",
       "      <td>0.010379</td>\n",
       "      <td>-0.001824</td>\n",
       "      <td>-0.000435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001948</td>\n",
       "      <td>-0.065779</td>\n",
       "      <td>0.013413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1967.510010</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003110</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-27</th>\n",
       "      <td>3124.042969</td>\n",
       "      <td>9507.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3299.219971</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8058.600098</td>\n",
       "      <td>7788.299805</td>\n",
       "      <td>3984.870117</td>\n",
       "      <td>75390.500000</td>\n",
       "      <td>830.539978</td>\n",
       "      <td>124496.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8132.490234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18774.710938</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22373.400391</td>\n",
       "      <td>18827.349609</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7176.419922</td>\n",
       "      <td>4252.540039</td>\n",
       "      <td>1618.270020</td>\n",
       "      <td>2722.989990</td>\n",
       "      <td>1520809.0</td>\n",
       "      <td>55452.878906</td>\n",
       "      <td>1553.880005</td>\n",
       "      <td>38900.019531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>11756.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3318.449951</td>\n",
       "      <td>5059.200195</td>\n",
       "      <td>1967.520020</td>\n",
       "      <td>21803.769531</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.011387</td>\n",
       "      <td>0.008825</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.028641</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.007426</td>\n",
       "      <td>0.007855</td>\n",
       "      <td>0.003766</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001528</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004635</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004351</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.011737</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006364</td>\n",
       "      <td>0.001923</td>\n",
       "      <td>-0.000698</td>\n",
       "      <td>0.013168</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000717</td>\n",
       "      <td>0.004149</td>\n",
       "      <td>0.006570</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.002324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000570</td>\n",
       "      <td>0.004725</td>\n",
       "      <td>0.000005</td>\n",
       "      <td>0.011056</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-28</th>\n",
       "      <td>3109.572021</td>\n",
       "      <td>9391.049805</td>\n",
       "      <td>189.770355</td>\n",
       "      <td>478.429993</td>\n",
       "      <td>171.610001</td>\n",
       "      <td>140.797913</td>\n",
       "      <td>45.886879</td>\n",
       "      <td>3302.909912</td>\n",
       "      <td>429.545776</td>\n",
       "      <td>132.554062</td>\n",
       "      <td>113.881905</td>\n",
       "      <td>277.404297</td>\n",
       "      <td>124.139496</td>\n",
       "      <td>211.119293</td>\n",
       "      <td>-3.204001e+02</td>\n",
       "      <td>158.324310</td>\n",
       "      <td>198.308990</td>\n",
       "      <td>8034.899902</td>\n",
       "      <td>7766.700195</td>\n",
       "      <td>3948.080078</td>\n",
       "      <td>75170.453125</td>\n",
       "      <td>824.140015</td>\n",
       "      <td>123780.0</td>\n",
       "      <td>38852.859375</td>\n",
       "      <td>8057.799805</td>\n",
       "      <td>8254.200195</td>\n",
       "      <td>18677.869141</td>\n",
       "      <td>5306.040039</td>\n",
       "      <td>22265.099609</td>\n",
       "      <td>18821.160156</td>\n",
       "      <td>17019.880859</td>\n",
       "      <td>7253.625977</td>\n",
       "      <td>4242.720215</td>\n",
       "      <td>1615.819946</td>\n",
       "      <td>2722.850098</td>\n",
       "      <td>1575417.0</td>\n",
       "      <td>55184.949219</td>\n",
       "      <td>1545.500000</td>\n",
       "      <td>38855.371094</td>\n",
       "      <td>18007.009766</td>\n",
       "      <td>11682.509766</td>\n",
       "      <td>2066.850098</td>\n",
       "      <td>3330.090088</td>\n",
       "      <td>5030.350098</td>\n",
       "      <td>1985.180054</td>\n",
       "      <td>21858.410156</td>\n",
       "      <td>12.920000</td>\n",
       "      <td>4978.810059</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>-0.012274</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.031558</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>-0.003016</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.024558</td>\n",
       "      <td>0.069804</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.012930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003688</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>-0.002941</td>\n",
       "      <td>-0.002773</td>\n",
       "      <td>-0.009232</td>\n",
       "      <td>-0.002919</td>\n",
       "      <td>-0.007706</td>\n",
       "      <td>-0.005751</td>\n",
       "      <td>-0.005547</td>\n",
       "      <td>-0.009184</td>\n",
       "      <td>-0.007622</td>\n",
       "      <td>-0.005158</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>-0.004841</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.035907</td>\n",
       "      <td>-0.004832</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.005720</td>\n",
       "      <td>-0.006251</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>-0.005703</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.082984</td>\n",
       "      <td>0.016470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-29</th>\n",
       "      <td>3111.018066</td>\n",
       "      <td>9414.980469</td>\n",
       "      <td>190.069992</td>\n",
       "      <td>477.600006</td>\n",
       "      <td>165.139999</td>\n",
       "      <td>138.626816</td>\n",
       "      <td>45.688580</td>\n",
       "      <td>3318.030029</td>\n",
       "      <td>428.397858</td>\n",
       "      <td>131.465012</td>\n",
       "      <td>114.805748</td>\n",
       "      <td>271.650421</td>\n",
       "      <td>123.391609</td>\n",
       "      <td>206.343658</td>\n",
       "      <td>-3.204001e+02</td>\n",
       "      <td>153.288834</td>\n",
       "      <td>193.649323</td>\n",
       "      <td>7935.700195</td>\n",
       "      <td>7665.600098</td>\n",
       "      <td>3922.530029</td>\n",
       "      <td>74502.898438</td>\n",
       "      <td>816.359985</td>\n",
       "      <td>122707.0</td>\n",
       "      <td>38441.539062</td>\n",
       "      <td>7935.029785</td>\n",
       "      <td>8183.100098</td>\n",
       "      <td>18473.289062</td>\n",
       "      <td>5266.950195</td>\n",
       "      <td>21898.000000</td>\n",
       "      <td>18477.009766</td>\n",
       "      <td>16920.580078</td>\n",
       "      <td>7140.229004</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1605.349976</td>\n",
       "      <td>2677.300049</td>\n",
       "      <td>1567731.0</td>\n",
       "      <td>55212.640625</td>\n",
       "      <td>1527.510010</td>\n",
       "      <td>38556.871094</td>\n",
       "      <td>17794.890625</td>\n",
       "      <td>11678.679688</td>\n",
       "      <td>2036.189941</td>\n",
       "      <td>3323.199951</td>\n",
       "      <td>4963.200195</td>\n",
       "      <td>1989.859985</td>\n",
       "      <td>21662.500000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>4898.259766</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>-0.001735</td>\n",
       "      <td>-0.037702</td>\n",
       "      <td>-0.015420</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>-0.002672</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>-0.020742</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.022621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031805</td>\n",
       "      <td>-0.023497</td>\n",
       "      <td>-0.012346</td>\n",
       "      <td>-0.013017</td>\n",
       "      <td>-0.006472</td>\n",
       "      <td>-0.008881</td>\n",
       "      <td>-0.009440</td>\n",
       "      <td>-0.008669</td>\n",
       "      <td>-0.010587</td>\n",
       "      <td>-0.015236</td>\n",
       "      <td>-0.008614</td>\n",
       "      <td>-0.010953</td>\n",
       "      <td>-0.007367</td>\n",
       "      <td>-0.016488</td>\n",
       "      <td>-0.018285</td>\n",
       "      <td>-0.005834</td>\n",
       "      <td>-0.015633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006480</td>\n",
       "      <td>-0.016729</td>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>-0.011640</td>\n",
       "      <td>-0.007682</td>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.014834</td>\n",
       "      <td>-0.002069</td>\n",
       "      <td>-0.013349</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>-0.008963</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>-0.016179</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>736 rows  96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Ticker        000001.SS     399001.SZ        AAPL        ADBE         AMD  \\\n",
       "Date                                                                        \n",
       "2022-01-02          NaN           NaN         NaN         NaN         NaN   \n",
       "2022-01-03          NaN           NaN  179.273621  564.369995  150.240005   \n",
       "2022-01-04  3632.330078  14791.309570  176.998352  554.000000  144.419998   \n",
       "2022-01-05  3595.179932  14525.759766  172.290207  514.429993  136.149994   \n",
       "2022-01-06  3586.080078  14429.509766  169.414108  514.119995  136.229996   \n",
       "...                 ...           ...         ...         ...         ...   \n",
       "2024-05-24  3088.871094   9424.580078  189.760345  475.429993  166.360001   \n",
       "2024-05-26          NaN           NaN         NaN         NaN         NaN   \n",
       "2024-05-27  3124.042969   9507.750000         NaN         NaN         NaN   \n",
       "2024-05-28  3109.572021   9391.049805  189.770355  478.429993  171.610001   \n",
       "2024-05-29  3111.018066   9414.980469  190.069992  477.600006  165.139999   \n",
       "\n",
       "Ticker            AVGO       CSCO     IMOEX.ME        MSFT          MU  \\\n",
       "Date                                                                     \n",
       "2022-01-02         NaN        NaN          NaN         NaN         NaN   \n",
       "2022-01-03   62.370541  57.983242  3852.500000  326.940796   94.188248   \n",
       "2022-01-04   63.085148  56.561138  3873.489990  321.334717   94.768616   \n",
       "2022-01-05   60.459896  55.665394  3815.050049  308.999420   92.860268   \n",
       "2022-01-06   59.898548  56.256390  3753.290039  306.557678   94.089890   \n",
       "...                ...        ...          ...         ...         ...   \n",
       "2024-05-24  140.338379  46.025688  3396.500000  429.386078  129.376846   \n",
       "2024-05-26         NaN        NaN          NaN         NaN         NaN   \n",
       "2024-05-27         NaN        NaN  3299.219971         NaN         NaN   \n",
       "2024-05-28  140.797913  45.886879  3302.909912  429.545776  132.554062   \n",
       "2024-05-29  138.626816  45.688580  3318.030029  428.397858  131.465012   \n",
       "\n",
       "Ticker            NVDA        NXPI        ORCL        QCOM         SSNLF  \\\n",
       "Date                                                                       \n",
       "2022-01-02         NaN         NaN         NaN         NaN           NaN   \n",
       "2022-01-03   30.073059  219.249969   84.268791  175.001373  2.395472e+10   \n",
       "2022-01-04   29.243383  220.814590   85.169952  175.959976  2.395472e+10   \n",
       "2022-01-05   27.560068  212.944275   82.888268  175.273880  2.395472e+10   \n",
       "2022-01-06   28.133154  215.210541   83.080719  174.756989  2.395472e+10   \n",
       "...                ...         ...         ...         ...           ...   \n",
       "2024-05-24  106.451149  274.963867  122.563950  208.424332 -3.204001e+02   \n",
       "2024-05-26         NaN         NaN         NaN         NaN           NaN   \n",
       "2024-05-27         NaN         NaN         NaN         NaN           NaN   \n",
       "2024-05-28  113.881905  277.404297  124.139496  211.119293 -3.204001e+02   \n",
       "2024-05-29  114.805748  271.650421  123.391609  206.343658 -3.204001e+02   \n",
       "\n",
       "Ticker             TSM         TXN        ^AORD        ^AXJO         ^BFX  \\\n",
       "Date                                                                        \n",
       "2022-01-02         NaN         NaN          NaN          NaN          NaN   \n",
       "2022-01-03  122.335548  175.891220          NaN          NaN  4335.279785   \n",
       "2022-01-04  126.704697  176.075790  7926.799805  7589.799805  4362.209961   \n",
       "2022-01-05  120.682884  172.402908  7899.600098  7565.799805  4344.620117   \n",
       "2022-01-06  122.022118  172.347549  7679.299805  7358.299805  4308.600098   \n",
       "...                ...         ...          ...          ...          ...   \n",
       "2024-05-24  158.910294  197.891693  7999.200195  7727.600098  3969.919922   \n",
       "2024-05-26         NaN         NaN          NaN          NaN          NaN   \n",
       "2024-05-27         NaN         NaN  8058.600098  7788.299805  3984.870117   \n",
       "2024-05-28  158.324310  198.308990  8034.899902  7766.700195  3948.080078   \n",
       "2024-05-29  153.288834  193.649323  7935.700195  7665.600098  3922.530029   \n",
       "\n",
       "Ticker            ^BSESN    ^BUK100P     ^BVSP          ^DJI        ^FCHI  \\\n",
       "Date                                                                        \n",
       "2022-01-02           NaN         NaN       NaN           NaN          NaN   \n",
       "2022-01-03  59183.218750  732.270020  103922.0  36585.058594  7217.220215   \n",
       "2022-01-04  59855.929688  743.950012  103514.0  36799.648438  7317.410156   \n",
       "2022-01-05  60223.148438  746.140015  101006.0  36407.109375  7376.370117   \n",
       "2022-01-06  59601.839844  738.669983  101561.0  36236.468750  7249.660156   \n",
       "...                  ...         ...       ...           ...          ...   \n",
       "2024-05-24  75410.390625  830.539978  124306.0  39069.589844  8094.970215   \n",
       "2024-05-26           NaN         NaN       NaN           NaN          NaN   \n",
       "2024-05-27  75390.500000  830.539978  124496.0           NaN  8132.490234   \n",
       "2024-05-28  75170.453125  824.140015  123780.0  38852.859375  8057.799805   \n",
       "2024-05-29  74502.898438  816.359985  122707.0  38441.539062  7935.029785   \n",
       "\n",
       "Ticker            ^FTSE        ^GDAXI        ^GSPC       ^GSPTSE  \\\n",
       "Date                                                               \n",
       "2022-01-02          NaN           NaN          NaN           NaN   \n",
       "2022-01-03          NaN  16020.730469  4796.560059           NaN   \n",
       "2022-01-04  7505.200195  16152.610352  4793.540039  21236.500000   \n",
       "2022-01-05  7516.899902  16271.750000  4700.580078  21039.699219   \n",
       "2022-01-06  7450.399902  16052.030273  4696.049805  21072.199219   \n",
       "...                 ...           ...          ...           ...   \n",
       "2024-05-24  8317.599609  18693.369141  5304.720215  22320.900391   \n",
       "2024-05-26          NaN           NaN          NaN           NaN   \n",
       "2024-05-27          NaN  18774.710938          NaN  22373.400391   \n",
       "2024-05-28  8254.200195  18677.869141  5306.040039  22265.099609   \n",
       "2024-05-29  8183.100098  18473.289062  5266.950195  21898.000000   \n",
       "\n",
       "Ticker              ^HSI         ^IXIC        ^JKSE     ^JN0U.JO        ^KLSE  \\\n",
       "Date                                                                            \n",
       "2022-01-02           NaN           NaN          NaN          NaN          NaN   \n",
       "2022-01-03  23274.750000  15832.799805  6665.308105  4532.049805  1549.050049   \n",
       "2022-01-04  23289.839844  15622.719727  6695.373047  4603.919922  1541.900024   \n",
       "2022-01-05  22907.250000  15100.169922  6662.298828  4673.850098  1547.949951   \n",
       "2022-01-06  23072.859375  15080.860352  6653.351074  4617.549805  1533.359985   \n",
       "...                  ...           ...          ...          ...          ...   \n",
       "2024-05-24  18608.939453  16920.789062          NaN  4244.379883  1619.400024   \n",
       "2024-05-26           NaN           NaN          NaN          NaN          NaN   \n",
       "2024-05-27  18827.349609           NaN  7176.419922  4252.540039  1618.270020   \n",
       "2024-05-28  18821.160156  17019.880859  7253.625977  4242.720215  1615.819946   \n",
       "2024-05-29  18477.009766  16920.580078  7140.229004          NaN  1605.349976   \n",
       "\n",
       "Ticker            ^KS11      ^MERV          ^MXX        ^N100         ^N225  \\\n",
       "Date                                                                          \n",
       "2022-01-02          NaN        NaN           NaN          NaN           NaN   \n",
       "2022-01-03          NaN    85579.0  52941.011719  1372.060059           NaN   \n",
       "2022-01-04  2989.239990    85286.0  53016.781250  1383.489990  29301.789062   \n",
       "2022-01-05  2953.969971    83836.0  53024.148438  1388.089966  29332.160156   \n",
       "2022-01-06  2920.530029    83748.0  53055.308594  1365.170044  28487.869141   \n",
       "...                 ...        ...           ...          ...           ...   \n",
       "2024-05-24  2687.600098  1520809.0  55413.121094  1547.459961  38646.109375   \n",
       "2024-05-26          NaN        NaN           NaN          NaN           NaN   \n",
       "2024-05-27  2722.989990  1520809.0  55452.878906  1553.880005  38900.019531   \n",
       "2024-05-28  2722.850098  1575417.0  55184.949219  1545.500000  38855.371094   \n",
       "2024-05-29  2677.300049  1567731.0  55212.640625  1527.510010  38556.871094   \n",
       "\n",
       "Ticker              ^NYA         ^NZ50         ^RUT         ^STI    ^STOXX50E  \\\n",
       "Date                                                                            \n",
       "2022-01-02           NaN           NaN          NaN          NaN          NaN   \n",
       "2022-01-03  17226.099609           NaN  2272.560059  3134.250000  4331.819824   \n",
       "2022-01-04  17336.759766           NaN  2268.870117  3181.129883  4367.620117   \n",
       "2022-01-05  17112.599609  13150.379883  2194.000000  3163.439941  4392.149902   \n",
       "2022-01-06  17156.519531  12983.009766  2206.370117  3184.300049  4324.810059   \n",
       "...                  ...           ...          ...          ...          ...   \n",
       "2024-05-24  18110.599609  11783.389648  2069.669922  3316.560059  5035.410156   \n",
       "2024-05-26           NaN           NaN          NaN          NaN          NaN   \n",
       "2024-05-27           NaN  11756.000000          NaN  3318.449951  5059.200195   \n",
       "2024-05-28  18007.009766  11682.509766  2066.850098  3330.090088  5030.350098   \n",
       "2024-05-29  17794.890625  11678.679688  2036.189941  3323.199951  4963.200195   \n",
       "\n",
       "Ticker        ^TA125.TA         ^TWII       ^VIX         ^XAX  \\\n",
       "Date                                                            \n",
       "2022-01-02  2073.090088           NaN        NaN          NaN   \n",
       "2022-01-03  2086.850098  18270.509766  16.600000  3461.719971   \n",
       "2022-01-04  2088.530029  18526.349609  16.910000  3496.050049   \n",
       "2022-01-05  2105.479980  18499.960938  19.730000  3469.100098   \n",
       "2022-01-06  2088.489990  18367.919922  19.610001  3546.679932   \n",
       "...                 ...           ...        ...          ...   \n",
       "2024-05-24          NaN  21565.339844  11.930000  4898.140137   \n",
       "2024-05-26  1967.510010           NaN        NaN          NaN   \n",
       "2024-05-27  1967.520020  21803.769531        NaN          NaN   \n",
       "2024-05-28  1985.180054  21858.410156  12.920000  4978.810059   \n",
       "2024-05-29  1989.859985  21662.500000  14.280000  4898.259766   \n",
       "\n",
       "Ticker      Return_000001.SS  Return_399001.SZ  Return_AAPL  Return_ADBE  \\\n",
       "Date                                                                       \n",
       "2022-01-02               NaN               NaN          NaN          NaN   \n",
       "2022-01-03               NaN               NaN          NaN          NaN   \n",
       "2022-01-04               NaN               NaN    -0.012692    -0.018374   \n",
       "2022-01-05         -0.010228         -0.017953    -0.026600    -0.071426   \n",
       "2022-01-06         -0.002531         -0.006626    -0.016693    -0.000603   \n",
       "...                      ...               ...          ...          ...   \n",
       "2024-05-24         -0.008829         -0.012268     0.016588    -0.016304   \n",
       "2024-05-26          0.000000          0.000000     0.000000     0.000000   \n",
       "2024-05-27          0.011387          0.008825     0.000000     0.000000   \n",
       "2024-05-28         -0.004632         -0.012274     0.000053     0.006310   \n",
       "2024-05-29          0.000465          0.002548     0.001579    -0.001735   \n",
       "\n",
       "Ticker      Return_AMD  Return_AVGO  Return_CSCO  Return_IMOEX.ME  \\\n",
       "Date                                                                \n",
       "2022-01-02         NaN          NaN          NaN              NaN   \n",
       "2022-01-03         NaN          NaN          NaN              NaN   \n",
       "2022-01-04   -0.038738     0.011457    -0.024526         0.005448   \n",
       "2022-01-05   -0.057264    -0.041614    -0.015837        -0.015087   \n",
       "2022-01-06    0.000588    -0.009285     0.010617        -0.016189   \n",
       "...                ...          ...          ...              ...   \n",
       "2024-05-24    0.036963     0.010443    -0.003863        -0.013446   \n",
       "2024-05-26    0.000000     0.000000     0.000000         0.000000   \n",
       "2024-05-27    0.000000     0.000000     0.000000        -0.028641   \n",
       "2024-05-28    0.031558     0.003274    -0.003016         0.001118   \n",
       "2024-05-29   -0.037702    -0.015420    -0.004321         0.004578   \n",
       "\n",
       "Ticker      Return_MSFT  Return_MU  Return_NVDA  Return_NXPI  Return_ORCL  \\\n",
       "Date                                                                        \n",
       "2022-01-02          NaN        NaN          NaN          NaN          NaN   \n",
       "2022-01-03          NaN        NaN          NaN          NaN          NaN   \n",
       "2022-01-04    -0.017147   0.006162    -0.027589     0.007136     0.010694   \n",
       "2022-01-05    -0.038388  -0.020137    -0.057562    -0.035642    -0.026790   \n",
       "2022-01-06    -0.007902   0.013242     0.020794     0.010643     0.002322   \n",
       "...                 ...        ...          ...          ...          ...   \n",
       "2024-05-24     0.007400   0.025501     0.025723     0.016317    -0.009509   \n",
       "2024-05-26     0.000000   0.000000     0.000000     0.000000     0.000000   \n",
       "2024-05-27     0.000000   0.000000     0.000000     0.000000     0.000000   \n",
       "2024-05-28     0.000372   0.024558     0.069804     0.008875     0.012855   \n",
       "2024-05-29    -0.002672  -0.008216     0.008112    -0.020742    -0.006025   \n",
       "\n",
       "Ticker      Return_QCOM  Return_SSNLF  Return_TSM  Return_TXN  Return_^AORD  \\\n",
       "Date                                                                          \n",
       "2022-01-02          NaN           NaN         NaN         NaN           NaN   \n",
       "2022-01-03          NaN           NaN         NaN         NaN           NaN   \n",
       "2022-01-04     0.005478           0.0    0.035714    0.001049           NaN   \n",
       "2022-01-05    -0.003899           0.0   -0.047526   -0.020860     -0.003431   \n",
       "2022-01-06    -0.002949           0.0    0.011097   -0.000321     -0.027888   \n",
       "...                 ...           ...         ...         ...           ...   \n",
       "2024-05-24     0.042625           0.0    0.018524    0.009580     -0.010380   \n",
       "2024-05-26     0.000000           0.0    0.000000    0.000000      0.000000   \n",
       "2024-05-27     0.000000           0.0    0.000000    0.000000      0.007426   \n",
       "2024-05-28     0.012930           0.0   -0.003688    0.002109     -0.002941   \n",
       "2024-05-29    -0.022621           0.0   -0.031805   -0.023497     -0.012346   \n",
       "\n",
       "Ticker      Return_^AXJO  Return_^BFX  Return_^BSESN  Return_^BUK100P  \\\n",
       "Date                                                                    \n",
       "2022-01-02           NaN          NaN            NaN              NaN   \n",
       "2022-01-03           NaN          NaN            NaN              NaN   \n",
       "2022-01-04           NaN     0.006212       0.011367         0.015950   \n",
       "2022-01-05     -0.003162    -0.004032       0.006135         0.002944   \n",
       "2022-01-06     -0.027426    -0.008291      -0.010317        -0.010012   \n",
       "...                  ...          ...            ...              ...   \n",
       "2024-05-24     -0.010779    -0.001464      -0.000101        -0.002510   \n",
       "2024-05-26      0.000000     0.000000       0.000000         0.000000   \n",
       "2024-05-27      0.007855     0.003766      -0.000264         0.000000   \n",
       "2024-05-28     -0.002773    -0.009232      -0.002919        -0.007706   \n",
       "2024-05-29     -0.013017    -0.006472      -0.008881        -0.009440   \n",
       "\n",
       "Ticker      Return_^BVSP  Return_^DJI  Return_^FCHI  Return_^FTSE  \\\n",
       "Date                                                                \n",
       "2022-01-02           NaN          NaN           NaN           NaN   \n",
       "2022-01-03           NaN          NaN           NaN           NaN   \n",
       "2022-01-04     -0.003926     0.005866      0.013882           NaN   \n",
       "2022-01-05     -0.024229    -0.010667      0.008057      0.001559   \n",
       "2022-01-06      0.005495    -0.004687     -0.017178     -0.008847   \n",
       "...                  ...          ...           ...           ...   \n",
       "2024-05-24     -0.003391     0.000111     -0.000908     -0.002590   \n",
       "2024-05-26      0.000000     0.000000      0.000000      0.000000   \n",
       "2024-05-27      0.001528     0.000000      0.004635      0.000000   \n",
       "2024-05-28     -0.005751    -0.005547     -0.009184     -0.007622   \n",
       "2024-05-29     -0.008669    -0.010587     -0.015236     -0.008614   \n",
       "\n",
       "Ticker      Return_^GDAXI  Return_^GSPC  Return_^GSPTSE  Return_^HSI  \\\n",
       "Date                                                                   \n",
       "2022-01-02            NaN           NaN             NaN          NaN   \n",
       "2022-01-03            NaN           NaN             NaN          NaN   \n",
       "2022-01-04       0.008232     -0.000630             NaN     0.000648   \n",
       "2022-01-05       0.007376     -0.019393       -0.009267    -0.016427   \n",
       "2022-01-06      -0.013503     -0.000964        0.001545     0.007230   \n",
       "...                   ...           ...             ...          ...   \n",
       "2024-05-24       0.000110      0.007001        0.005410    -0.013767   \n",
       "2024-05-26       0.000000      0.000000        0.000000     0.000000   \n",
       "2024-05-27       0.004351      0.000000        0.002352     0.011737   \n",
       "2024-05-28      -0.005158      0.000249       -0.004841    -0.000329   \n",
       "2024-05-29      -0.010953     -0.007367       -0.016488    -0.018285   \n",
       "\n",
       "Ticker      Return_^IXIC  Return_^JKSE  Return_^JN0U.JO  Return_^KLSE  \\\n",
       "Date                                                                    \n",
       "2022-01-02           NaN           NaN              NaN           NaN   \n",
       "2022-01-03           NaN           NaN              NaN           NaN   \n",
       "2022-01-04     -0.013269      0.004511         0.015858     -0.004616   \n",
       "2022-01-05     -0.033448     -0.004940         0.015189      0.003924   \n",
       "2022-01-06     -0.001279     -0.001343        -0.012046     -0.009425   \n",
       "...                  ...           ...              ...           ...   \n",
       "2024-05-24      0.011040      0.000000         0.004485     -0.006003   \n",
       "2024-05-26      0.000000      0.000000         0.000000      0.000000   \n",
       "2024-05-27      0.000000     -0.006364         0.001923     -0.000698   \n",
       "2024-05-28      0.005856      0.010758        -0.002309     -0.001514   \n",
       "2024-05-29     -0.005834     -0.015633         0.000000     -0.006480   \n",
       "\n",
       "Ticker      Return_^KS11  Return_^MERV  Return_^MXX  Return_^N100  \\\n",
       "Date                                                                \n",
       "2022-01-02           NaN           NaN          NaN           NaN   \n",
       "2022-01-03           NaN           NaN          NaN           NaN   \n",
       "2022-01-04           NaN     -0.003424     0.001431      0.008330   \n",
       "2022-01-05     -0.011799     -0.017002     0.000139      0.003325   \n",
       "2022-01-06     -0.011320     -0.001050     0.000588     -0.016512   \n",
       "...                  ...           ...          ...           ...   \n",
       "2024-05-24     -0.012569      0.004645    -0.009036     -0.000336   \n",
       "2024-05-26      0.000000      0.000000     0.000000      0.000000   \n",
       "2024-05-27      0.013168      0.000000     0.000717      0.004149   \n",
       "2024-05-28     -0.000051      0.035907    -0.004832     -0.005393   \n",
       "2024-05-29     -0.016729     -0.004879     0.000502     -0.011640   \n",
       "\n",
       "Ticker      Return_^N225  Return_^NYA  Return_^NZ50  Return_^RUT  Return_^STI  \\\n",
       "Date                                                                            \n",
       "2022-01-02           NaN          NaN           NaN          NaN          NaN   \n",
       "2022-01-03           NaN          NaN           NaN          NaN          NaN   \n",
       "2022-01-04           NaN     0.006424           NaN    -0.001624     0.014957   \n",
       "2022-01-05      0.001036    -0.012930           NaN    -0.032999    -0.005561   \n",
       "2022-01-06     -0.028784     0.002567     -0.012727     0.005638     0.006594   \n",
       "...                  ...          ...           ...          ...          ...   \n",
       "2024-05-24     -0.011690     0.004556     -0.002209     0.010379    -0.001824   \n",
       "2024-05-26      0.000000     0.000000      0.000000     0.000000     0.000000   \n",
       "2024-05-27      0.006570     0.000000     -0.002324     0.000000     0.000570   \n",
       "2024-05-28     -0.001148    -0.005720     -0.006251    -0.001362     0.003508   \n",
       "2024-05-29     -0.007682    -0.011780     -0.000328    -0.014834    -0.002069   \n",
       "\n",
       "Ticker      Return_^STOXX50E  Return_^TA125.TA  Return_^TWII  Return_^VIX  \\\n",
       "Date                                                                        \n",
       "2022-01-02               NaN               NaN           NaN          NaN   \n",
       "2022-01-03               NaN          0.006637           NaN          NaN   \n",
       "2022-01-04          0.008264          0.000805      0.014003     0.018675   \n",
       "2022-01-05          0.005616          0.008116     -0.001424     0.166765   \n",
       "2022-01-06         -0.015332         -0.008069     -0.007137    -0.006082   \n",
       "...                      ...               ...           ...          ...   \n",
       "2024-05-24         -0.000435          0.000000     -0.001948    -0.065779   \n",
       "2024-05-26          0.000000          0.003110      0.000000     0.000000   \n",
       "2024-05-27          0.004725          0.000005      0.011056     0.000000   \n",
       "2024-05-28         -0.005703          0.008976      0.002506     0.082984   \n",
       "2024-05-29         -0.013349          0.002357     -0.008963     0.105263   \n",
       "\n",
       "Ticker      Return_^XAX  \n",
       "Date                     \n",
       "2022-01-02          NaN  \n",
       "2022-01-03          NaN  \n",
       "2022-01-04     0.009917  \n",
       "2022-01-05    -0.007709  \n",
       "2022-01-06     0.022363  \n",
       "...                 ...  \n",
       "2024-05-24     0.013413  \n",
       "2024-05-26     0.000000  \n",
       "2024-05-27     0.000000  \n",
       "2024-05-28     0.016470  \n",
       "2024-05-29    -0.016179  \n",
       "\n",
       "[736 rows x 96 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "viM3z-O77c4H"
   },
   "outputs": [],
   "source": [
    "intel[\"Return\"]=intel[\"Adj Close\"].pct_change()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "iegWF8Gj7c4H"
   },
   "outputs": [],
   "source": [
    "new_colnames=[]\n",
    "for i in intel.columns:\n",
    "    new_colnames.append(\"Intel_\"+i)\n",
    "intel.columns=new_colnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 455
    },
    "id": "iggzyw197c4H",
    "outputId": "c61d6ae8-aeda-4083-b53b-3b76fab2f795"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intel_Open</th>\n",
       "      <th>Intel_High</th>\n",
       "      <th>Intel_Low</th>\n",
       "      <th>Intel_Close</th>\n",
       "      <th>Intel_Adj Close</th>\n",
       "      <th>Intel_Volume</th>\n",
       "      <th>Intel_Return</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>51.650002</td>\n",
       "      <td>53.230000</td>\n",
       "      <td>51.599998</td>\n",
       "      <td>53.209999</td>\n",
       "      <td>49.350483</td>\n",
       "      <td>33857600</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>53.570000</td>\n",
       "      <td>53.939999</td>\n",
       "      <td>52.650002</td>\n",
       "      <td>53.139999</td>\n",
       "      <td>49.285557</td>\n",
       "      <td>45681400</td>\n",
       "      <td>-0.001316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>54.189999</td>\n",
       "      <td>56.169998</td>\n",
       "      <td>53.830002</td>\n",
       "      <td>53.869999</td>\n",
       "      <td>49.962608</td>\n",
       "      <td>59109300</td>\n",
       "      <td>0.013737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>54.610001</td>\n",
       "      <td>54.669998</td>\n",
       "      <td>53.419998</td>\n",
       "      <td>54.009998</td>\n",
       "      <td>50.092457</td>\n",
       "      <td>35757900</td>\n",
       "      <td>0.002599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>54.189999</td>\n",
       "      <td>54.389999</td>\n",
       "      <td>53.150002</td>\n",
       "      <td>53.439999</td>\n",
       "      <td>49.563808</td>\n",
       "      <td>30717200</td>\n",
       "      <td>-0.010553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-22</th>\n",
       "      <td>31.910000</td>\n",
       "      <td>32.070000</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>31.420000</td>\n",
       "      <td>31.221941</td>\n",
       "      <td>36706400</td>\n",
       "      <td>-0.010082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-23</th>\n",
       "      <td>31.450001</td>\n",
       "      <td>31.570000</td>\n",
       "      <td>29.870001</td>\n",
       "      <td>30.080000</td>\n",
       "      <td>29.890387</td>\n",
       "      <td>62014500</td>\n",
       "      <td>-0.042648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-24</th>\n",
       "      <td>30.290001</td>\n",
       "      <td>31.020000</td>\n",
       "      <td>30.129999</td>\n",
       "      <td>30.719999</td>\n",
       "      <td>30.526352</td>\n",
       "      <td>42408200</td>\n",
       "      <td>0.021277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-28</th>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.370001</td>\n",
       "      <td>30.660000</td>\n",
       "      <td>31.059999</td>\n",
       "      <td>30.864210</td>\n",
       "      <td>36799100</td>\n",
       "      <td>0.011068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-29</th>\n",
       "      <td>30.469999</td>\n",
       "      <td>30.670000</td>\n",
       "      <td>30.110001</td>\n",
       "      <td>30.129999</td>\n",
       "      <td>29.940071</td>\n",
       "      <td>35848600</td>\n",
       "      <td>-0.029942</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>604 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Intel_Open  Intel_High  Intel_Low  Intel_Close  Intel_Adj Close  \\\n",
       "Date                                                                          \n",
       "2022-01-03   51.650002   53.230000  51.599998    53.209999        49.350483   \n",
       "2022-01-04   53.570000   53.939999  52.650002    53.139999        49.285557   \n",
       "2022-01-05   54.189999   56.169998  53.830002    53.869999        49.962608   \n",
       "2022-01-06   54.610001   54.669998  53.419998    54.009998        50.092457   \n",
       "2022-01-07   54.189999   54.389999  53.150002    53.439999        49.563808   \n",
       "...                ...         ...        ...          ...              ...   \n",
       "2024-05-22   31.910000   32.070000  31.100000    31.420000        31.221941   \n",
       "2024-05-23   31.450001   31.570000  29.870001    30.080000        29.890387   \n",
       "2024-05-24   30.290001   31.020000  30.129999    30.719999        30.526352   \n",
       "2024-05-28   30.940001   31.370001  30.660000    31.059999        30.864210   \n",
       "2024-05-29   30.469999   30.670000  30.110001    30.129999        29.940071   \n",
       "\n",
       "            Intel_Volume  Intel_Return  \n",
       "Date                                    \n",
       "2022-01-03      33857600           NaN  \n",
       "2022-01-04      45681400     -0.001316  \n",
       "2022-01-05      59109300      0.013737  \n",
       "2022-01-06      35757900      0.002599  \n",
       "2022-01-07      30717200     -0.010553  \n",
       "...                  ...           ...  \n",
       "2024-05-22      36706400     -0.010082  \n",
       "2024-05-23      62014500     -0.042648  \n",
       "2024-05-24      42408200      0.021277  \n",
       "2024-05-28      36799100      0.011068  \n",
       "2024-05-29      35848600     -0.029942  \n",
       "\n",
       "[604 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then I calculated indicators of Intel asset by using Ta-Lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Vr9rqhgC7c4H"
   },
   "outputs": [],
   "source": [
    "intel['EMA_10'] = talib.EMA(intel['Intel_Close'], timeperiod=10)\n",
    "intel['EMA_12'] = talib.EMA(intel['Intel_Close'], timeperiod=12)\n",
    "intel['EMA_50'] = talib.EMA(intel['Intel_Close'], timeperiod=50)\n",
    "intel['EMA_100'] = talib.EMA(intel['Intel_Close'], timeperiod=100)\n",
    "\n",
    "intel[\"SMA_10\"] = talib.SMA(intel['Intel_Close'], timeperiod=10)\n",
    "intel['SMA_12'] = talib.SMA(intel['Intel_Close'], timeperiod=12)\n",
    "intel['SMA_50'] = talib.SMA(intel['Intel_Close'], timeperiod=50)\n",
    "intel['SMA_100'] = talib.SMA(intel['Intel_Close'], timeperiod=100)\n",
    "\n",
    "intel['STD_10']=intel['Intel_Adj Close'].rolling(10).std()\n",
    "intel['STD_12']=intel['Intel_Adj Close'].rolling(12).std()\n",
    "intel['STD_50']=intel['Intel_Adj Close'].rolling(50).std()\n",
    "intel['STD_100']=intel['Intel_Adj Close'].rolling(100).std()\n",
    "\n",
    "intel['ADX_14'] = talib.ADX(intel['Intel_High'],intel['Intel_Low'],intel['Intel_Close'], timeperiod = 14)\n",
    "intel[\"ADX_21\"] = talib.ADX(intel['Intel_High'],intel['Intel_Low'],intel['Intel_Close'], timeperiod = 21)\n",
    "\n",
    "intel['RSI_14'] = talib.RSI(intel['Intel_Close'], timeperiod = 14)\n",
    "intel['RSI_21'] = talib.RSI(intel['Intel_Close'], timeperiod = 21)\n",
    "\n",
    "intel[\"MACD\"], intel[\"MACDSignal\"], intel[\"MACDHist\"] = talib.MACD(\n",
    "intel['Intel_Adj Close'], fastperiod=12, slowperiod=26, signalperiod=9)\n",
    "\n",
    "intel['UPPER_BBand_20'], intel['MID_BBand_20'],intel['LOWER_BBand_20'] = talib.BBANDS(intel['Intel_Close'],nbdevup=2,nbdevdn=2,timeperiod=20)\n",
    "\n",
    "intel['Volatility'] = talib.TRANGE(intel['Intel_High'], intel['Intel_Low'], intel['Intel_Close'])\n",
    "\n",
    "intel['Volatility'] = talib.TRANGE(intel['Intel_High'], intel['Intel_Low'], intel['Intel_Close'])\n",
    "intel['STOCH_K'], intel['STOCH_D'] = talib.STOCH(intel['Intel_High'], intel['Intel_Low'], intel['Intel_Close'])\n",
    "\n",
    "\n",
    "intel['CCI_14'] = talib.CCI(intel['Intel_High'], intel['Intel_Low'], intel['Intel_Close'],timeperiod=14)\n",
    "intel['CCI_20'] = talib.CCI(intel['Intel_High'], intel['Intel_Low'], intel['Intel_Close'], timeperiod=20)\n",
    "\n",
    "intel['OBV'] = talib.OBV(intel['Intel_Close'],intel['Intel_Volume'])\n",
    "\n",
    "intel['WilliamsR'] = talib.WILLR(intel['Intel_High'], intel['Intel_Low'], intel['Intel_Close'], timeperiod=14)\n",
    "\n",
    "\n",
    "intel[\"MOMENT_10\"] = talib.MOM(intel['Intel_Close'], timeperiod=10)\n",
    "intel[\"MOMENT_20\"] = talib.MOM(intel['Intel_Close'], timeperiod=20)\n",
    "\n",
    "intel[\"ROC_10\"] = talib.ROC(intel['Intel_Close'], timeperiod=10)\n",
    "intel[\"ROC_20\"] = talib.ROC(intel['Intel_Close'], timeperiod=20)\n",
    "\n",
    "intel['Trix'] = talib.TRIX(intel['Intel_Close'], timeperiod=30)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8cr7VZsf7c4I",
    "outputId": "256dacf9-4592-4dbb-f19e-077c6ada1924"
   },
   "outputs": [],
   "source": [
    "df = pd.merge(intel, peers, on='Date', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xK7ltLg97c4I",
    "outputId": "9fdd0428-6873-4fcd-f710-264e5accb9a2"
   },
   "outputs": [],
   "source": [
    "#To set the target I calculated the close-open difference\n",
    "df[\"Target_BASE\"]=df.Intel_Close - df.Intel_Open"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "17E23zL97c4I",
    "outputId": "d52b024a-932d-4afe-ca18-877b48aded93"
   },
   "outputs": [],
   "source": [
    "# Then target setted binary according to the difference is positive or negative\n",
    "df['Target'] = df['Target_BASE'].apply(lambda x: 0 if x < 0 else 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "7CoMyrQ17c4I"
   },
   "outputs": [],
   "source": [
    "# SHIFTING 1 TO PREVENT LEAKAGE!!\n",
    "df['Target'] = df['Target'].shift(1)\n",
    "df['Target_BASE'] =df['Target_BASE'].shift(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "5aW6Q8G97c4J"
   },
   "outputs": [],
   "source": [
    "df=df.bfill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "yEvKSTlB7c4J"
   },
   "outputs": [],
   "source": [
    "data_main=df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 666
    },
    "id": "3T29sy3lTjTT",
    "outputId": "ec654d1e-5936-4ecc-f72b-a573f6ba60bf"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intel_Open</th>\n",
       "      <th>Intel_High</th>\n",
       "      <th>Intel_Low</th>\n",
       "      <th>Intel_Close</th>\n",
       "      <th>Intel_Adj Close</th>\n",
       "      <th>Intel_Volume</th>\n",
       "      <th>Intel_Return</th>\n",
       "      <th>EMA_10</th>\n",
       "      <th>EMA_12</th>\n",
       "      <th>EMA_50</th>\n",
       "      <th>EMA_100</th>\n",
       "      <th>SMA_10</th>\n",
       "      <th>SMA_12</th>\n",
       "      <th>SMA_50</th>\n",
       "      <th>SMA_100</th>\n",
       "      <th>STD_10</th>\n",
       "      <th>STD_12</th>\n",
       "      <th>STD_50</th>\n",
       "      <th>STD_100</th>\n",
       "      <th>ADX_14</th>\n",
       "      <th>ADX_21</th>\n",
       "      <th>RSI_14</th>\n",
       "      <th>RSI_21</th>\n",
       "      <th>MACD</th>\n",
       "      <th>MACDSignal</th>\n",
       "      <th>MACDHist</th>\n",
       "      <th>UPPER_BBand_20</th>\n",
       "      <th>MID_BBand_20</th>\n",
       "      <th>LOWER_BBand_20</th>\n",
       "      <th>Volatility</th>\n",
       "      <th>STOCH_K</th>\n",
       "      <th>STOCH_D</th>\n",
       "      <th>CCI_14</th>\n",
       "      <th>CCI_20</th>\n",
       "      <th>OBV</th>\n",
       "      <th>WilliamsR</th>\n",
       "      <th>MOMENT_10</th>\n",
       "      <th>MOMENT_20</th>\n",
       "      <th>ROC_10</th>\n",
       "      <th>ROC_20</th>\n",
       "      <th>Trix</th>\n",
       "      <th>000001.SS</th>\n",
       "      <th>399001.SZ</th>\n",
       "      <th>AAPL</th>\n",
       "      <th>ADBE</th>\n",
       "      <th>AMD</th>\n",
       "      <th>AVGO</th>\n",
       "      <th>CSCO</th>\n",
       "      <th>IMOEX.ME</th>\n",
       "      <th>MSFT</th>\n",
       "      <th>...</th>\n",
       "      <th>Return_000001.SS</th>\n",
       "      <th>Return_399001.SZ</th>\n",
       "      <th>Return_AAPL</th>\n",
       "      <th>Return_ADBE</th>\n",
       "      <th>Return_AMD</th>\n",
       "      <th>Return_AVGO</th>\n",
       "      <th>Return_CSCO</th>\n",
       "      <th>Return_IMOEX.ME</th>\n",
       "      <th>Return_MSFT</th>\n",
       "      <th>Return_MU</th>\n",
       "      <th>Return_NVDA</th>\n",
       "      <th>Return_NXPI</th>\n",
       "      <th>Return_ORCL</th>\n",
       "      <th>Return_QCOM</th>\n",
       "      <th>Return_SSNLF</th>\n",
       "      <th>Return_TSM</th>\n",
       "      <th>Return_TXN</th>\n",
       "      <th>Return_^AORD</th>\n",
       "      <th>Return_^AXJO</th>\n",
       "      <th>Return_^BFX</th>\n",
       "      <th>Return_^BSESN</th>\n",
       "      <th>Return_^BUK100P</th>\n",
       "      <th>Return_^BVSP</th>\n",
       "      <th>Return_^DJI</th>\n",
       "      <th>Return_^FCHI</th>\n",
       "      <th>Return_^FTSE</th>\n",
       "      <th>Return_^GDAXI</th>\n",
       "      <th>Return_^GSPC</th>\n",
       "      <th>Return_^GSPTSE</th>\n",
       "      <th>Return_^HSI</th>\n",
       "      <th>Return_^IXIC</th>\n",
       "      <th>Return_^JKSE</th>\n",
       "      <th>Return_^JN0U.JO</th>\n",
       "      <th>Return_^KLSE</th>\n",
       "      <th>Return_^KS11</th>\n",
       "      <th>Return_^MERV</th>\n",
       "      <th>Return_^MXX</th>\n",
       "      <th>Return_^N100</th>\n",
       "      <th>Return_^N225</th>\n",
       "      <th>Return_^NYA</th>\n",
       "      <th>Return_^NZ50</th>\n",
       "      <th>Return_^RUT</th>\n",
       "      <th>Return_^STI</th>\n",
       "      <th>Return_^STOXX50E</th>\n",
       "      <th>Return_^TA125.TA</th>\n",
       "      <th>Return_^TWII</th>\n",
       "      <th>Return_^VIX</th>\n",
       "      <th>Return_^XAX</th>\n",
       "      <th>Target_BASE</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-01-03</th>\n",
       "      <td>51.650002</td>\n",
       "      <td>53.230000</td>\n",
       "      <td>51.599998</td>\n",
       "      <td>53.209999</td>\n",
       "      <td>49.350483</td>\n",
       "      <td>33857600</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.590600</td>\n",
       "      <td>47.924300</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.5906</td>\n",
       "      <td>47.9243</td>\n",
       "      <td>1.022311</td>\n",
       "      <td>0.959088</td>\n",
       "      <td>2.930448</td>\n",
       "      <td>3.076405</td>\n",
       "      <td>16.472383</td>\n",
       "      <td>18.220351</td>\n",
       "      <td>43.294616</td>\n",
       "      <td>39.014250</td>\n",
       "      <td>-1.759980</td>\n",
       "      <td>-1.915761</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>57.646486</td>\n",
       "      <td>52.842999</td>\n",
       "      <td>48.039513</td>\n",
       "      <td>1.289997</td>\n",
       "      <td>77.108971</td>\n",
       "      <td>75.841475</td>\n",
       "      <td>-127.145951</td>\n",
       "      <td>-153.836562</td>\n",
       "      <td>33857600.0</td>\n",
       "      <td>-90.598239</td>\n",
       "      <td>1.549999</td>\n",
       "      <td>-4.259998</td>\n",
       "      <td>2.912985</td>\n",
       "      <td>-8.006011</td>\n",
       "      <td>-0.124921</td>\n",
       "      <td>3632.330078</td>\n",
       "      <td>14791.309570</td>\n",
       "      <td>179.273621</td>\n",
       "      <td>564.369995</td>\n",
       "      <td>150.240005</td>\n",
       "      <td>62.370541</td>\n",
       "      <td>57.983242</td>\n",
       "      <td>3852.500000</td>\n",
       "      <td>326.940796</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010228</td>\n",
       "      <td>-0.017953</td>\n",
       "      <td>-0.012692</td>\n",
       "      <td>-0.018374</td>\n",
       "      <td>-0.038738</td>\n",
       "      <td>0.011457</td>\n",
       "      <td>-0.024526</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>-0.017147</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>-0.003162</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>0.015950</td>\n",
       "      <td>-0.003926</td>\n",
       "      <td>0.005866</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.008232</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>-0.009267</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>-0.013269</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.015858</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>-0.003424</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.008330</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>-0.012727</td>\n",
       "      <td>-0.001624</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.006637</td>\n",
       "      <td>0.014003</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>1.559998</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-04</th>\n",
       "      <td>53.570000</td>\n",
       "      <td>53.939999</td>\n",
       "      <td>52.650002</td>\n",
       "      <td>53.139999</td>\n",
       "      <td>49.285557</td>\n",
       "      <td>45681400</td>\n",
       "      <td>-0.001316</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.590600</td>\n",
       "      <td>47.924300</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.5906</td>\n",
       "      <td>47.9243</td>\n",
       "      <td>1.022311</td>\n",
       "      <td>0.959088</td>\n",
       "      <td>2.930448</td>\n",
       "      <td>3.076405</td>\n",
       "      <td>16.472383</td>\n",
       "      <td>18.220351</td>\n",
       "      <td>43.294616</td>\n",
       "      <td>39.014250</td>\n",
       "      <td>-1.759980</td>\n",
       "      <td>-1.915761</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>57.646486</td>\n",
       "      <td>52.842999</td>\n",
       "      <td>48.039513</td>\n",
       "      <td>1.289997</td>\n",
       "      <td>77.108971</td>\n",
       "      <td>75.841475</td>\n",
       "      <td>-127.145951</td>\n",
       "      <td>-153.836562</td>\n",
       "      <td>-11823800.0</td>\n",
       "      <td>-90.598239</td>\n",
       "      <td>1.549999</td>\n",
       "      <td>-4.259998</td>\n",
       "      <td>2.912985</td>\n",
       "      <td>-8.006011</td>\n",
       "      <td>-0.124921</td>\n",
       "      <td>3632.330078</td>\n",
       "      <td>14791.309570</td>\n",
       "      <td>176.998352</td>\n",
       "      <td>554.000000</td>\n",
       "      <td>144.419998</td>\n",
       "      <td>63.085148</td>\n",
       "      <td>56.561138</td>\n",
       "      <td>3873.489990</td>\n",
       "      <td>321.334717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010228</td>\n",
       "      <td>-0.017953</td>\n",
       "      <td>-0.012692</td>\n",
       "      <td>-0.018374</td>\n",
       "      <td>-0.038738</td>\n",
       "      <td>0.011457</td>\n",
       "      <td>-0.024526</td>\n",
       "      <td>0.005448</td>\n",
       "      <td>-0.017147</td>\n",
       "      <td>0.006162</td>\n",
       "      <td>-0.027589</td>\n",
       "      <td>0.007136</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.005478</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.001049</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>-0.003162</td>\n",
       "      <td>0.006212</td>\n",
       "      <td>0.011367</td>\n",
       "      <td>0.015950</td>\n",
       "      <td>-0.003926</td>\n",
       "      <td>0.005866</td>\n",
       "      <td>0.013882</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.008232</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>-0.009267</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>-0.013269</td>\n",
       "      <td>0.004511</td>\n",
       "      <td>0.015858</td>\n",
       "      <td>-0.004616</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>-0.003424</td>\n",
       "      <td>0.001431</td>\n",
       "      <td>0.008330</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.006424</td>\n",
       "      <td>-0.012727</td>\n",
       "      <td>-0.001624</td>\n",
       "      <td>0.014957</td>\n",
       "      <td>0.008264</td>\n",
       "      <td>0.000805</td>\n",
       "      <td>0.014003</td>\n",
       "      <td>0.018675</td>\n",
       "      <td>0.009917</td>\n",
       "      <td>1.559998</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-05</th>\n",
       "      <td>54.189999</td>\n",
       "      <td>56.169998</td>\n",
       "      <td>53.830002</td>\n",
       "      <td>53.869999</td>\n",
       "      <td>49.962608</td>\n",
       "      <td>59109300</td>\n",
       "      <td>0.013737</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.590600</td>\n",
       "      <td>47.924300</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.5906</td>\n",
       "      <td>47.9243</td>\n",
       "      <td>1.022311</td>\n",
       "      <td>0.959088</td>\n",
       "      <td>2.930448</td>\n",
       "      <td>3.076405</td>\n",
       "      <td>16.472383</td>\n",
       "      <td>18.220351</td>\n",
       "      <td>43.294616</td>\n",
       "      <td>39.014250</td>\n",
       "      <td>-1.759980</td>\n",
       "      <td>-1.915761</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>57.646486</td>\n",
       "      <td>52.842999</td>\n",
       "      <td>48.039513</td>\n",
       "      <td>3.029999</td>\n",
       "      <td>77.108971</td>\n",
       "      <td>75.841475</td>\n",
       "      <td>-127.145951</td>\n",
       "      <td>-153.836562</td>\n",
       "      <td>47285500.0</td>\n",
       "      <td>-90.598239</td>\n",
       "      <td>1.549999</td>\n",
       "      <td>-4.259998</td>\n",
       "      <td>2.912985</td>\n",
       "      <td>-8.006011</td>\n",
       "      <td>-0.124921</td>\n",
       "      <td>3595.179932</td>\n",
       "      <td>14525.759766</td>\n",
       "      <td>172.290207</td>\n",
       "      <td>514.429993</td>\n",
       "      <td>136.149994</td>\n",
       "      <td>60.459896</td>\n",
       "      <td>55.665394</td>\n",
       "      <td>3815.050049</td>\n",
       "      <td>308.999420</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.010228</td>\n",
       "      <td>-0.017953</td>\n",
       "      <td>-0.026600</td>\n",
       "      <td>-0.071426</td>\n",
       "      <td>-0.057264</td>\n",
       "      <td>-0.041614</td>\n",
       "      <td>-0.015837</td>\n",
       "      <td>-0.015087</td>\n",
       "      <td>-0.038388</td>\n",
       "      <td>-0.020137</td>\n",
       "      <td>-0.057562</td>\n",
       "      <td>-0.035642</td>\n",
       "      <td>-0.026790</td>\n",
       "      <td>-0.003899</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.047526</td>\n",
       "      <td>-0.020860</td>\n",
       "      <td>-0.003431</td>\n",
       "      <td>-0.003162</td>\n",
       "      <td>-0.004032</td>\n",
       "      <td>0.006135</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>-0.024229</td>\n",
       "      <td>-0.010667</td>\n",
       "      <td>0.008057</td>\n",
       "      <td>0.001559</td>\n",
       "      <td>0.007376</td>\n",
       "      <td>-0.019393</td>\n",
       "      <td>-0.009267</td>\n",
       "      <td>-0.016427</td>\n",
       "      <td>-0.033448</td>\n",
       "      <td>-0.004940</td>\n",
       "      <td>0.015189</td>\n",
       "      <td>0.003924</td>\n",
       "      <td>-0.011799</td>\n",
       "      <td>-0.017002</td>\n",
       "      <td>0.000139</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.001036</td>\n",
       "      <td>-0.012930</td>\n",
       "      <td>-0.012727</td>\n",
       "      <td>-0.032999</td>\n",
       "      <td>-0.005561</td>\n",
       "      <td>0.005616</td>\n",
       "      <td>0.008116</td>\n",
       "      <td>-0.001424</td>\n",
       "      <td>0.166765</td>\n",
       "      <td>-0.007709</td>\n",
       "      <td>-0.430000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-06</th>\n",
       "      <td>54.610001</td>\n",
       "      <td>54.669998</td>\n",
       "      <td>53.419998</td>\n",
       "      <td>54.009998</td>\n",
       "      <td>50.092457</td>\n",
       "      <td>35757900</td>\n",
       "      <td>0.002599</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.590600</td>\n",
       "      <td>47.924300</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.5906</td>\n",
       "      <td>47.9243</td>\n",
       "      <td>1.022311</td>\n",
       "      <td>0.959088</td>\n",
       "      <td>2.930448</td>\n",
       "      <td>3.076405</td>\n",
       "      <td>16.472383</td>\n",
       "      <td>18.220351</td>\n",
       "      <td>43.294616</td>\n",
       "      <td>39.014250</td>\n",
       "      <td>-1.759980</td>\n",
       "      <td>-1.915761</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>57.646486</td>\n",
       "      <td>52.842999</td>\n",
       "      <td>48.039513</td>\n",
       "      <td>1.250000</td>\n",
       "      <td>77.108971</td>\n",
       "      <td>75.841475</td>\n",
       "      <td>-127.145951</td>\n",
       "      <td>-153.836562</td>\n",
       "      <td>83043400.0</td>\n",
       "      <td>-90.598239</td>\n",
       "      <td>1.549999</td>\n",
       "      <td>-4.259998</td>\n",
       "      <td>2.912985</td>\n",
       "      <td>-8.006011</td>\n",
       "      <td>-0.124921</td>\n",
       "      <td>3586.080078</td>\n",
       "      <td>14429.509766</td>\n",
       "      <td>169.414108</td>\n",
       "      <td>514.119995</td>\n",
       "      <td>136.229996</td>\n",
       "      <td>59.898548</td>\n",
       "      <td>56.256390</td>\n",
       "      <td>3753.290039</td>\n",
       "      <td>306.557678</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002531</td>\n",
       "      <td>-0.006626</td>\n",
       "      <td>-0.016693</td>\n",
       "      <td>-0.000603</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.009285</td>\n",
       "      <td>0.010617</td>\n",
       "      <td>-0.016189</td>\n",
       "      <td>-0.007902</td>\n",
       "      <td>0.013242</td>\n",
       "      <td>0.020794</td>\n",
       "      <td>0.010643</td>\n",
       "      <td>0.002322</td>\n",
       "      <td>-0.002949</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.011097</td>\n",
       "      <td>-0.000321</td>\n",
       "      <td>-0.027888</td>\n",
       "      <td>-0.027426</td>\n",
       "      <td>-0.008291</td>\n",
       "      <td>-0.010317</td>\n",
       "      <td>-0.010012</td>\n",
       "      <td>0.005495</td>\n",
       "      <td>-0.004687</td>\n",
       "      <td>-0.017178</td>\n",
       "      <td>-0.008847</td>\n",
       "      <td>-0.013503</td>\n",
       "      <td>-0.000964</td>\n",
       "      <td>0.001545</td>\n",
       "      <td>0.007230</td>\n",
       "      <td>-0.001279</td>\n",
       "      <td>-0.001343</td>\n",
       "      <td>-0.012046</td>\n",
       "      <td>-0.009425</td>\n",
       "      <td>-0.011320</td>\n",
       "      <td>-0.001050</td>\n",
       "      <td>0.000588</td>\n",
       "      <td>-0.016512</td>\n",
       "      <td>-0.028784</td>\n",
       "      <td>0.002567</td>\n",
       "      <td>-0.012727</td>\n",
       "      <td>0.005638</td>\n",
       "      <td>0.006594</td>\n",
       "      <td>-0.015332</td>\n",
       "      <td>-0.008069</td>\n",
       "      <td>-0.007137</td>\n",
       "      <td>-0.006082</td>\n",
       "      <td>0.022363</td>\n",
       "      <td>-0.320000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-01-07</th>\n",
       "      <td>54.189999</td>\n",
       "      <td>54.389999</td>\n",
       "      <td>53.150002</td>\n",
       "      <td>53.439999</td>\n",
       "      <td>49.563808</td>\n",
       "      <td>30717200</td>\n",
       "      <td>-0.010553</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.590600</td>\n",
       "      <td>47.924300</td>\n",
       "      <td>54.516999</td>\n",
       "      <td>54.462499</td>\n",
       "      <td>49.5906</td>\n",
       "      <td>47.9243</td>\n",
       "      <td>1.022311</td>\n",
       "      <td>0.959088</td>\n",
       "      <td>2.930448</td>\n",
       "      <td>3.076405</td>\n",
       "      <td>16.472383</td>\n",
       "      <td>18.220351</td>\n",
       "      <td>43.294616</td>\n",
       "      <td>39.014250</td>\n",
       "      <td>-1.759980</td>\n",
       "      <td>-1.915761</td>\n",
       "      <td>0.155781</td>\n",
       "      <td>57.646486</td>\n",
       "      <td>52.842999</td>\n",
       "      <td>48.039513</td>\n",
       "      <td>1.239998</td>\n",
       "      <td>77.108971</td>\n",
       "      <td>75.841475</td>\n",
       "      <td>-127.145951</td>\n",
       "      <td>-153.836562</td>\n",
       "      <td>52326200.0</td>\n",
       "      <td>-90.598239</td>\n",
       "      <td>1.549999</td>\n",
       "      <td>-4.259998</td>\n",
       "      <td>2.912985</td>\n",
       "      <td>-8.006011</td>\n",
       "      <td>-0.124921</td>\n",
       "      <td>3579.540039</td>\n",
       "      <td>14343.650391</td>\n",
       "      <td>169.581543</td>\n",
       "      <td>510.700012</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>58.217331</td>\n",
       "      <td>56.450321</td>\n",
       "      <td>3738.679932</td>\n",
       "      <td>306.713959</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.001824</td>\n",
       "      <td>-0.005950</td>\n",
       "      <td>0.000988</td>\n",
       "      <td>-0.006652</td>\n",
       "      <td>-0.031050</td>\n",
       "      <td>-0.028068</td>\n",
       "      <td>0.003447</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000510</td>\n",
       "      <td>-0.012546</td>\n",
       "      <td>-0.033040</td>\n",
       "      <td>-0.024806</td>\n",
       "      <td>0.013551</td>\n",
       "      <td>-0.029793</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.038686</td>\n",
       "      <td>-0.039195</td>\n",
       "      <td>0.012384</td>\n",
       "      <td>0.012911</td>\n",
       "      <td>-0.000518</td>\n",
       "      <td>0.002396</td>\n",
       "      <td>0.004901</td>\n",
       "      <td>0.011402</td>\n",
       "      <td>-0.000133</td>\n",
       "      <td>-0.004163</td>\n",
       "      <td>0.004684</td>\n",
       "      <td>-0.006497</td>\n",
       "      <td>-0.004050</td>\n",
       "      <td>0.000584</td>\n",
       "      <td>0.018226</td>\n",
       "      <td>-0.009612</td>\n",
       "      <td>0.007209</td>\n",
       "      <td>0.002683</td>\n",
       "      <td>0.006359</td>\n",
       "      <td>0.011765</td>\n",
       "      <td>0.007809</td>\n",
       "      <td>0.002767</td>\n",
       "      <td>-0.001575</td>\n",
       "      <td>-0.000327</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>-0.000952</td>\n",
       "      <td>-0.012038</td>\n",
       "      <td>0.006582</td>\n",
       "      <td>-0.004389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.010788</td>\n",
       "      <td>-0.043345</td>\n",
       "      <td>0.001661</td>\n",
       "      <td>-0.600002</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-22</th>\n",
       "      <td>31.910000</td>\n",
       "      <td>32.070000</td>\n",
       "      <td>31.100000</td>\n",
       "      <td>31.420000</td>\n",
       "      <td>31.221941</td>\n",
       "      <td>36706400</td>\n",
       "      <td>-0.010082</td>\n",
       "      <td>31.474919</td>\n",
       "      <td>31.513129</td>\n",
       "      <td>35.244977</td>\n",
       "      <td>37.858013</td>\n",
       "      <td>31.189000</td>\n",
       "      <td>31.047500</td>\n",
       "      <td>36.2242</td>\n",
       "      <td>40.6712</td>\n",
       "      <td>0.797209</td>\n",
       "      <td>0.805347</td>\n",
       "      <td>4.917986</td>\n",
       "      <td>5.792286</td>\n",
       "      <td>35.637491</td>\n",
       "      <td>31.369431</td>\n",
       "      <td>39.085774</td>\n",
       "      <td>36.890862</td>\n",
       "      <td>-1.163296</td>\n",
       "      <td>-1.647915</td>\n",
       "      <td>0.484619</td>\n",
       "      <td>33.448438</td>\n",
       "      <td>31.207000</td>\n",
       "      <td>28.965562</td>\n",
       "      <td>0.969999</td>\n",
       "      <td>54.775639</td>\n",
       "      <td>71.019414</td>\n",
       "      <td>54.056797</td>\n",
       "      <td>27.506663</td>\n",
       "      <td>200978900.0</td>\n",
       "      <td>-37.174669</td>\n",
       "      <td>1.420000</td>\n",
       "      <td>-3.080000</td>\n",
       "      <td>4.733334</td>\n",
       "      <td>-8.927536</td>\n",
       "      <td>-0.556908</td>\n",
       "      <td>3158.540039</td>\n",
       "      <td>9693.049805</td>\n",
       "      <td>190.679291</td>\n",
       "      <td>483.929993</td>\n",
       "      <td>165.520004</td>\n",
       "      <td>138.783325</td>\n",
       "      <td>47.027111</td>\n",
       "      <td>3444.639893</td>\n",
       "      <td>429.745422</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000182</td>\n",
       "      <td>0.001176</td>\n",
       "      <td>-0.007538</td>\n",
       "      <td>0.004317</td>\n",
       "      <td>0.005223</td>\n",
       "      <td>-0.004974</td>\n",
       "      <td>0.010439</td>\n",
       "      <td>0.004743</td>\n",
       "      <td>0.003450</td>\n",
       "      <td>-0.009569</td>\n",
       "      <td>-0.004571</td>\n",
       "      <td>0.016070</td>\n",
       "      <td>-0.000241</td>\n",
       "      <td>0.010356</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016138</td>\n",
       "      <td>0.017738</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>-0.000459</td>\n",
       "      <td>0.001428</td>\n",
       "      <td>0.003621</td>\n",
       "      <td>-0.005404</td>\n",
       "      <td>-0.013829</td>\n",
       "      <td>-0.005065</td>\n",
       "      <td>-0.006062</td>\n",
       "      <td>-0.005489</td>\n",
       "      <td>-0.002486</td>\n",
       "      <td>-0.002706</td>\n",
       "      <td>-0.005403</td>\n",
       "      <td>-0.001302</td>\n",
       "      <td>-0.001846</td>\n",
       "      <td>0.005058</td>\n",
       "      <td>-0.015814</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000264</td>\n",
       "      <td>-0.011548</td>\n",
       "      <td>-0.005791</td>\n",
       "      <td>-0.002914</td>\n",
       "      <td>-0.008469</td>\n",
       "      <td>-0.005522</td>\n",
       "      <td>0.004821</td>\n",
       "      <td>-0.007935</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.004323</td>\n",
       "      <td>-0.004080</td>\n",
       "      <td>0.014837</td>\n",
       "      <td>0.036256</td>\n",
       "      <td>-0.017918</td>\n",
       "      <td>-0.240000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-23</th>\n",
       "      <td>31.450001</td>\n",
       "      <td>31.570000</td>\n",
       "      <td>29.870001</td>\n",
       "      <td>30.080000</td>\n",
       "      <td>29.890387</td>\n",
       "      <td>62014500</td>\n",
       "      <td>-0.042648</td>\n",
       "      <td>31.221297</td>\n",
       "      <td>31.292647</td>\n",
       "      <td>35.042429</td>\n",
       "      <td>37.703992</td>\n",
       "      <td>31.188000</td>\n",
       "      <td>30.997500</td>\n",
       "      <td>35.9612</td>\n",
       "      <td>40.4695</td>\n",
       "      <td>0.798727</td>\n",
       "      <td>0.847227</td>\n",
       "      <td>4.886677</td>\n",
       "      <td>5.805915</td>\n",
       "      <td>35.485685</td>\n",
       "      <td>31.462805</td>\n",
       "      <td>32.389611</td>\n",
       "      <td>32.980547</td>\n",
       "      <td>-1.189535</td>\n",
       "      <td>-1.556239</td>\n",
       "      <td>0.366704</td>\n",
       "      <td>32.362048</td>\n",
       "      <td>30.955500</td>\n",
       "      <td>29.548952</td>\n",
       "      <td>1.699999</td>\n",
       "      <td>30.080583</td>\n",
       "      <td>52.847406</td>\n",
       "      <td>-57.101332</td>\n",
       "      <td>-59.536812</td>\n",
       "      <td>138964400.0</td>\n",
       "      <td>-86.988827</td>\n",
       "      <td>-0.010000</td>\n",
       "      <td>-5.030001</td>\n",
       "      <td>-0.033234</td>\n",
       "      <td>-14.326404</td>\n",
       "      <td>-0.560661</td>\n",
       "      <td>3116.386963</td>\n",
       "      <td>9541.639648</td>\n",
       "      <td>186.663940</td>\n",
       "      <td>483.309998</td>\n",
       "      <td>160.429993</td>\n",
       "      <td>138.887985</td>\n",
       "      <td>46.204159</td>\n",
       "      <td>3442.790039</td>\n",
       "      <td>426.231750</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.013346</td>\n",
       "      <td>-0.015620</td>\n",
       "      <td>-0.021058</td>\n",
       "      <td>-0.001281</td>\n",
       "      <td>-0.030752</td>\n",
       "      <td>0.000754</td>\n",
       "      <td>-0.017500</td>\n",
       "      <td>-0.000537</td>\n",
       "      <td>-0.008176</td>\n",
       "      <td>-0.000079</td>\n",
       "      <td>0.093196</td>\n",
       "      <td>-0.021913</td>\n",
       "      <td>-0.004093</td>\n",
       "      <td>-0.005765</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>-0.025921</td>\n",
       "      <td>-0.004336</td>\n",
       "      <td>-0.004625</td>\n",
       "      <td>-0.005244</td>\n",
       "      <td>0.016127</td>\n",
       "      <td>-0.003435</td>\n",
       "      <td>-0.007330</td>\n",
       "      <td>-0.015270</td>\n",
       "      <td>0.001263</td>\n",
       "      <td>-0.003715</td>\n",
       "      <td>0.000595</td>\n",
       "      <td>-0.007381</td>\n",
       "      <td>-0.006533</td>\n",
       "      <td>-0.017029</td>\n",
       "      <td>-0.003899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.018132</td>\n",
       "      <td>0.004371</td>\n",
       "      <td>-0.000606</td>\n",
       "      <td>-0.031399</td>\n",
       "      <td>-0.009102</td>\n",
       "      <td>0.003065</td>\n",
       "      <td>0.012588</td>\n",
       "      <td>-0.011415</td>\n",
       "      <td>0.006580</td>\n",
       "      <td>-0.015996</td>\n",
       "      <td>0.004450</td>\n",
       "      <td>0.002474</td>\n",
       "      <td>-0.003100</td>\n",
       "      <td>0.002580</td>\n",
       "      <td>0.039056</td>\n",
       "      <td>-0.006732</td>\n",
       "      <td>-0.490000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-24</th>\n",
       "      <td>30.290001</td>\n",
       "      <td>31.020000</td>\n",
       "      <td>30.129999</td>\n",
       "      <td>30.719999</td>\n",
       "      <td>30.526352</td>\n",
       "      <td>42408200</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>31.130152</td>\n",
       "      <td>31.204548</td>\n",
       "      <td>34.872922</td>\n",
       "      <td>37.565696</td>\n",
       "      <td>31.275000</td>\n",
       "      <td>31.057500</td>\n",
       "      <td>35.7206</td>\n",
       "      <td>40.2987</td>\n",
       "      <td>0.676221</td>\n",
       "      <td>0.794676</td>\n",
       "      <td>4.841667</td>\n",
       "      <td>5.837862</td>\n",
       "      <td>35.344722</td>\n",
       "      <td>31.551732</td>\n",
       "      <td>37.864861</td>\n",
       "      <td>36.363258</td>\n",
       "      <td>-1.145804</td>\n",
       "      <td>-1.474152</td>\n",
       "      <td>0.328348</td>\n",
       "      <td>32.241030</td>\n",
       "      <td>30.897500</td>\n",
       "      <td>29.553970</td>\n",
       "      <td>0.940001</td>\n",
       "      <td>21.937006</td>\n",
       "      <td>35.597743</td>\n",
       "      <td>-40.249904</td>\n",
       "      <td>-41.280765</td>\n",
       "      <td>181372600.0</td>\n",
       "      <td>-63.197016</td>\n",
       "      <td>0.869999</td>\n",
       "      <td>-1.160000</td>\n",
       "      <td>2.914569</td>\n",
       "      <td>-3.638645</td>\n",
       "      <td>-0.563569</td>\n",
       "      <td>3088.871094</td>\n",
       "      <td>9424.580078</td>\n",
       "      <td>189.760345</td>\n",
       "      <td>475.429993</td>\n",
       "      <td>166.360001</td>\n",
       "      <td>140.338379</td>\n",
       "      <td>46.025688</td>\n",
       "      <td>3396.500000</td>\n",
       "      <td>429.386078</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.008829</td>\n",
       "      <td>-0.012268</td>\n",
       "      <td>0.016588</td>\n",
       "      <td>-0.016304</td>\n",
       "      <td>0.036963</td>\n",
       "      <td>0.010443</td>\n",
       "      <td>-0.003863</td>\n",
       "      <td>-0.013446</td>\n",
       "      <td>0.007400</td>\n",
       "      <td>0.025501</td>\n",
       "      <td>0.025723</td>\n",
       "      <td>0.016317</td>\n",
       "      <td>-0.009509</td>\n",
       "      <td>0.042625</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.018524</td>\n",
       "      <td>0.009580</td>\n",
       "      <td>-0.010380</td>\n",
       "      <td>-0.010779</td>\n",
       "      <td>-0.001464</td>\n",
       "      <td>-0.000101</td>\n",
       "      <td>-0.002510</td>\n",
       "      <td>-0.003391</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>-0.000908</td>\n",
       "      <td>-0.002590</td>\n",
       "      <td>0.000110</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>0.005410</td>\n",
       "      <td>-0.013767</td>\n",
       "      <td>0.011040</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.004485</td>\n",
       "      <td>-0.006003</td>\n",
       "      <td>-0.012569</td>\n",
       "      <td>0.004645</td>\n",
       "      <td>-0.009036</td>\n",
       "      <td>-0.000336</td>\n",
       "      <td>-0.011690</td>\n",
       "      <td>0.004556</td>\n",
       "      <td>-0.002209</td>\n",
       "      <td>0.010379</td>\n",
       "      <td>-0.001824</td>\n",
       "      <td>-0.000435</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001948</td>\n",
       "      <td>-0.065779</td>\n",
       "      <td>0.013413</td>\n",
       "      <td>-1.370001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-28</th>\n",
       "      <td>30.940001</td>\n",
       "      <td>31.370001</td>\n",
       "      <td>30.660000</td>\n",
       "      <td>31.059999</td>\n",
       "      <td>30.864210</td>\n",
       "      <td>36799100</td>\n",
       "      <td>0.011068</td>\n",
       "      <td>31.117397</td>\n",
       "      <td>31.182309</td>\n",
       "      <td>34.723395</td>\n",
       "      <td>37.436870</td>\n",
       "      <td>31.330000</td>\n",
       "      <td>31.138333</td>\n",
       "      <td>35.4890</td>\n",
       "      <td>40.1388</td>\n",
       "      <td>0.628347</td>\n",
       "      <td>0.735150</td>\n",
       "      <td>4.781268</td>\n",
       "      <td>5.868656</td>\n",
       "      <td>34.699121</td>\n",
       "      <td>31.430747</td>\n",
       "      <td>40.616192</td>\n",
       "      <td>38.105877</td>\n",
       "      <td>-1.071533</td>\n",
       "      <td>-1.393628</td>\n",
       "      <td>0.322095</td>\n",
       "      <td>32.211662</td>\n",
       "      <td>30.882500</td>\n",
       "      <td>29.553337</td>\n",
       "      <td>0.710001</td>\n",
       "      <td>30.593157</td>\n",
       "      <td>27.536915</td>\n",
       "      <td>1.929444</td>\n",
       "      <td>13.611346</td>\n",
       "      <td>218171700.0</td>\n",
       "      <td>-50.557598</td>\n",
       "      <td>0.549999</td>\n",
       "      <td>-0.300001</td>\n",
       "      <td>1.802685</td>\n",
       "      <td>-0.956636</td>\n",
       "      <td>-0.565393</td>\n",
       "      <td>3109.572021</td>\n",
       "      <td>9391.049805</td>\n",
       "      <td>189.770355</td>\n",
       "      <td>478.429993</td>\n",
       "      <td>171.610001</td>\n",
       "      <td>140.797913</td>\n",
       "      <td>45.886879</td>\n",
       "      <td>3302.909912</td>\n",
       "      <td>429.545776</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.004632</td>\n",
       "      <td>-0.012274</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.006310</td>\n",
       "      <td>0.031558</td>\n",
       "      <td>0.003274</td>\n",
       "      <td>-0.003016</td>\n",
       "      <td>0.001118</td>\n",
       "      <td>0.000372</td>\n",
       "      <td>0.024558</td>\n",
       "      <td>0.069804</td>\n",
       "      <td>0.008875</td>\n",
       "      <td>0.012855</td>\n",
       "      <td>0.012930</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.003688</td>\n",
       "      <td>0.002109</td>\n",
       "      <td>-0.002941</td>\n",
       "      <td>-0.002773</td>\n",
       "      <td>-0.009232</td>\n",
       "      <td>-0.002919</td>\n",
       "      <td>-0.007706</td>\n",
       "      <td>-0.005751</td>\n",
       "      <td>-0.005547</td>\n",
       "      <td>-0.009184</td>\n",
       "      <td>-0.007622</td>\n",
       "      <td>-0.005158</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>-0.004841</td>\n",
       "      <td>-0.000329</td>\n",
       "      <td>0.005856</td>\n",
       "      <td>0.010758</td>\n",
       "      <td>-0.002309</td>\n",
       "      <td>-0.001514</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.035907</td>\n",
       "      <td>-0.004832</td>\n",
       "      <td>-0.005393</td>\n",
       "      <td>-0.001148</td>\n",
       "      <td>-0.005720</td>\n",
       "      <td>-0.006251</td>\n",
       "      <td>-0.001362</td>\n",
       "      <td>0.003508</td>\n",
       "      <td>-0.005703</td>\n",
       "      <td>0.008976</td>\n",
       "      <td>0.002506</td>\n",
       "      <td>0.082984</td>\n",
       "      <td>0.016470</td>\n",
       "      <td>0.429998</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024-05-29</th>\n",
       "      <td>30.469999</td>\n",
       "      <td>30.670000</td>\n",
       "      <td>30.110001</td>\n",
       "      <td>30.129999</td>\n",
       "      <td>29.940071</td>\n",
       "      <td>35848600</td>\n",
       "      <td>-0.029942</td>\n",
       "      <td>30.937870</td>\n",
       "      <td>31.020416</td>\n",
       "      <td>34.543262</td>\n",
       "      <td>37.292179</td>\n",
       "      <td>31.238000</td>\n",
       "      <td>31.161666</td>\n",
       "      <td>35.2374</td>\n",
       "      <td>39.9714</td>\n",
       "      <td>0.731383</td>\n",
       "      <td>0.694358</td>\n",
       "      <td>4.724869</td>\n",
       "      <td>5.911675</td>\n",
       "      <td>34.540178</td>\n",
       "      <td>31.487044</td>\n",
       "      <td>35.929701</td>\n",
       "      <td>35.327444</td>\n",
       "      <td>-1.074853</td>\n",
       "      <td>-1.329873</td>\n",
       "      <td>0.255021</td>\n",
       "      <td>32.223711</td>\n",
       "      <td>30.865500</td>\n",
       "      <td>29.507289</td>\n",
       "      <td>0.949999</td>\n",
       "      <td>31.787438</td>\n",
       "      <td>28.105867</td>\n",
       "      <td>-81.233979</td>\n",
       "      <td>-77.910099</td>\n",
       "      <td>182323100.0</td>\n",
       "      <td>-87.739469</td>\n",
       "      <td>-0.920000</td>\n",
       "      <td>-0.340000</td>\n",
       "      <td>-2.962963</td>\n",
       "      <td>-1.115852</td>\n",
       "      <td>-0.566814</td>\n",
       "      <td>3111.018066</td>\n",
       "      <td>9414.980469</td>\n",
       "      <td>190.069992</td>\n",
       "      <td>477.600006</td>\n",
       "      <td>165.139999</td>\n",
       "      <td>138.626816</td>\n",
       "      <td>45.688580</td>\n",
       "      <td>3318.030029</td>\n",
       "      <td>428.397858</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000465</td>\n",
       "      <td>0.002548</td>\n",
       "      <td>0.001579</td>\n",
       "      <td>-0.001735</td>\n",
       "      <td>-0.037702</td>\n",
       "      <td>-0.015420</td>\n",
       "      <td>-0.004321</td>\n",
       "      <td>0.004578</td>\n",
       "      <td>-0.002672</td>\n",
       "      <td>-0.008216</td>\n",
       "      <td>0.008112</td>\n",
       "      <td>-0.020742</td>\n",
       "      <td>-0.006025</td>\n",
       "      <td>-0.022621</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.031805</td>\n",
       "      <td>-0.023497</td>\n",
       "      <td>-0.012346</td>\n",
       "      <td>-0.013017</td>\n",
       "      <td>-0.006472</td>\n",
       "      <td>-0.008881</td>\n",
       "      <td>-0.009440</td>\n",
       "      <td>-0.008669</td>\n",
       "      <td>-0.010587</td>\n",
       "      <td>-0.015236</td>\n",
       "      <td>-0.008614</td>\n",
       "      <td>-0.010953</td>\n",
       "      <td>-0.007367</td>\n",
       "      <td>-0.016488</td>\n",
       "      <td>-0.018285</td>\n",
       "      <td>-0.005834</td>\n",
       "      <td>-0.015633</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.006480</td>\n",
       "      <td>-0.016729</td>\n",
       "      <td>-0.004879</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>-0.011640</td>\n",
       "      <td>-0.007682</td>\n",
       "      <td>-0.011780</td>\n",
       "      <td>-0.000328</td>\n",
       "      <td>-0.014834</td>\n",
       "      <td>-0.002069</td>\n",
       "      <td>-0.013349</td>\n",
       "      <td>0.002357</td>\n",
       "      <td>-0.008963</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>-0.016179</td>\n",
       "      <td>0.119999</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>604 rows  139 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Intel_Open  Intel_High  Intel_Low  Intel_Close  Intel_Adj Close  \\\n",
       "Date                                                                          \n",
       "2022-01-03   51.650002   53.230000  51.599998    53.209999        49.350483   \n",
       "2022-01-04   53.570000   53.939999  52.650002    53.139999        49.285557   \n",
       "2022-01-05   54.189999   56.169998  53.830002    53.869999        49.962608   \n",
       "2022-01-06   54.610001   54.669998  53.419998    54.009998        50.092457   \n",
       "2022-01-07   54.189999   54.389999  53.150002    53.439999        49.563808   \n",
       "...                ...         ...        ...          ...              ...   \n",
       "2024-05-22   31.910000   32.070000  31.100000    31.420000        31.221941   \n",
       "2024-05-23   31.450001   31.570000  29.870001    30.080000        29.890387   \n",
       "2024-05-24   30.290001   31.020000  30.129999    30.719999        30.526352   \n",
       "2024-05-28   30.940001   31.370001  30.660000    31.059999        30.864210   \n",
       "2024-05-29   30.469999   30.670000  30.110001    30.129999        29.940071   \n",
       "\n",
       "            Intel_Volume  Intel_Return     EMA_10     EMA_12     EMA_50  \\\n",
       "Date                                                                      \n",
       "2022-01-03      33857600     -0.001316  54.516999  54.462499  49.590600   \n",
       "2022-01-04      45681400     -0.001316  54.516999  54.462499  49.590600   \n",
       "2022-01-05      59109300      0.013737  54.516999  54.462499  49.590600   \n",
       "2022-01-06      35757900      0.002599  54.516999  54.462499  49.590600   \n",
       "2022-01-07      30717200     -0.010553  54.516999  54.462499  49.590600   \n",
       "...                  ...           ...        ...        ...        ...   \n",
       "2024-05-22      36706400     -0.010082  31.474919  31.513129  35.244977   \n",
       "2024-05-23      62014500     -0.042648  31.221297  31.292647  35.042429   \n",
       "2024-05-24      42408200      0.021277  31.130152  31.204548  34.872922   \n",
       "2024-05-28      36799100      0.011068  31.117397  31.182309  34.723395   \n",
       "2024-05-29      35848600     -0.029942  30.937870  31.020416  34.543262   \n",
       "\n",
       "              EMA_100     SMA_10     SMA_12   SMA_50  SMA_100    STD_10  \\\n",
       "Date                                                                      \n",
       "2022-01-03  47.924300  54.516999  54.462499  49.5906  47.9243  1.022311   \n",
       "2022-01-04  47.924300  54.516999  54.462499  49.5906  47.9243  1.022311   \n",
       "2022-01-05  47.924300  54.516999  54.462499  49.5906  47.9243  1.022311   \n",
       "2022-01-06  47.924300  54.516999  54.462499  49.5906  47.9243  1.022311   \n",
       "2022-01-07  47.924300  54.516999  54.462499  49.5906  47.9243  1.022311   \n",
       "...               ...        ...        ...      ...      ...       ...   \n",
       "2024-05-22  37.858013  31.189000  31.047500  36.2242  40.6712  0.797209   \n",
       "2024-05-23  37.703992  31.188000  30.997500  35.9612  40.4695  0.798727   \n",
       "2024-05-24  37.565696  31.275000  31.057500  35.7206  40.2987  0.676221   \n",
       "2024-05-28  37.436870  31.330000  31.138333  35.4890  40.1388  0.628347   \n",
       "2024-05-29  37.292179  31.238000  31.161666  35.2374  39.9714  0.731383   \n",
       "\n",
       "              STD_12    STD_50   STD_100     ADX_14     ADX_21     RSI_14  \\\n",
       "Date                                                                        \n",
       "2022-01-03  0.959088  2.930448  3.076405  16.472383  18.220351  43.294616   \n",
       "2022-01-04  0.959088  2.930448  3.076405  16.472383  18.220351  43.294616   \n",
       "2022-01-05  0.959088  2.930448  3.076405  16.472383  18.220351  43.294616   \n",
       "2022-01-06  0.959088  2.930448  3.076405  16.472383  18.220351  43.294616   \n",
       "2022-01-07  0.959088  2.930448  3.076405  16.472383  18.220351  43.294616   \n",
       "...              ...       ...       ...        ...        ...        ...   \n",
       "2024-05-22  0.805347  4.917986  5.792286  35.637491  31.369431  39.085774   \n",
       "2024-05-23  0.847227  4.886677  5.805915  35.485685  31.462805  32.389611   \n",
       "2024-05-24  0.794676  4.841667  5.837862  35.344722  31.551732  37.864861   \n",
       "2024-05-28  0.735150  4.781268  5.868656  34.699121  31.430747  40.616192   \n",
       "2024-05-29  0.694358  4.724869  5.911675  34.540178  31.487044  35.929701   \n",
       "\n",
       "               RSI_21      MACD  MACDSignal  MACDHist  UPPER_BBand_20  \\\n",
       "Date                                                                    \n",
       "2022-01-03  39.014250 -1.759980   -1.915761  0.155781       57.646486   \n",
       "2022-01-04  39.014250 -1.759980   -1.915761  0.155781       57.646486   \n",
       "2022-01-05  39.014250 -1.759980   -1.915761  0.155781       57.646486   \n",
       "2022-01-06  39.014250 -1.759980   -1.915761  0.155781       57.646486   \n",
       "2022-01-07  39.014250 -1.759980   -1.915761  0.155781       57.646486   \n",
       "...               ...       ...         ...       ...             ...   \n",
       "2024-05-22  36.890862 -1.163296   -1.647915  0.484619       33.448438   \n",
       "2024-05-23  32.980547 -1.189535   -1.556239  0.366704       32.362048   \n",
       "2024-05-24  36.363258 -1.145804   -1.474152  0.328348       32.241030   \n",
       "2024-05-28  38.105877 -1.071533   -1.393628  0.322095       32.211662   \n",
       "2024-05-29  35.327444 -1.074853   -1.329873  0.255021       32.223711   \n",
       "\n",
       "            MID_BBand_20  LOWER_BBand_20  Volatility    STOCH_K    STOCH_D  \\\n",
       "Date                                                                         \n",
       "2022-01-03     52.842999       48.039513    1.289997  77.108971  75.841475   \n",
       "2022-01-04     52.842999       48.039513    1.289997  77.108971  75.841475   \n",
       "2022-01-05     52.842999       48.039513    3.029999  77.108971  75.841475   \n",
       "2022-01-06     52.842999       48.039513    1.250000  77.108971  75.841475   \n",
       "2022-01-07     52.842999       48.039513    1.239998  77.108971  75.841475   \n",
       "...                  ...             ...         ...        ...        ...   \n",
       "2024-05-22     31.207000       28.965562    0.969999  54.775639  71.019414   \n",
       "2024-05-23     30.955500       29.548952    1.699999  30.080583  52.847406   \n",
       "2024-05-24     30.897500       29.553970    0.940001  21.937006  35.597743   \n",
       "2024-05-28     30.882500       29.553337    0.710001  30.593157  27.536915   \n",
       "2024-05-29     30.865500       29.507289    0.949999  31.787438  28.105867   \n",
       "\n",
       "                CCI_14      CCI_20          OBV  WilliamsR  MOMENT_10  \\\n",
       "Date                                                                    \n",
       "2022-01-03 -127.145951 -153.836562   33857600.0 -90.598239   1.549999   \n",
       "2022-01-04 -127.145951 -153.836562  -11823800.0 -90.598239   1.549999   \n",
       "2022-01-05 -127.145951 -153.836562   47285500.0 -90.598239   1.549999   \n",
       "2022-01-06 -127.145951 -153.836562   83043400.0 -90.598239   1.549999   \n",
       "2022-01-07 -127.145951 -153.836562   52326200.0 -90.598239   1.549999   \n",
       "...                ...         ...          ...        ...        ...   \n",
       "2024-05-22   54.056797   27.506663  200978900.0 -37.174669   1.420000   \n",
       "2024-05-23  -57.101332  -59.536812  138964400.0 -86.988827  -0.010000   \n",
       "2024-05-24  -40.249904  -41.280765  181372600.0 -63.197016   0.869999   \n",
       "2024-05-28    1.929444   13.611346  218171700.0 -50.557598   0.549999   \n",
       "2024-05-29  -81.233979  -77.910099  182323100.0 -87.739469  -0.920000   \n",
       "\n",
       "            MOMENT_20    ROC_10     ROC_20      Trix    000001.SS  \\\n",
       "Date                                                                \n",
       "2022-01-03  -4.259998  2.912985  -8.006011 -0.124921  3632.330078   \n",
       "2022-01-04  -4.259998  2.912985  -8.006011 -0.124921  3632.330078   \n",
       "2022-01-05  -4.259998  2.912985  -8.006011 -0.124921  3595.179932   \n",
       "2022-01-06  -4.259998  2.912985  -8.006011 -0.124921  3586.080078   \n",
       "2022-01-07  -4.259998  2.912985  -8.006011 -0.124921  3579.540039   \n",
       "...               ...       ...        ...       ...          ...   \n",
       "2024-05-22  -3.080000  4.733334  -8.927536 -0.556908  3158.540039   \n",
       "2024-05-23  -5.030001 -0.033234 -14.326404 -0.560661  3116.386963   \n",
       "2024-05-24  -1.160000  2.914569  -3.638645 -0.563569  3088.871094   \n",
       "2024-05-28  -0.300001  1.802685  -0.956636 -0.565393  3109.572021   \n",
       "2024-05-29  -0.340000 -2.962963  -1.115852 -0.566814  3111.018066   \n",
       "\n",
       "               399001.SZ        AAPL        ADBE         AMD        AVGO  \\\n",
       "Date                                                                       \n",
       "2022-01-03  14791.309570  179.273621  564.369995  150.240005   62.370541   \n",
       "2022-01-04  14791.309570  176.998352  554.000000  144.419998   63.085148   \n",
       "2022-01-05  14525.759766  172.290207  514.429993  136.149994   60.459896   \n",
       "2022-01-06  14429.509766  169.414108  514.119995  136.229996   59.898548   \n",
       "2022-01-07  14343.650391  169.581543  510.700012  132.000000   58.217331   \n",
       "...                  ...         ...         ...         ...         ...   \n",
       "2024-05-22   9693.049805  190.679291  483.929993  165.520004  138.783325   \n",
       "2024-05-23   9541.639648  186.663940  483.309998  160.429993  138.887985   \n",
       "2024-05-24   9424.580078  189.760345  475.429993  166.360001  140.338379   \n",
       "2024-05-28   9391.049805  189.770355  478.429993  171.610001  140.797913   \n",
       "2024-05-29   9414.980469  190.069992  477.600006  165.139999  138.626816   \n",
       "\n",
       "                 CSCO     IMOEX.ME        MSFT  ...  Return_000001.SS  \\\n",
       "Date                                            ...                     \n",
       "2022-01-03  57.983242  3852.500000  326.940796  ...         -0.010228   \n",
       "2022-01-04  56.561138  3873.489990  321.334717  ...         -0.010228   \n",
       "2022-01-05  55.665394  3815.050049  308.999420  ...         -0.010228   \n",
       "2022-01-06  56.256390  3753.290039  306.557678  ...         -0.002531   \n",
       "2022-01-07  56.450321  3738.679932  306.713959  ...         -0.001824   \n",
       "...               ...          ...         ...  ...               ...   \n",
       "2024-05-22  47.027111  3444.639893  429.745422  ...          0.000182   \n",
       "2024-05-23  46.204159  3442.790039  426.231750  ...         -0.013346   \n",
       "2024-05-24  46.025688  3396.500000  429.386078  ...         -0.008829   \n",
       "2024-05-28  45.886879  3302.909912  429.545776  ...         -0.004632   \n",
       "2024-05-29  45.688580  3318.030029  428.397858  ...          0.000465   \n",
       "\n",
       "            Return_399001.SZ  Return_AAPL  Return_ADBE  Return_AMD  \\\n",
       "Date                                                                 \n",
       "2022-01-03         -0.017953    -0.012692    -0.018374   -0.038738   \n",
       "2022-01-04         -0.017953    -0.012692    -0.018374   -0.038738   \n",
       "2022-01-05         -0.017953    -0.026600    -0.071426   -0.057264   \n",
       "2022-01-06         -0.006626    -0.016693    -0.000603    0.000588   \n",
       "2022-01-07         -0.005950     0.000988    -0.006652   -0.031050   \n",
       "...                      ...          ...          ...         ...   \n",
       "2024-05-22          0.001176    -0.007538     0.004317    0.005223   \n",
       "2024-05-23         -0.015620    -0.021058    -0.001281   -0.030752   \n",
       "2024-05-24         -0.012268     0.016588    -0.016304    0.036963   \n",
       "2024-05-28         -0.012274     0.000053     0.006310    0.031558   \n",
       "2024-05-29          0.002548     0.001579    -0.001735   -0.037702   \n",
       "\n",
       "            Return_AVGO  Return_CSCO  Return_IMOEX.ME  Return_MSFT  Return_MU  \\\n",
       "Date                                                                            \n",
       "2022-01-03     0.011457    -0.024526         0.005448    -0.017147   0.006162   \n",
       "2022-01-04     0.011457    -0.024526         0.005448    -0.017147   0.006162   \n",
       "2022-01-05    -0.041614    -0.015837        -0.015087    -0.038388  -0.020137   \n",
       "2022-01-06    -0.009285     0.010617        -0.016189    -0.007902   0.013242   \n",
       "2022-01-07    -0.028068     0.003447         0.000000     0.000510  -0.012546   \n",
       "...                 ...          ...              ...          ...        ...   \n",
       "2024-05-22    -0.004974     0.010439         0.004743     0.003450  -0.009569   \n",
       "2024-05-23     0.000754    -0.017500        -0.000537    -0.008176  -0.000079   \n",
       "2024-05-24     0.010443    -0.003863        -0.013446     0.007400   0.025501   \n",
       "2024-05-28     0.003274    -0.003016         0.001118     0.000372   0.024558   \n",
       "2024-05-29    -0.015420    -0.004321         0.004578    -0.002672  -0.008216   \n",
       "\n",
       "            Return_NVDA  Return_NXPI  Return_ORCL  Return_QCOM  Return_SSNLF  \\\n",
       "Date                                                                           \n",
       "2022-01-03    -0.027589     0.007136     0.010694     0.005478           0.0   \n",
       "2022-01-04    -0.027589     0.007136     0.010694     0.005478           0.0   \n",
       "2022-01-05    -0.057562    -0.035642    -0.026790    -0.003899           0.0   \n",
       "2022-01-06     0.020794     0.010643     0.002322    -0.002949           0.0   \n",
       "2022-01-07    -0.033040    -0.024806     0.013551    -0.029793           0.0   \n",
       "...                 ...          ...          ...          ...           ...   \n",
       "2024-05-22    -0.004571     0.016070    -0.000241     0.010356           0.0   \n",
       "2024-05-23     0.093196    -0.021913    -0.004093    -0.005765           0.0   \n",
       "2024-05-24     0.025723     0.016317    -0.009509     0.042625           0.0   \n",
       "2024-05-28     0.069804     0.008875     0.012855     0.012930           0.0   \n",
       "2024-05-29     0.008112    -0.020742    -0.006025    -0.022621           0.0   \n",
       "\n",
       "            Return_TSM  Return_TXN  Return_^AORD  Return_^AXJO  Return_^BFX  \\\n",
       "Date                                                                          \n",
       "2022-01-03    0.035714    0.001049     -0.003431     -0.003162     0.006212   \n",
       "2022-01-04    0.035714    0.001049     -0.003431     -0.003162     0.006212   \n",
       "2022-01-05   -0.047526   -0.020860     -0.003431     -0.003162    -0.004032   \n",
       "2022-01-06    0.011097   -0.000321     -0.027888     -0.027426    -0.008291   \n",
       "2022-01-07   -0.038686   -0.039195      0.012384      0.012911    -0.000518   \n",
       "...                ...         ...           ...           ...          ...   \n",
       "2024-05-22    0.016138    0.017738     -0.000234     -0.000459     0.001428   \n",
       "2024-05-23    0.006020   -0.025921     -0.004336     -0.004625    -0.005244   \n",
       "2024-05-24    0.018524    0.009580     -0.010380     -0.010779    -0.001464   \n",
       "2024-05-28   -0.003688    0.002109     -0.002941     -0.002773    -0.009232   \n",
       "2024-05-29   -0.031805   -0.023497     -0.012346     -0.013017    -0.006472   \n",
       "\n",
       "            Return_^BSESN  Return_^BUK100P  Return_^BVSP  Return_^DJI  \\\n",
       "Date                                                                    \n",
       "2022-01-03       0.011367         0.015950     -0.003926     0.005866   \n",
       "2022-01-04       0.011367         0.015950     -0.003926     0.005866   \n",
       "2022-01-05       0.006135         0.002944     -0.024229    -0.010667   \n",
       "2022-01-06      -0.010317        -0.010012      0.005495    -0.004687   \n",
       "2022-01-07       0.002396         0.004901      0.011402    -0.000133   \n",
       "...                   ...              ...           ...          ...   \n",
       "2024-05-22       0.003621        -0.005404     -0.013829    -0.005065   \n",
       "2024-05-23       0.016127        -0.003435     -0.007330    -0.015270   \n",
       "2024-05-24      -0.000101        -0.002510     -0.003391     0.000111   \n",
       "2024-05-28      -0.002919        -0.007706     -0.005751    -0.005547   \n",
       "2024-05-29      -0.008881        -0.009440     -0.008669    -0.010587   \n",
       "\n",
       "            Return_^FCHI  Return_^FTSE  Return_^GDAXI  Return_^GSPC  \\\n",
       "Date                                                                  \n",
       "2022-01-03      0.013882      0.001559       0.008232     -0.000630   \n",
       "2022-01-04      0.013882      0.001559       0.008232     -0.000630   \n",
       "2022-01-05      0.008057      0.001559       0.007376     -0.019393   \n",
       "2022-01-06     -0.017178     -0.008847      -0.013503     -0.000964   \n",
       "2022-01-07     -0.004163      0.004684      -0.006497     -0.004050   \n",
       "...                  ...           ...            ...           ...   \n",
       "2024-05-22     -0.006062     -0.005489      -0.002486     -0.002706   \n",
       "2024-05-23      0.001263     -0.003715       0.000595     -0.007381   \n",
       "2024-05-24     -0.000908     -0.002590       0.000110      0.007001   \n",
       "2024-05-28     -0.009184     -0.007622      -0.005158      0.000249   \n",
       "2024-05-29     -0.015236     -0.008614      -0.010953     -0.007367   \n",
       "\n",
       "            Return_^GSPTSE  Return_^HSI  Return_^IXIC  Return_^JKSE  \\\n",
       "Date                                                                  \n",
       "2022-01-03       -0.009267     0.000648     -0.013269      0.004511   \n",
       "2022-01-04       -0.009267     0.000648     -0.013269      0.004511   \n",
       "2022-01-05       -0.009267    -0.016427     -0.033448     -0.004940   \n",
       "2022-01-06        0.001545     0.007230     -0.001279     -0.001343   \n",
       "2022-01-07        0.000584     0.018226     -0.009612      0.007209   \n",
       "...                    ...          ...           ...           ...   \n",
       "2024-05-22       -0.005403    -0.001302     -0.001846      0.005058   \n",
       "2024-05-23       -0.006533    -0.017029     -0.003899      0.000000   \n",
       "2024-05-24        0.005410    -0.013767      0.011040      0.000000   \n",
       "2024-05-28       -0.004841    -0.000329      0.005856      0.010758   \n",
       "2024-05-29       -0.016488    -0.018285     -0.005834     -0.015633   \n",
       "\n",
       "            Return_^JN0U.JO  Return_^KLSE  Return_^KS11  Return_^MERV  \\\n",
       "Date                                                                    \n",
       "2022-01-03         0.015858     -0.004616     -0.011799     -0.003424   \n",
       "2022-01-04         0.015858     -0.004616     -0.011799     -0.003424   \n",
       "2022-01-05         0.015189      0.003924     -0.011799     -0.017002   \n",
       "2022-01-06        -0.012046     -0.009425     -0.011320     -0.001050   \n",
       "2022-01-07         0.002683      0.006359      0.011765      0.007809   \n",
       "...                     ...           ...           ...           ...   \n",
       "2024-05-22        -0.015814      0.000000     -0.000264     -0.011548   \n",
       "2024-05-23        -0.018132      0.004371     -0.000606     -0.031399   \n",
       "2024-05-24         0.004485     -0.006003     -0.012569      0.004645   \n",
       "2024-05-28        -0.002309     -0.001514     -0.000051      0.035907   \n",
       "2024-05-29         0.000000     -0.006480     -0.016729     -0.004879   \n",
       "\n",
       "            Return_^MXX  Return_^N100  Return_^N225  Return_^NYA  \\\n",
       "Date                                                               \n",
       "2022-01-03     0.001431      0.008330      0.001036     0.006424   \n",
       "2022-01-04     0.001431      0.008330      0.001036     0.006424   \n",
       "2022-01-05     0.000139      0.003325      0.001036    -0.012930   \n",
       "2022-01-06     0.000588     -0.016512     -0.028784     0.002567   \n",
       "2022-01-07     0.002767     -0.001575     -0.000327     0.000569   \n",
       "...                 ...           ...           ...          ...   \n",
       "2024-05-22    -0.005791     -0.002914     -0.008469    -0.005522   \n",
       "2024-05-23    -0.009102      0.003065      0.012588    -0.011415   \n",
       "2024-05-24    -0.009036     -0.000336     -0.011690     0.004556   \n",
       "2024-05-28    -0.004832     -0.005393     -0.001148    -0.005720   \n",
       "2024-05-29     0.000502     -0.011640     -0.007682    -0.011780   \n",
       "\n",
       "            Return_^NZ50  Return_^RUT  Return_^STI  Return_^STOXX50E  \\\n",
       "Date                                                                   \n",
       "2022-01-03     -0.012727    -0.001624     0.014957          0.008264   \n",
       "2022-01-04     -0.012727    -0.001624     0.014957          0.008264   \n",
       "2022-01-05     -0.012727    -0.032999    -0.005561          0.005616   \n",
       "2022-01-06     -0.012727     0.005638     0.006594         -0.015332   \n",
       "2022-01-07     -0.000952    -0.012038     0.006582         -0.004389   \n",
       "...                  ...          ...          ...               ...   \n",
       "2024-05-22      0.004821    -0.007935     0.000000         -0.004323   \n",
       "2024-05-23      0.006580    -0.015996     0.004450          0.002474   \n",
       "2024-05-24     -0.002209     0.010379    -0.001824         -0.000435   \n",
       "2024-05-28     -0.006251    -0.001362     0.003508         -0.005703   \n",
       "2024-05-29     -0.000328    -0.014834    -0.002069         -0.013349   \n",
       "\n",
       "            Return_^TA125.TA  Return_^TWII  Return_^VIX  Return_^XAX  \\\n",
       "Date                                                                   \n",
       "2022-01-03          0.006637      0.014003     0.018675     0.009917   \n",
       "2022-01-04          0.000805      0.014003     0.018675     0.009917   \n",
       "2022-01-05          0.008116     -0.001424     0.166765    -0.007709   \n",
       "2022-01-06         -0.008069     -0.007137    -0.006082     0.022363   \n",
       "2022-01-07          0.000000     -0.010788    -0.043345     0.001661   \n",
       "...                      ...           ...          ...          ...   \n",
       "2024-05-22         -0.004080      0.014837     0.036256    -0.017918   \n",
       "2024-05-23         -0.003100      0.002580     0.039056    -0.006732   \n",
       "2024-05-24          0.000000     -0.001948    -0.065779     0.013413   \n",
       "2024-05-28          0.008976      0.002506     0.082984     0.016470   \n",
       "2024-05-29          0.002357     -0.008963     0.105263    -0.016179   \n",
       "\n",
       "            Target_BASE  Target  \n",
       "Date                             \n",
       "2022-01-03     1.559998     1.0  \n",
       "2022-01-04     1.559998     1.0  \n",
       "2022-01-05    -0.430000     0.0  \n",
       "2022-01-06    -0.320000     0.0  \n",
       "2022-01-07    -0.600002     0.0  \n",
       "...                 ...     ...  \n",
       "2024-05-22    -0.240000     0.0  \n",
       "2024-05-23    -0.490000     0.0  \n",
       "2024-05-24    -1.370001     0.0  \n",
       "2024-05-28     0.429998     1.0  \n",
       "2024-05-29     0.119999     1.0  \n",
       "\n",
       "[604 rows x 139 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After data preparation and target set I can go with the modeling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V8CXdN61SOm_"
   },
   "source": [
    "# ROLLING WINDOW CROSS-VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mTSYY13D7c4J",
    "outputId": "24e96a10-a833-458f-817d-b46db9222d6f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model CART, Train Acc: 0.968, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.3181818181818182\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model RF, Train Acc: 0.968, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 14, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.36363636363636365\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model XGB, Train Acc: 0.968, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model XGB, Train Acc: 0.992, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9921875, Test Acc: 0.5238095238095238\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9921259842519685, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9920634920634921, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.8571428571428571\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model XGB, Train Acc: 0.991869918699187, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9919354838709677, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.36363636363636365\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model LDA, Train Acc: 0.64, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6290322580645161, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6015625, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model LDA, Train Acc: 0.64, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6796875, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model LDA, Train Acc: 0.7007874015748031, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6904761904761905, Test Acc: 0.7272727272727273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6850393700787402, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model LDA, Train Acc: 0.7479674796747967, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model LDA, Train Acc: 0.744, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model LDA, Train Acc: 0.7398373983739838, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model LDA, Train Acc: 0.7580645161290323, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model LDA, Train Acc: 0.7258064516129032, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model LDA, Train Acc: 0.717741935483871, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6929133858267716, Test Acc: 0.55\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6428571428571429, Test Acc: 0.782608695652174\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6953125, Test Acc: 0.45\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6190476190476191, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6614173228346457, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6456692913385826, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6290322580645161, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model LDA, Train Acc: 0.64, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 1, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 1, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 2, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 2, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [ 30  32  35 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  32  35 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 30  32  35 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  32  35 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (ChiSquareFS): [ 30  32  35 106 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  32  35 106 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [ 30  31  32  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  31  32  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 8, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 8, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "Selected features (ChiSquareFS): [30 32 33 35 55]\n",
      "Fold 9, VarNum 5, Method CHI2, Model CART, Selected Features: [30 32 33 35 55]\n",
      "Fold 9, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [30 32 33 35 36]\n",
      "Fold 12, VarNum 5, Method CHI2, Model CART, Selected Features: [30 32 33 35 36]\n",
      "Fold 12, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model CART, Selected Features: [21 30 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model CART, Selected Features: [21 30 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.45\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model CART, Selected Features: [21 30 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model CART, Selected Features: [21 30 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [ 21  30  32  35 106]\n",
      "Fold 17, VarNum 5, Method CHI2, Model CART, Selected Features: [ 21  30  32  35 106]\n",
      "Fold 17, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 30  32  35 122 127]\n",
      "Fold 18, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  32  35 122 127]\n",
      "Fold 18, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [30 32 33 35 38]\n",
      "Fold 19, VarNum 5, Method CHI2, Model CART, Selected Features: [30 32 33 35 38]\n",
      "Fold 19, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [21 30 31 32 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model CART, Selected Features: [21 30 31 32 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [30 31 32 35 55]\n",
      "Fold 21, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 35 55]\n",
      "Fold 21, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.36363636363636365\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 121 127 129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 1, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 1, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 2, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 2, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.782608695652174\n",
      "Selected features (ChiSquareFS): [ 30  32  35 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  32  35 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 30  32  35 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  32  35 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [ 30  32  35 106 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  32  35 106 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.8181818181818182\n",
      "Selected features (ChiSquareFS): [ 30  31  32  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  31  32  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 8, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 8, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.8571428571428571\n",
      "Selected features (ChiSquareFS): [30 32 33 35 55]\n",
      "Fold 9, VarNum 5, Method CHI2, Model RF, Selected Features: [30 32 33 35 55]\n",
      "Fold 9, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [30 32 33 35 36]\n",
      "Fold 12, VarNum 5, Method CHI2, Model RF, Selected Features: [30 32 33 35 36]\n",
      "Fold 12, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model RF, Selected Features: [21 30 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model RF, Selected Features: [21 30 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model RF, Selected Features: [21 30 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model RF, Selected Features: [21 30 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [ 21  30  32  35 106]\n",
      "Fold 17, VarNum 5, Method CHI2, Model RF, Selected Features: [ 21  30  32  35 106]\n",
      "Fold 17, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (ChiSquareFS): [ 30  32  35 122 127]\n",
      "Fold 18, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  32  35 122 127]\n",
      "Fold 18, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [30 32 33 35 38]\n",
      "Fold 19, VarNum 5, Method CHI2, Model RF, Selected Features: [30 32 33 35 38]\n",
      "Fold 19, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [21 30 31 32 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model RF, Selected Features: [21 30 31 32 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [30 31 32 35 55]\n",
      "Fold 21, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 35 55]\n",
      "Fold 21, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 1, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 1, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 2, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 2, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 30  32  35 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  32  35 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  32  35 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  32  35 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [ 30  32  35 106 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  32  35 106 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.8181818181818182\n",
      "Selected features (ChiSquareFS): [ 30  31  32  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  31  32  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 8, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 8, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "Selected features (ChiSquareFS): [30 32 33 35 55]\n",
      "Fold 9, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 32 33 35 55]\n",
      "Fold 9, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.991869918699187, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9919354838709677, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [30 32 33 35 36]\n",
      "Fold 12, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 32 33 35 36]\n",
      "Fold 12, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model XGB, Selected Features: [21 30 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model XGB, Selected Features: [21 30 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9921259842519685, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model XGB, Selected Features: [21 30 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model XGB, Selected Features: [21 30 32 33 35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 16, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [ 21  30  32  35 106]\n",
      "Fold 17, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 21  30  32  35 106]\n",
      "Fold 17, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 30  32  35 122 127]\n",
      "Fold 18, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  32  35 122 127]\n",
      "Fold 18, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [30 32 33 35 38]\n",
      "Fold 19, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 32 33 35 38]\n",
      "Fold 19, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [21 30 31 32 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model XGB, Selected Features: [21 30 31 32 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [30 31 32 35 55]\n",
      "Fold 21, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 35 55]\n",
      "Fold 21, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.2727272727272727\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.704, Test Acc: 0.8095238095238095\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 1, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 1, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7096774193548387, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 2, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 2, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7109375, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.696, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 30  32  35 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  32  35 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6953125, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  32  35 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  32  35 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7165354330708661, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 30  32  35 106 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  32  35 106 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7380952380952381, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 30  31  32  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  31  32  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7401574803149606, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 8, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 8, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7560975609756098, Test Acc: 0.9047619047619048\n",
      "Selected features (ChiSquareFS): [30 32 33 35 55]\n",
      "Fold 9, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 32 33 35 55]\n",
      "Fold 9, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.736, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7398373983739838, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7741935483870968, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [30 32 33 35 36]\n",
      "Fold 12, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 32 33 35 36]\n",
      "Fold 12, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.75, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model LDA, Selected Features: [21 30 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.717741935483871, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model LDA, Selected Features: [21 30 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6929133858267716, Test Acc: 0.55\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model LDA, Selected Features: [21 30 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6428571428571429, Test Acc: 0.782608695652174\n",
      "Selected features (ChiSquareFS): [21 30 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model LDA, Selected Features: [21 30 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6953125, Test Acc: 0.45\n",
      "Selected features (ChiSquareFS): [ 21  30  32  35 106]\n",
      "Fold 17, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 21  30  32  35 106]\n",
      "Fold 17, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6746031746031746, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [ 30  32  35 122 127]\n",
      "Fold 18, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  32  35 122 127]\n",
      "Fold 18, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6771653543307087, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [30 32 33 35 38]\n",
      "Fold 19, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 32 33 35 38]\n",
      "Fold 19, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6456692913385826, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [21 30 31 32 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model LDA, Selected Features: [21 30 31 32 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6693548387096774, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [30 31 32 35 55]\n",
      "Fold 21, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 35 55]\n",
      "Fold 21, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.656, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [129 134 106  30 107]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387]\n",
      "Fold 0, VarNum 5, Method RF, Model CART, Selected Features: [129 134 106  30 107]\n",
      "Fold 0, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [134 107 106 100 129]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054]\n",
      "Fold 1, VarNum 5, Method RF, Model CART, Selected Features: [134 107 106 100 129]\n",
      "Fold 1, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [107  30 105  33 129]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624]\n",
      "Fold 2, VarNum 5, Method RF, Model CART, Selected Features: [107  30 105  33 129]\n",
      "Fold 2, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [107 106 122 127  30]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713]\n",
      "Fold 3, VarNum 5, Method RF, Model CART, Selected Features: [107 106 122 127  30]\n",
      "Fold 3, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.43478260869565216\n",
      "Selected features (RF): [106 107  32  30  33]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174  ]\n",
      "Fold 4, VarNum 5, Method RF, Model CART, Selected Features: [106 107  32  30  33]\n",
      "Fold 4, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [107 127 106 109  30]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567]\n",
      "Fold 5, VarNum 5, Method RF, Model CART, Selected Features: [107 127 106 109  30]\n",
      "Fold 5, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [106 107 127 109  32]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486]\n",
      "Fold 6, VarNum 5, Method RF, Model CART, Selected Features: [106 107 127 109  32]\n",
      "Fold 6, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [107 106 127  30 118]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774]\n",
      "Fold 7, VarNum 5, Method RF, Model CART, Selected Features: [107 106 127  30 118]\n",
      "Fold 7, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [107 106  30 109  33]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996]\n",
      "Fold 8, VarNum 5, Method RF, Model CART, Selected Features: [107 106  30 109  33]\n",
      "Fold 8, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 132  35 106]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646]\n",
      "Fold 9, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 132  35 106]\n",
      "Fold 9, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [30 32 33 35 57]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143 ]\n",
      "Fold 10, VarNum 5, Method RF, Model CART, Selected Features: [30 32 33 35 57]\n",
      "Fold 10, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [30 33 32 35 21]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159]\n",
      "Fold 11, VarNum 5, Method RF, Model CART, Selected Features: [30 33 32 35 21]\n",
      "Fold 11, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [33 35 30 21 32]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812]\n",
      "Fold 12, VarNum 5, Method RF, Model CART, Selected Features: [33 35 30 21 32]\n",
      "Fold 12, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [33 30 32 21 35]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099  ]\n",
      "Fold 13, VarNum 5, Method RF, Model CART, Selected Features: [33 30 32 21 35]\n",
      "Fold 13, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 33  30  35  32 127]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125]\n",
      "Fold 14, VarNum 5, Method RF, Model CART, Selected Features: [ 33  30  35  32 127]\n",
      "Fold 14, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.35\n",
      "Selected features (RF): [ 33  30 110  21 101]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285]\n",
      "Fold 15, VarNum 5, Method RF, Model CART, Selected Features: [ 33  30 110  21 101]\n",
      "Fold 15, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "Selected features (RF): [ 33 105  30  21  32]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172]\n",
      "Fold 16, VarNum 5, Method RF, Model CART, Selected Features: [ 33 105  30  21  32]\n",
      "Fold 16, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [122 113  30  21 134]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434]\n",
      "Fold 17, VarNum 5, Method RF, Model CART, Selected Features: [122 113  30  21 134]\n",
      "Fold 17, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [122 113  91 114  33]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344]\n",
      "Fold 18, VarNum 5, Method RF, Model CART, Selected Features: [122 113  91 114  33]\n",
      "Fold 18, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (RF): [134  92 122  32  33]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174]\n",
      "Fold 19, VarNum 5, Method RF, Model CART, Selected Features: [134  92 122  32  33]\n",
      "Fold 19, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30  32 134  33  91]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132 ]\n",
      "Fold 20, VarNum 5, Method RF, Model CART, Selected Features: [ 30  32 134  33  91]\n",
      "Fold 20, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30 136 100  73  98]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984]\n",
      "Fold 21, VarNum 5, Method RF, Model CART, Selected Features: [ 30 136 100  73  98]\n",
      "Fold 21, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [129 134 106  30 107]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387]\n",
      "Fold 0, VarNum 5, Method RF, Model RF, Selected Features: [129 134 106  30 107]\n",
      "Fold 0, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [134 107 106 100 129]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054]\n",
      "Fold 1, VarNum 5, Method RF, Model RF, Selected Features: [134 107 106 100 129]\n",
      "Fold 1, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [107  30 105  33 129]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624]\n",
      "Fold 2, VarNum 5, Method RF, Model RF, Selected Features: [107  30 105  33 129]\n",
      "Fold 2, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [107 106 122 127  30]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713]\n",
      "Fold 3, VarNum 5, Method RF, Model RF, Selected Features: [107 106 122 127  30]\n",
      "Fold 3, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [106 107  32  30  33]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174  ]\n",
      "Fold 4, VarNum 5, Method RF, Model RF, Selected Features: [106 107  32  30  33]\n",
      "Fold 4, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [107 127 106 109  30]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567]\n",
      "Fold 5, VarNum 5, Method RF, Model RF, Selected Features: [107 127 106 109  30]\n",
      "Fold 5, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [106 107 127 109  32]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486]\n",
      "Fold 6, VarNum 5, Method RF, Model RF, Selected Features: [106 107 127 109  32]\n",
      "Fold 6, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [107 106 127  30 118]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774]\n",
      "Fold 7, VarNum 5, Method RF, Model RF, Selected Features: [107 106 127  30 118]\n",
      "Fold 7, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [107 106  30 109  33]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996]\n",
      "Fold 8, VarNum 5, Method RF, Model RF, Selected Features: [107 106  30 109  33]\n",
      "Fold 8, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30 107 132  35 106]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646]\n",
      "Fold 9, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 132  35 106]\n",
      "Fold 9, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [30 32 33 35 57]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143 ]\n",
      "Fold 10, VarNum 5, Method RF, Model RF, Selected Features: [30 32 33 35 57]\n",
      "Fold 10, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [30 33 32 35 21]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159]\n",
      "Fold 11, VarNum 5, Method RF, Model RF, Selected Features: [30 33 32 35 21]\n",
      "Fold 11, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (RF): [33 35 30 21 32]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812]\n",
      "Fold 12, VarNum 5, Method RF, Model RF, Selected Features: [33 35 30 21 32]\n",
      "Fold 12, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [33 30 32 21 35]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099  ]\n",
      "Fold 13, VarNum 5, Method RF, Model RF, Selected Features: [33 30 32 21 35]\n",
      "Fold 13, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [ 33  30  35  32 127]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125]\n",
      "Fold 14, VarNum 5, Method RF, Model RF, Selected Features: [ 33  30  35  32 127]\n",
      "Fold 14, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [ 33  30 110  21 101]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285]\n",
      "Fold 15, VarNum 5, Method RF, Model RF, Selected Features: [ 33  30 110  21 101]\n",
      "Fold 15, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [ 33 105  30  21  32]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172]\n",
      "Fold 16, VarNum 5, Method RF, Model RF, Selected Features: [ 33 105  30  21  32]\n",
      "Fold 16, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (RF): [122 113  30  21 134]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434]\n",
      "Fold 17, VarNum 5, Method RF, Model RF, Selected Features: [122 113  30  21 134]\n",
      "Fold 17, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (RF): [122 113  91 114  33]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344]\n",
      "Fold 18, VarNum 5, Method RF, Model RF, Selected Features: [122 113  91 114  33]\n",
      "Fold 18, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [134  92 122  32  33]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174]\n",
      "Fold 19, VarNum 5, Method RF, Model RF, Selected Features: [134  92 122  32  33]\n",
      "Fold 19, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (RF): [ 30  32 134  33  91]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132 ]\n",
      "Fold 20, VarNum 5, Method RF, Model RF, Selected Features: [ 30  32 134  33  91]\n",
      "Fold 20, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 136 100  73  98]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984]\n",
      "Fold 21, VarNum 5, Method RF, Model RF, Selected Features: [ 30 136 100  73  98]\n",
      "Fold 21, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [129 134 106  30 107]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387]\n",
      "Fold 0, VarNum 5, Method RF, Model XGB, Selected Features: [129 134 106  30 107]\n",
      "Fold 0, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [134 107 106 100 129]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054]\n",
      "Fold 1, VarNum 5, Method RF, Model XGB, Selected Features: [134 107 106 100 129]\n",
      "Fold 1, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (RF): [107  30 105  33 129]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624]\n",
      "Fold 2, VarNum 5, Method RF, Model XGB, Selected Features: [107  30 105  33 129]\n",
      "Fold 2, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [107 106 122 127  30]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713]\n",
      "Fold 3, VarNum 5, Method RF, Model XGB, Selected Features: [107 106 122 127  30]\n",
      "Fold 3, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [106 107  32  30  33]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174  ]\n",
      "Fold 4, VarNum 5, Method RF, Model XGB, Selected Features: [106 107  32  30  33]\n",
      "Fold 4, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [107 127 106 109  30]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567]\n",
      "Fold 5, VarNum 5, Method RF, Model XGB, Selected Features: [107 127 106 109  30]\n",
      "Fold 5, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [106 107 127 109  32]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486]\n",
      "Fold 6, VarNum 5, Method RF, Model XGB, Selected Features: [106 107 127 109  32]\n",
      "Fold 6, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [107 106 127  30 118]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774]\n",
      "Fold 7, VarNum 5, Method RF, Model XGB, Selected Features: [107 106 127  30 118]\n",
      "Fold 7, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [107 106  30 109  33]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996]\n",
      "Fold 8, VarNum 5, Method RF, Model XGB, Selected Features: [107 106  30 109  33]\n",
      "Fold 8, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107 132  35 106]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646]\n",
      "Fold 9, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 132  35 106]\n",
      "Fold 9, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [30 32 33 35 57]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143 ]\n",
      "Fold 10, VarNum 5, Method RF, Model XGB, Selected Features: [30 32 33 35 57]\n",
      "Fold 10, VarNum 5, Method RF, Model XGB, Train Acc: 0.991869918699187, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [30 33 32 35 21]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159]\n",
      "Fold 11, VarNum 5, Method RF, Model XGB, Selected Features: [30 33 32 35 21]\n",
      "Fold 11, VarNum 5, Method RF, Model XGB, Train Acc: 0.9919354838709677, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [33 35 30 21 32]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812]\n",
      "Fold 12, VarNum 5, Method RF, Model XGB, Selected Features: [33 35 30 21 32]\n",
      "Fold 12, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [33 30 32 21 35]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099  ]\n",
      "Fold 13, VarNum 5, Method RF, Model XGB, Selected Features: [33 30 32 21 35]\n",
      "Fold 13, VarNum 5, Method RF, Model XGB, Train Acc: 0.9919354838709677, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 33  30  35  32 127]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125]\n",
      "Fold 14, VarNum 5, Method RF, Model XGB, Selected Features: [ 33  30  35  32 127]\n",
      "Fold 14, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (RF): [ 33  30 110  21 101]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285]\n",
      "Fold 15, VarNum 5, Method RF, Model XGB, Selected Features: [ 33  30 110  21 101]\n",
      "Fold 15, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (RF): [ 33 105  30  21  32]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172]\n",
      "Fold 16, VarNum 5, Method RF, Model XGB, Selected Features: [ 33 105  30  21  32]\n",
      "Fold 16, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.75\n",
      "Selected features (RF): [122 113  30  21 134]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434]\n",
      "Fold 17, VarNum 5, Method RF, Model XGB, Selected Features: [122 113  30  21 134]\n",
      "Fold 17, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [122 113  91 114  33]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344]\n",
      "Fold 18, VarNum 5, Method RF, Model XGB, Selected Features: [122 113  91 114  33]\n",
      "Fold 18, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (RF): [134  92 122  32  33]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174]\n",
      "Fold 19, VarNum 5, Method RF, Model XGB, Selected Features: [134  92 122  32  33]\n",
      "Fold 19, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [ 30  32 134  33  91]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132 ]\n",
      "Fold 20, VarNum 5, Method RF, Model XGB, Selected Features: [ 30  32 134  33  91]\n",
      "Fold 20, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [ 30 136 100  73  98]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984]\n",
      "Fold 21, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 136 100  73  98]\n",
      "Fold 21, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [129 134 106  30 107]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387]\n",
      "Fold 0, VarNum 5, Method RF, Model LDA, Selected Features: [129 134 106  30 107]\n",
      "Fold 0, VarNum 5, Method RF, Model LDA, Train Acc: 0.648, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [134 107 106 100 129]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054]\n",
      "Fold 1, VarNum 5, Method RF, Model LDA, Selected Features: [134 107 106 100 129]\n",
      "Fold 1, VarNum 5, Method RF, Model LDA, Train Acc: 0.7096774193548387, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [107  30 105  33 129]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624]\n",
      "Fold 2, VarNum 5, Method RF, Model LDA, Selected Features: [107  30 105  33 129]\n",
      "Fold 2, VarNum 5, Method RF, Model LDA, Train Acc: 0.71875, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [107 106 122 127  30]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713]\n",
      "Fold 3, VarNum 5, Method RF, Model LDA, Selected Features: [107 106 122 127  30]\n",
      "Fold 3, VarNum 5, Method RF, Model LDA, Train Acc: 0.704, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [106 107  32  30  33]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174  ]\n",
      "Fold 4, VarNum 5, Method RF, Model LDA, Selected Features: [106 107  32  30  33]\n",
      "Fold 4, VarNum 5, Method RF, Model LDA, Train Acc: 0.71875, Test Acc: 0.8571428571428571\n",
      "Selected features (RF): [107 127 106 109  30]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567]\n",
      "Fold 5, VarNum 5, Method RF, Model LDA, Selected Features: [107 127 106 109  30]\n",
      "Fold 5, VarNum 5, Method RF, Model LDA, Train Acc: 0.7401574803149606, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [106 107 127 109  32]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486]\n",
      "Fold 6, VarNum 5, Method RF, Model LDA, Selected Features: [106 107 127 109  32]\n",
      "Fold 6, VarNum 5, Method RF, Model LDA, Train Acc: 0.746031746031746, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [107 106 127  30 118]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774]\n",
      "Fold 7, VarNum 5, Method RF, Model LDA, Selected Features: [107 106 127  30 118]\n",
      "Fold 7, VarNum 5, Method RF, Model LDA, Train Acc: 0.7637795275590551, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [107 106  30 109  33]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996]\n",
      "Fold 8, VarNum 5, Method RF, Model LDA, Selected Features: [107 106  30 109  33]\n",
      "Fold 8, VarNum 5, Method RF, Model LDA, Train Acc: 0.8048780487804879, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107 132  35 106]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646]\n",
      "Fold 9, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 132  35 106]\n",
      "Fold 9, VarNum 5, Method RF, Model LDA, Train Acc: 0.712, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [30 32 33 35 57]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143 ]\n",
      "Fold 10, VarNum 5, Method RF, Model LDA, Selected Features: [30 32 33 35 57]\n",
      "Fold 10, VarNum 5, Method RF, Model LDA, Train Acc: 0.7154471544715447, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [30 33 32 35 21]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159]\n",
      "Fold 11, VarNum 5, Method RF, Model LDA, Selected Features: [30 33 32 35 21]\n",
      "Fold 11, VarNum 5, Method RF, Model LDA, Train Acc: 0.7580645161290323, Test Acc: 0.5263157894736842\n",
      "Selected features (RF): [33 35 30 21 32]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812]\n",
      "Fold 12, VarNum 5, Method RF, Model LDA, Selected Features: [33 35 30 21 32]\n",
      "Fold 12, VarNum 5, Method RF, Model LDA, Train Acc: 0.7258064516129032, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [33 30 32 21 35]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099  ]\n",
      "Fold 13, VarNum 5, Method RF, Model LDA, Selected Features: [33 30 32 21 35]\n",
      "Fold 13, VarNum 5, Method RF, Model LDA, Train Acc: 0.717741935483871, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 33  30  35  32 127]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125]\n",
      "Fold 14, VarNum 5, Method RF, Model LDA, Selected Features: [ 33  30  35  32 127]\n",
      "Fold 14, VarNum 5, Method RF, Model LDA, Train Acc: 0.6929133858267716, Test Acc: 0.6\n",
      "Selected features (RF): [ 33  30 110  21 101]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285]\n",
      "Fold 15, VarNum 5, Method RF, Model LDA, Selected Features: [ 33  30 110  21 101]\n",
      "Fold 15, VarNum 5, Method RF, Model LDA, Train Acc: 0.6507936507936508, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [ 33 105  30  21  32]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172]\n",
      "Fold 16, VarNum 5, Method RF, Model LDA, Selected Features: [ 33 105  30  21  32]\n",
      "Fold 16, VarNum 5, Method RF, Model LDA, Train Acc: 0.6796875, Test Acc: 0.6\n",
      "Selected features (RF): [122 113  30  21 134]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434]\n",
      "Fold 17, VarNum 5, Method RF, Model LDA, Selected Features: [122 113  30  21 134]\n",
      "Fold 17, VarNum 5, Method RF, Model LDA, Train Acc: 0.6904761904761905, Test Acc: 0.7\n",
      "Selected features (RF): [122 113  91 114  33]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344]\n",
      "Fold 18, VarNum 5, Method RF, Model LDA, Selected Features: [122 113  91 114  33]\n",
      "Fold 18, VarNum 5, Method RF, Model LDA, Train Acc: 0.7165354330708661, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [134  92 122  32  33]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174]\n",
      "Fold 19, VarNum 5, Method RF, Model LDA, Selected Features: [134  92 122  32  33]\n",
      "Fold 19, VarNum 5, Method RF, Model LDA, Train Acc: 0.6692913385826772, Test Acc: 0.47368421052631576\n",
      "Selected features (RF): [ 30  32 134  33  91]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132 ]\n",
      "Fold 20, VarNum 5, Method RF, Model LDA, Selected Features: [ 30  32 134  33  91]\n",
      "Fold 20, VarNum 5, Method RF, Model LDA, Train Acc: 0.6209677419354839, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 136 100  73  98]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984]\n",
      "Fold 21, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 136 100  73  98]\n",
      "Fold 21, VarNum 5, Method RF, Model LDA, Train Acc: 0.704, Test Acc: 0.7272727272727273\n",
      "[15:21:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202]\n",
      "Fold 0, VarNum 5, Method XGB, Model CART, Selected Features: [ 54  92  67 105  46]\n",
      "Fold 0, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.38095238095238093\n",
      "[15:21:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 1 52 49 43 97]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752]\n",
      "Fold 1, VarNum 5, Method XGB, Model CART, Selected Features: [ 1 52 49 43 97]\n",
      "Fold 1, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "[15:21:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024]\n",
      "Fold 2, VarNum 5, Method XGB, Model CART, Selected Features: [ 54  93  92 127  35]\n",
      "Fold 2, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [127  19 112 111  71]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887]\n",
      "Fold 3, VarNum 5, Method XGB, Model CART, Selected Features: [127  19 112 111  71]\n",
      "Fold 3, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.43478260869565216\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301]\n",
      "Fold 4, VarNum 5, Method XGB, Model CART, Selected Features: [ 32  61 106  29  81]\n",
      "Fold 4, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [37 63 23 72  6]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091]\n",
      "Fold 5, VarNum 5, Method XGB, Model CART, Selected Features: [37 63 23 72  6]\n",
      "Fold 5, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347]\n",
      "Fold 6, VarNum 5, Method XGB, Model CART, Selected Features: [107 105 122  52  63]\n",
      "Fold 6, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871]\n",
      "Fold 7, VarNum 5, Method XGB, Model CART, Selected Features: [  1 107  80 115  37]\n",
      "Fold 7, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188]\n",
      "Fold 8, VarNum 5, Method XGB, Model CART, Selected Features: [108  41 110  35 107]\n",
      "Fold 8, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.38095238095238093\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769]\n",
      "Fold 9, VarNum 5, Method XGB, Model CART, Selected Features: [126 112  27  70  35]\n",
      "Fold 9, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013]\n",
      "Fold 10, VarNum 5, Method XGB, Model CART, Selected Features: [ 35  50 132  33  69]\n",
      "Fold 10, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "[15:21:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683]\n",
      "Fold 11, VarNum 5, Method XGB, Model CART, Selected Features: [ 33  92  24 131  28]\n",
      "Fold 11, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [33 25 17  8 21]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572]\n",
      "Fold 12, VarNum 5, Method XGB, Model CART, Selected Features: [33 25 17  8 21]\n",
      "Fold 12, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192]\n",
      "Fold 13, VarNum 5, Method XGB, Model CART, Selected Features: [125  33  47  26  65]\n",
      "Fold 13, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805]\n",
      "Fold 14, VarNum 5, Method XGB, Model CART, Selected Features: [ 33  63  42 108  51]\n",
      "Fold 14, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [89 30 73 69 22]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143  ]\n",
      "Fold 15, VarNum 5, Method XGB, Model CART, Selected Features: [89 30 73 69 22]\n",
      "Fold 15, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619]\n",
      "Fold 16, VarNum 5, Method XGB, Model CART, Selected Features: [115  88  32 109   7]\n",
      "Fold 16, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.3\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458]\n",
      "Fold 17, VarNum 5, Method XGB, Model CART, Selected Features: [112   2  36 113  74]\n",
      "Fold 17, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386]\n",
      "Fold 18, VarNum 5, Method XGB, Model CART, Selected Features: [112  31 110 130  32]\n",
      "Fold 18, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978]\n",
      "Fold 19, VarNum 5, Method XGB, Model CART, Selected Features: [ 16  12  89  80 131]\n",
      "Fold 19, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441]\n",
      "Fold 20, VarNum 5, Method XGB, Model CART, Selected Features: [119  48   8  25 135]\n",
      "Fold 20, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "[15:21:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 30  15 129 105 109]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843]\n",
      "Fold 21, VarNum 5, Method XGB, Model CART, Selected Features: [ 30  15 129 105 109]\n",
      "Fold 21, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:21:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202]\n",
      "Fold 0, VarNum 5, Method XGB, Model RF, Selected Features: [ 54  92  67 105  46]\n",
      "Fold 0, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.38095238095238093\n",
      "[15:21:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 1 52 49 43 97]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752]\n",
      "Fold 1, VarNum 5, Method XGB, Model RF, Selected Features: [ 1 52 49 43 97]\n",
      "Fold 1, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "[15:21:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024]\n",
      "Fold 2, VarNum 5, Method XGB, Model RF, Selected Features: [ 54  93  92 127  35]\n",
      "Fold 2, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:21:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887]\n",
      "Fold 3, VarNum 5, Method XGB, Model RF, Selected Features: [127  19 112 111  71]\n",
      "Fold 3, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "[15:21:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301]\n",
      "Fold 4, VarNum 5, Method XGB, Model RF, Selected Features: [ 32  61 106  29  81]\n",
      "Fold 4, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:21:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [37 63 23 72  6]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091]\n",
      "Fold 5, VarNum 5, Method XGB, Model RF, Selected Features: [37 63 23 72  6]\n",
      "Fold 5, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:21:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347]\n",
      "Fold 6, VarNum 5, Method XGB, Model RF, Selected Features: [107 105 122  52  63]\n",
      "Fold 6, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:21:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871]\n",
      "Fold 7, VarNum 5, Method XGB, Model RF, Selected Features: [  1 107  80 115  37]\n",
      "Fold 7, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "[15:21:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188]\n",
      "Fold 8, VarNum 5, Method XGB, Model RF, Selected Features: [108  41 110  35 107]\n",
      "Fold 8, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "[15:21:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769]\n",
      "Fold 9, VarNum 5, Method XGB, Model RF, Selected Features: [126 112  27  70  35]\n",
      "Fold 9, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:21:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013]\n",
      "Fold 10, VarNum 5, Method XGB, Model RF, Selected Features: [ 35  50 132  33  69]\n",
      "Fold 10, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "[15:21:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683]\n",
      "Fold 11, VarNum 5, Method XGB, Model RF, Selected Features: [ 33  92  24 131  28]\n",
      "Fold 11, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:21:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [33 25 17  8 21]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572]\n",
      "Fold 12, VarNum 5, Method XGB, Model RF, Selected Features: [33 25 17  8 21]\n",
      "Fold 12, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:21:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192]\n",
      "Fold 13, VarNum 5, Method XGB, Model RF, Selected Features: [125  33  47  26  65]\n",
      "Fold 13, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:21:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805]\n",
      "Fold 14, VarNum 5, Method XGB, Model RF, Selected Features: [ 33  63  42 108  51]\n",
      "Fold 14, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.55\n",
      "[15:21:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [89 30 73 69 22]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143  ]\n",
      "Fold 15, VarNum 5, Method XGB, Model RF, Selected Features: [89 30 73 69 22]\n",
      "Fold 15, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "[15:21:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619]\n",
      "Fold 16, VarNum 5, Method XGB, Model RF, Selected Features: [115  88  32 109   7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 16, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.45\n",
      "[15:21:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458]\n",
      "Fold 17, VarNum 5, Method XGB, Model RF, Selected Features: [112   2  36 113  74]\n",
      "Fold 17, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:21:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386]\n",
      "Fold 18, VarNum 5, Method XGB, Model RF, Selected Features: [112  31 110 130  32]\n",
      "Fold 18, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:21:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978]\n",
      "Fold 19, VarNum 5, Method XGB, Model RF, Selected Features: [ 16  12  89  80 131]\n",
      "Fold 19, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "[15:21:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441]\n",
      "Fold 20, VarNum 5, Method XGB, Model RF, Selected Features: [119  48   8  25 135]\n",
      "Fold 20, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:21:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843]\n",
      "Fold 21, VarNum 5, Method XGB, Model RF, Selected Features: [ 30  15 129 105 109]\n",
      "Fold 21, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.8181818181818182\n",
      "[15:21:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202]\n",
      "Fold 0, VarNum 5, Method XGB, Model XGB, Selected Features: [ 54  92  67 105  46]\n",
      "Fold 0, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "[15:21:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 1 52 49 43 97]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752]\n",
      "Fold 1, VarNum 5, Method XGB, Model XGB, Selected Features: [ 1 52 49 43 97]\n",
      "Fold 1, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "[15:21:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024]\n",
      "Fold 2, VarNum 5, Method XGB, Model XGB, Selected Features: [ 54  93  92 127  35]\n",
      "Fold 2, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:21:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887]\n",
      "Fold 3, VarNum 5, Method XGB, Model XGB, Selected Features: [127  19 112 111  71]\n",
      "Fold 3, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "[15:21:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301]\n",
      "Fold 4, VarNum 5, Method XGB, Model XGB, Selected Features: [ 32  61 106  29  81]\n",
      "Fold 4, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:21:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [37 63 23 72  6]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091]\n",
      "Fold 5, VarNum 5, Method XGB, Model XGB, Selected Features: [37 63 23 72  6]\n",
      "Fold 5, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:21:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347]\n",
      "Fold 6, VarNum 5, Method XGB, Model XGB, Selected Features: [107 105 122  52  63]\n",
      "Fold 6, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:21:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871]\n",
      "Fold 7, VarNum 5, Method XGB, Model XGB, Selected Features: [  1 107  80 115  37]\n",
      "Fold 7, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "[15:21:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188]\n",
      "Fold 8, VarNum 5, Method XGB, Model XGB, Selected Features: [108  41 110  35 107]\n",
      "Fold 8, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.47619047619047616\n",
      "[15:21:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769]\n",
      "Fold 9, VarNum 5, Method XGB, Model XGB, Selected Features: [126 112  27  70  35]\n",
      "Fold 9, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "[15:21:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013]\n",
      "Fold 10, VarNum 5, Method XGB, Model XGB, Selected Features: [ 35  50 132  33  69]\n",
      "Fold 10, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "[15:21:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683]\n",
      "Fold 11, VarNum 5, Method XGB, Model XGB, Selected Features: [ 33  92  24 131  28]\n",
      "Fold 11, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:21:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [33 25 17  8 21]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572]\n",
      "Fold 12, VarNum 5, Method XGB, Model XGB, Selected Features: [33 25 17  8 21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 12, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:21:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192]\n",
      "Fold 13, VarNum 5, Method XGB, Model XGB, Selected Features: [125  33  47  26  65]\n",
      "Fold 13, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805]\n",
      "Fold 14, VarNum 5, Method XGB, Model XGB, Selected Features: [ 33  63  42 108  51]\n",
      "Fold 14, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.45\n",
      "[15:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [89 30 73 69 22]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143  ]\n",
      "Fold 15, VarNum 5, Method XGB, Model XGB, Selected Features: [89 30 73 69 22]\n",
      "Fold 15, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619]\n",
      "Fold 16, VarNum 5, Method XGB, Model XGB, Selected Features: [115  88  32 109   7]\n",
      "Fold 16, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.55\n",
      "[15:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458]\n",
      "Fold 17, VarNum 5, Method XGB, Model XGB, Selected Features: [112   2  36 113  74]\n",
      "Fold 17, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.55\n",
      "[15:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386]\n",
      "Fold 18, VarNum 5, Method XGB, Model XGB, Selected Features: [112  31 110 130  32]\n",
      "Fold 18, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978]\n",
      "Fold 19, VarNum 5, Method XGB, Model XGB, Selected Features: [ 16  12  89  80 131]\n",
      "Fold 19, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:21:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441]\n",
      "Fold 20, VarNum 5, Method XGB, Model XGB, Selected Features: [119  48   8  25 135]\n",
      "Fold 20, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843]\n",
      "Fold 21, VarNum 5, Method XGB, Model XGB, Selected Features: [ 30  15 129 105 109]\n",
      "Fold 21, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202]\n",
      "Fold 0, VarNum 5, Method XGB, Model LDA, Selected Features: [ 54  92  67 105  46]\n",
      "Fold 0, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6, Test Acc: 0.38095238095238093\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 1 52 49 43 97]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752]\n",
      "Fold 1, VarNum 5, Method XGB, Model LDA, Selected Features: [ 1 52 49 43 97]\n",
      "Fold 1, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6209677419354839, Test Acc: 0.43478260869565216\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024]\n",
      "Fold 2, VarNum 5, Method XGB, Model LDA, Selected Features: [ 54  93  92 127  35]\n",
      "Fold 2, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6875, Test Acc: 0.8421052631578947\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887]\n",
      "Fold 3, VarNum 5, Method XGB, Model LDA, Selected Features: [127  19 112 111  71]\n",
      "Fold 3, VarNum 5, Method XGB, Model LDA, Train Acc: 0.664, Test Acc: 0.6086956521739131\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301]\n",
      "Fold 4, VarNum 5, Method XGB, Model LDA, Selected Features: [ 32  61 106  29  81]\n",
      "Fold 4, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6953125, Test Acc: 0.7142857142857143\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [37 63 23 72  6]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091]\n",
      "Fold 5, VarNum 5, Method XGB, Model LDA, Selected Features: [37 63 23 72  6]\n",
      "Fold 5, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6692913385826772, Test Acc: 0.5789473684210527\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347]\n",
      "Fold 6, VarNum 5, Method XGB, Model LDA, Selected Features: [107 105 122  52  63]\n",
      "Fold 6, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6904761904761905, Test Acc: 0.45454545454545453\n",
      "[15:21:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871]\n",
      "Fold 7, VarNum 5, Method XGB, Model LDA, Selected Features: [  1 107  80 115  37]\n",
      "Fold 7, VarNum 5, Method XGB, Model LDA, Train Acc: 0.7401574803149606, Test Acc: 0.7894736842105263\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188]\n",
      "Fold 8, VarNum 5, Method XGB, Model LDA, Selected Features: [108  41 110  35 107]\n",
      "Fold 8, VarNum 5, Method XGB, Model LDA, Train Acc: 0.7479674796747967, Test Acc: 0.7142857142857143\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [126 112  27  70  35]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769]\n",
      "Fold 9, VarNum 5, Method XGB, Model LDA, Selected Features: [126 112  27  70  35]\n",
      "Fold 9, VarNum 5, Method XGB, Model LDA, Train Acc: 0.688, Test Acc: 0.6190476190476191\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013]\n",
      "Fold 10, VarNum 5, Method XGB, Model LDA, Selected Features: [ 35  50 132  33  69]\n",
      "Fold 10, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6991869918699187, Test Acc: 0.5454545454545454\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683]\n",
      "Fold 11, VarNum 5, Method XGB, Model LDA, Selected Features: [ 33  92  24 131  28]\n",
      "Fold 11, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6693548387096774, Test Acc: 0.631578947368421\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [33 25 17  8 21]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572]\n",
      "Fold 12, VarNum 5, Method XGB, Model LDA, Selected Features: [33 25 17  8 21]\n",
      "Fold 12, VarNum 5, Method XGB, Model LDA, Train Acc: 0.7258064516129032, Test Acc: 0.5909090909090909\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192]\n",
      "Fold 13, VarNum 5, Method XGB, Model LDA, Selected Features: [125  33  47  26  65]\n",
      "Fold 13, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6451612903225806, Test Acc: 0.5909090909090909\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805]\n",
      "Fold 14, VarNum 5, Method XGB, Model LDA, Selected Features: [ 33  63  42 108  51]\n",
      "Fold 14, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6377952755905512, Test Acc: 0.55\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [89 30 73 69 22]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143  ]\n",
      "Fold 15, VarNum 5, Method XGB, Model LDA, Selected Features: [89 30 73 69 22]\n",
      "Fold 15, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6746031746031746, Test Acc: 0.6521739130434783\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619]\n",
      "Fold 16, VarNum 5, Method XGB, Model LDA, Selected Features: [115  88  32 109   7]\n",
      "Fold 16, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6640625, Test Acc: 0.5\n",
      "[15:21:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458]\n",
      "Fold 17, VarNum 5, Method XGB, Model LDA, Selected Features: [112   2  36 113  74]\n",
      "Fold 17, VarNum 5, Method XGB, Model LDA, Train Acc: 0.7063492063492064, Test Acc: 0.6\n",
      "[15:21:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386]\n",
      "Fold 18, VarNum 5, Method XGB, Model LDA, Selected Features: [112  31 110 130  32]\n",
      "Fold 18, VarNum 5, Method XGB, Model LDA, Train Acc: 0.7007874015748031, Test Acc: 0.45454545454545453\n",
      "[15:21:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978]\n",
      "Fold 19, VarNum 5, Method XGB, Model LDA, Selected Features: [ 16  12  89  80 131]\n",
      "Fold 19, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6141732283464567, Test Acc: 0.42105263157894735\n",
      "[15:21:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441]\n",
      "Fold 20, VarNum 5, Method XGB, Model LDA, Selected Features: [119  48   8  25 135]\n",
      "Fold 20, VarNum 5, Method XGB, Model LDA, Train Acc: 0.5645161290322581, Test Acc: 0.6190476190476191\n",
      "[15:21:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843]\n",
      "Fold 21, VarNum 5, Method XGB, Model LDA, Selected Features: [ 30  15 129 105 109]\n",
      "Fold 21, VarNum 5, Method XGB, Model LDA, Train Acc: 0.688, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model CART, Train Acc: 0.992, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.8181818181818182\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.75\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model RF, Train Acc: 0.992, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 16, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.3684210526315789\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model XGB, Train Acc: 0.992, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.8636363636363636\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model LDA, Train Acc: 0.736, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7741935483870968, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7578125, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model LDA, Train Acc: 0.728, Test Acc: 0.7391304347826086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model LDA, Train Acc: 0.75, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7401574803149606, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7698412698412699, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7559055118110236, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7804878048780488, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model LDA, Train Acc: 0.784, Test Acc: 0.47619047619047616\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7723577235772358, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7741935483870968, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model LDA, Train Acc: 0.8145161290322581, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7580645161290323, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7401574803149606, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7222222222222222, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7109375, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7063492063492064, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model LDA, Train Acc: 0.6535433070866141, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model LDA, Train Acc: 0.6771653543307087, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model LDA, Train Acc: 0.6774193548387096, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model LDA, Train Acc: 0.664, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55  87 106 107 121 127 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  33  35  55  87 106 107 121 127 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 100 106 107 127]\n",
      "Fold 2, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35 100 106 107 127]\n",
      "Fold 2, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35 106 107 127 129 131]\n",
      "Fold 3, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  31  32  33  35 106 107 127 129 131]\n",
      "Fold 3, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35 106 107 122 127 129]\n",
      "Fold 4, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  31  32  33  35 106 107 122 127 129]\n",
      "Fold 4, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 109 122 127 129]\n",
      "Fold 5, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 106 107 109 122 127 129]\n",
      "Fold 5, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 109 123 127 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 106 107 109 123 127 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35  55  76 106 107 127]\n",
      "Fold 7, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  31  32  33  35  55  76 106 107 127]\n",
      "Fold 7, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  38  39  55 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  38  39  55 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 3 21 30 31 32 33 35 36 38 55]\n",
      "Fold 9, VarNum 10, Method CHI2, Model CART, Selected Features: [ 3 21 30 31 32 33 35 36 38 55]\n",
      "Fold 9, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 10, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 10, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 11, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 11, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 12, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 12, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [21 22 30 32 33 35 36 37 38 39]\n",
      "Fold 13, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 30 32 33 35 36 37 38 39]\n",
      "Fold 13, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [21 22 25 30 32 33 35 36 37 38]\n",
      "Fold 14, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 25 30 32 33 35 36 37 38]\n",
      "Fold 14, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [21 22 30 32 33 35 38 51 53 56]\n",
      "Fold 15, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 30 32 33 35 38 51 53 56]\n",
      "Fold 15, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [20 21 22 30 32 33 35 36 38 39]\n",
      "Fold 16, VarNum 10, Method CHI2, Model CART, Selected Features: [20 21 22 30 32 33 35 36 38 39]\n",
      "Fold 16, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (ChiSquareFS): [ 21  22  30  32  33  35  36  38 106 107]\n",
      "Fold 17, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  22  30  32  33  35  36  38 106 107]\n",
      "Fold 17, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35  51 106 107 122 127 134]\n",
      "Fold 18, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  32  33  35  51 106 107 122 127 134]\n",
      "Fold 18, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  36  38 122 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  36  38 122 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.3684210526315789\n",
      "Selected features (ChiSquareFS): [10 13 18 21 22 30 31 32 33 35]\n",
      "Fold 20, VarNum 10, Method CHI2, Model CART, Selected Features: [10 13 18 21 22 30 31 32 33 35]\n",
      "Fold 20, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [13 18 21 22 30 31 32 33 35 55]\n",
      "Fold 21, VarNum 10, Method CHI2, Model CART, Selected Features: [13 18 21 22 30 31 32 33 35 55]\n",
      "Fold 21, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55  87 106 107 121 127 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  33  35  55  87 106 107 121 127 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 100 106 107 127]\n",
      "Fold 2, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35 100 106 107 127]\n",
      "Fold 2, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35 106 107 127 129 131]\n",
      "Fold 3, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  31  32  33  35 106 107 127 129 131]\n",
      "Fold 3, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35 106 107 122 127 129]\n",
      "Fold 4, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  31  32  33  35 106 107 122 127 129]\n",
      "Fold 4, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 109 122 127 129]\n",
      "Fold 5, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 106 107 109 122 127 129]\n",
      "Fold 5, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 109 123 127 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 106 107 109 123 127 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.8181818181818182\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35  55  76 106 107 127]\n",
      "Fold 7, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  31  32  33  35  55  76 106 107 127]\n",
      "Fold 7, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  38  39  55 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  38  39  55 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 3 21 30 31 32 33 35 36 38 55]\n",
      "Fold 9, VarNum 10, Method CHI2, Model RF, Selected Features: [ 3 21 30 31 32 33 35 36 38 55]\n",
      "Fold 9, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 10, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 10, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 11, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 11, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 12, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 12, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [21 22 30 32 33 35 36 37 38 39]\n",
      "Fold 13, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 30 32 33 35 36 37 38 39]\n",
      "Fold 13, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [21 22 25 30 32 33 35 36 37 38]\n",
      "Fold 14, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 25 30 32 33 35 36 37 38]\n",
      "Fold 14, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [21 22 30 32 33 35 38 51 53 56]\n",
      "Fold 15, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 30 32 33 35 38 51 53 56]\n",
      "Fold 15, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [20 21 22 30 32 33 35 36 38 39]\n",
      "Fold 16, VarNum 10, Method CHI2, Model RF, Selected Features: [20 21 22 30 32 33 35 36 38 39]\n",
      "Fold 16, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (ChiSquareFS): [ 21  22  30  32  33  35  36  38 106 107]\n",
      "Fold 17, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  22  30  32  33  35  36  38 106 107]\n",
      "Fold 17, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35  51 106 107 122 127 134]\n",
      "Fold 18, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  32  33  35  51 106 107 122 127 134]\n",
      "Fold 18, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  36  38 122 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  36  38 122 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [10 13 18 21 22 30 31 32 33 35]\n",
      "Fold 20, VarNum 10, Method CHI2, Model RF, Selected Features: [10 13 18 21 22 30 31 32 33 35]\n",
      "Fold 20, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [13 18 21 22 30 31 32 33 35 55]\n",
      "Fold 21, VarNum 10, Method CHI2, Model RF, Selected Features: [13 18 21 22 30 31 32 33 35 55]\n",
      "Fold 21, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55  87 106 107 121 127 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  33  35  55  87 106 107 121 127 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 100 106 107 127]\n",
      "Fold 2, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35 100 106 107 127]\n",
      "Fold 2, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35 106 107 127 129 131]\n",
      "Fold 3, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  31  32  33  35 106 107 127 129 131]\n",
      "Fold 3, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35 106 107 122 127 129]\n",
      "Fold 4, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  31  32  33  35 106 107 122 127 129]\n",
      "Fold 4, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 109 122 127 129]\n",
      "Fold 5, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 106 107 109 122 127 129]\n",
      "Fold 5, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 109 123 127 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 106 107 109 123 127 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35  55  76 106 107 127]\n",
      "Fold 7, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  31  32  33  35  55  76 106 107 127]\n",
      "Fold 7, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  38  39  55 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  38  39  55 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 3 21 30 31 32 33 35 36 38 55]\n",
      "Fold 9, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 3 21 30 31 32 33 35 36 38 55]\n",
      "Fold 9, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 10, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 10, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.991869918699187, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 11, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 11, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9919354838709677, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 12, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 12, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9919354838709677, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [21 22 30 32 33 35 36 37 38 39]\n",
      "Fold 13, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 30 32 33 35 36 37 38 39]\n",
      "Fold 13, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [21 22 25 30 32 33 35 36 37 38]\n",
      "Fold 14, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 25 30 32 33 35 36 37 38]\n",
      "Fold 14, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [21 22 30 32 33 35 38 51 53 56]\n",
      "Fold 15, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 30 32 33 35 38 51 53 56]\n",
      "Fold 15, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [20 21 22 30 32 33 35 36 38 39]\n",
      "Fold 16, VarNum 10, Method CHI2, Model XGB, Selected Features: [20 21 22 30 32 33 35 36 38 39]\n",
      "Fold 16, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [ 21  22  30  32  33  35  36  38 106 107]\n",
      "Fold 17, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  22  30  32  33  35  36  38 106 107]\n",
      "Fold 17, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35  51 106 107 122 127 134]\n",
      "Fold 18, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35  51 106 107 122 127 134]\n",
      "Fold 18, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  36  38 122 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  36  38 122 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (ChiSquareFS): [10 13 18 21 22 30 31 32 33 35]\n",
      "Fold 20, VarNum 10, Method CHI2, Model XGB, Selected Features: [10 13 18 21 22 30 31 32 33 35]\n",
      "Fold 20, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [13 18 21 22 30 31 32 33 35 55]\n",
      "Fold 21, VarNum 10, Method CHI2, Model XGB, Selected Features: [13 18 21 22 30 31 32 33 35 55]\n",
      "Fold 21, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.696, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55  87 106 107 121 127 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  33  35  55  87 106 107 121 127 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7419354838709677, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 100 106 107 127]\n",
      "Fold 2, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35 100 106 107 127]\n",
      "Fold 2, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7890625, Test Acc: 0.8947368421052632\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35 106 107 127 129 131]\n",
      "Fold 3, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  31  32  33  35 106 107 127 129 131]\n",
      "Fold 3, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.744, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35 106 107 122 127 129]\n",
      "Fold 4, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  31  32  33  35 106 107 122 127 129]\n",
      "Fold 4, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7421875, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 109 122 127 129]\n",
      "Fold 5, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 106 107 109 122 127 129]\n",
      "Fold 5, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7244094488188977, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 109 123 127 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 106 107 109 123 127 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7619047619047619, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35  55  76 106 107 127]\n",
      "Fold 7, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  31  32  33  35  55  76 106 107 127]\n",
      "Fold 7, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7716535433070866, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  38  39  55 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  38  39  55 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7642276422764228, Test Acc: 0.9523809523809523\n",
      "Selected features (ChiSquareFS): [ 3 21 30 31 32 33 35 36 38 55]\n",
      "Fold 9, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 3 21 30 31 32 33 35 36 38 55]\n",
      "Fold 9, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.768, Test Acc: 0.5238095238095238\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 10, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 10, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7642276422764228, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 11, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 11, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7983870967741935, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 12, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 25 30 31 32 33 35 36 38]\n",
      "Fold 12, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.8387096774193549, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [21 22 30 32 33 35 36 37 38 39]\n",
      "Fold 13, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 30 32 33 35 36 37 38 39]\n",
      "Fold 13, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.717741935483871, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [21 22 25 30 32 33 35 36 37 38]\n",
      "Fold 14, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 25 30 32 33 35 36 37 38]\n",
      "Fold 14, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7086614173228346, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [21 22 30 32 33 35 38 51 53 56]\n",
      "Fold 15, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 30 32 33 35 38 51 53 56]\n",
      "Fold 15, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.6666666666666666, Test Acc: 0.782608695652174\n",
      "Selected features (ChiSquareFS): [20 21 22 30 32 33 35 36 38 39]\n",
      "Fold 16, VarNum 10, Method CHI2, Model LDA, Selected Features: [20 21 22 30 32 33 35 36 38 39]\n",
      "Fold 16, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.6640625, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [ 21  22  30  32  33  35  36  38 106 107]\n",
      "Fold 17, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  22  30  32  33  35  36  38 106 107]\n",
      "Fold 17, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.6587301587301587, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35  51 106 107 122 127 134]\n",
      "Fold 18, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35  51 106 107 122 127 134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 18, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7007874015748031, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  36  38 122 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  36  38 122 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7559055118110236, Test Acc: 0.47368421052631576\n",
      "Selected features (ChiSquareFS): [10 13 18 21 22 30 31 32 33 35]\n",
      "Fold 20, VarNum 10, Method CHI2, Model LDA, Selected Features: [10 13 18 21 22 30 31 32 33 35]\n",
      "Fold 20, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.6774193548387096, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [13 18 21 22 30 31 32 33 35 55]\n",
      "Fold 21, VarNum 10, Method CHI2, Model LDA, Selected Features: [13 18 21 22 30 31 32 33 35 55]\n",
      "Fold 21, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.688, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252]\n",
      "Fold 0, VarNum 10, Method RF, Model CART, Selected Features: [129 134 106  30 107 105  43 123 100  96]\n",
      "Fold 0, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.38095238095238093\n",
      "Selected features (RF): [134 107 106 100 129  30 131 123 102 127]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054 0.02062351\n",
      " 0.02036939 0.02001713 0.01831205 0.01752115]\n",
      "Fold 1, VarNum 10, Method RF, Model CART, Selected Features: [134 107 106 100 129  30 131 123 102 127]\n",
      "Fold 1, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.782608695652174\n",
      "Selected features (RF): [107  30 105  33 129 106 100 127 102  98]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624 0.01825179\n",
      " 0.01749679 0.01692175 0.01671952 0.0158184 ]\n",
      "Fold 2, VarNum 10, Method RF, Model CART, Selected Features: [107  30 105  33 129 106 100 127 102  98]\n",
      "Fold 2, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [107 106 122 127  30  33 104 120 105 100]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713 0.02271285\n",
      " 0.02224426 0.01820072 0.01750092 0.01735569]\n",
      "Fold 3, VarNum 10, Method RF, Model CART, Selected Features: [107 106 122 127  30  33 104 120 105 100]\n",
      "Fold 3, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [106 107  32  30  33 129 109 120 127 100]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174   0.02665422\n",
      " 0.02360296 0.01937688 0.01756391 0.01635806]\n",
      "Fold 4, VarNum 10, Method RF, Model CART, Selected Features: [106 107  32  30  33 129 109 120 127 100]\n",
      "Fold 4, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [107 127 106 109  30 129 136 134  32  33]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567 0.01914487\n",
      " 0.01570817 0.0154738  0.01532336 0.01522347]\n",
      "Fold 5, VarNum 10, Method RF, Model CART, Selected Features: [107 127 106 109  30 129 136 134  32  33]\n",
      "Fold 5, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [106 107 127 109  32 118 122 134 131  89]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486 0.02450973\n",
      " 0.02411738 0.02402983 0.01708098 0.0164193 ]\n",
      "Fold 6, VarNum 10, Method RF, Model CART, Selected Features: [106 107 127 109  32 118 122 134 131  89]\n",
      "Fold 6, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [107 106 127  30 118 134  90 123 113 109]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774 0.02518253\n",
      " 0.02050878 0.0164142  0.01534288 0.01532239]\n",
      "Fold 7, VarNum 10, Method RF, Model CART, Selected Features: [107 106 127  30 118 134  90 123 113 109]\n",
      "Fold 7, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [107 106  30 109  33 134 122 123  57  35]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996 0.02325124\n",
      " 0.02306146 0.02232644 0.02068571 0.02016427]\n",
      "Fold 8, VarNum 10, Method RF, Model CART, Selected Features: [107 106  30 109  33 134 122 123  57  35]\n",
      "Fold 8, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30 107 132  35 106 123 118 134 127  33]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646 0.02035143\n",
      " 0.01837401 0.01697944 0.01689733 0.01622381]\n",
      "Fold 9, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 132  35 106 123 118 134 127  33]\n",
      "Fold 9, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (RF): [ 30  32  33  35  57   6 113 118  25  54]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143  0.02133614\n",
      " 0.01841816 0.01703708 0.01599764 0.01544085]\n",
      "Fold 10, VarNum 10, Method RF, Model CART, Selected Features: [ 30  32  33  35  57   6 113 118  25  54]\n",
      "Fold 10, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [30 33 32 35 21 37 39 22 57 25]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159 0.01897151\n",
      " 0.01866083 0.01702243 0.01647859 0.01574145]\n",
      "Fold 11, VarNum 10, Method RF, Model CART, Selected Features: [30 33 32 35 21 37 39 22 57 25]\n",
      "Fold 11, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [33 35 30 21 32 22  3 39 25 37]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812 0.01944994\n",
      " 0.01731347 0.0168168  0.01443683 0.01385973]\n",
      "Fold 12, VarNum 10, Method RF, Model CART, Selected Features: [33 35 30 21 32 22  3 39 25 37]\n",
      "Fold 12, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 33  30  32  21  35  37 136 101 100 115]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099   0.01780627\n",
      " 0.0133239  0.01289129 0.0127552  0.01254642]\n",
      "Fold 13, VarNum 10, Method RF, Model CART, Selected Features: [ 33  30  32  21  35  37 136 101 100 115]\n",
      "Fold 13, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 33  30  35  32 127  21 106  22 121 105]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125 0.01578645\n",
      " 0.01528147 0.01479727 0.01284964 0.01243961]\n",
      "Fold 14, VarNum 10, Method RF, Model CART, Selected Features: [ 33  30  35  32 127  21 106  22 121 105]\n",
      "Fold 14, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [ 33  30 110  21 101  32  35 136  37 114]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285 0.015737\n",
      " 0.01503168 0.0140918  0.01361461 0.01344505]\n",
      "Fold 15, VarNum 10, Method RF, Model CART, Selected Features: [ 33  30 110  21 101  32  35 136  37 114]\n",
      "Fold 15, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "Selected features (RF): [ 33 105  30  21  32 136 107  22 114 102]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172 0.01886913\n",
      " 0.01532578 0.01504565 0.01427528 0.01374455]\n",
      "Fold 16, VarNum 10, Method RF, Model CART, Selected Features: [ 33 105  30  21  32 136 107  22 114 102]\n",
      "Fold 16, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.75\n",
      "Selected features (RF): [122 113  30  21 134 105   6 109  16  32]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434 0.01428145\n",
      " 0.01421295 0.01348654 0.01343482 0.01305475]\n",
      "Fold 17, VarNum 10, Method RF, Model CART, Selected Features: [122 113  30  21 134 105   6 109  16  32]\n",
      "Fold 17, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (RF): [122 113  91 114  33 134  32 110  92  46]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344 0.01700055\n",
      " 0.01696383 0.01685772 0.016267   0.01624445]\n",
      "Fold 18, VarNum 10, Method RF, Model CART, Selected Features: [122 113  91 114  33 134  32 110  92  46]\n",
      "Fold 18, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.36363636363636365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [134  92 122  32  33 109  73 114 113 132]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174 0.01769571\n",
      " 0.01744237 0.01549418 0.01513342 0.01443422]\n",
      "Fold 19, VarNum 10, Method RF, Model CART, Selected Features: [134  92 122  32  33 109  73 114 113 132]\n",
      "Fold 19, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "Selected features (RF): [ 30  32 134  33  91 114 119 112  85 130]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132  0.01416553\n",
      " 0.01375293 0.01374425 0.01317677 0.0129457 ]\n",
      "Fold 20, VarNum 10, Method RF, Model CART, Selected Features: [ 30  32 134  33  91 114 119 112  85 130]\n",
      "Fold 20, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.2857142857142857\n",
      "Selected features (RF): [ 30 136 100  73  98  32  31  21  33   6]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984 0.01694408\n",
      " 0.01582854 0.01479282 0.01409128 0.01360734]\n",
      "Fold 21, VarNum 10, Method RF, Model CART, Selected Features: [ 30 136 100  73  98  32  31  21  33   6]\n",
      "Fold 21, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252]\n",
      "Fold 0, VarNum 10, Method RF, Model RF, Selected Features: [129 134 106  30 107 105  43 123 100  96]\n",
      "Fold 0, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [134 107 106 100 129  30 131 123 102 127]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054 0.02062351\n",
      " 0.02036939 0.02001713 0.01831205 0.01752115]\n",
      "Fold 1, VarNum 10, Method RF, Model RF, Selected Features: [134 107 106 100 129  30 131 123 102 127]\n",
      "Fold 1, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [107  30 105  33 129 106 100 127 102  98]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624 0.01825179\n",
      " 0.01749679 0.01692175 0.01671952 0.0158184 ]\n",
      "Fold 2, VarNum 10, Method RF, Model RF, Selected Features: [107  30 105  33 129 106 100 127 102  98]\n",
      "Fold 2, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [107 106 122 127  30  33 104 120 105 100]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713 0.02271285\n",
      " 0.02224426 0.01820072 0.01750092 0.01735569]\n",
      "Fold 3, VarNum 10, Method RF, Model RF, Selected Features: [107 106 122 127  30  33 104 120 105 100]\n",
      "Fold 3, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [106 107  32  30  33 129 109 120 127 100]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174   0.02665422\n",
      " 0.02360296 0.01937688 0.01756391 0.01635806]\n",
      "Fold 4, VarNum 10, Method RF, Model RF, Selected Features: [106 107  32  30  33 129 109 120 127 100]\n",
      "Fold 4, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [107 127 106 109  30 129 136 134  32  33]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567 0.01914487\n",
      " 0.01570817 0.0154738  0.01532336 0.01522347]\n",
      "Fold 5, VarNum 10, Method RF, Model RF, Selected Features: [107 127 106 109  30 129 136 134  32  33]\n",
      "Fold 5, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [106 107 127 109  32 118 122 134 131  89]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486 0.02450973\n",
      " 0.02411738 0.02402983 0.01708098 0.0164193 ]\n",
      "Fold 6, VarNum 10, Method RF, Model RF, Selected Features: [106 107 127 109  32 118 122 134 131  89]\n",
      "Fold 6, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [107 106 127  30 118 134  90 123 113 109]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774 0.02518253\n",
      " 0.02050878 0.0164142  0.01534288 0.01532239]\n",
      "Fold 7, VarNum 10, Method RF, Model RF, Selected Features: [107 106 127  30 118 134  90 123 113 109]\n",
      "Fold 7, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [107 106  30 109  33 134 122 123  57  35]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996 0.02325124\n",
      " 0.02306146 0.02232644 0.02068571 0.02016427]\n",
      "Fold 8, VarNum 10, Method RF, Model RF, Selected Features: [107 106  30 109  33 134 122 123  57  35]\n",
      "Fold 8, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 132  35 106 123 118 134 127  33]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646 0.02035143\n",
      " 0.01837401 0.01697944 0.01689733 0.01622381]\n",
      "Fold 9, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 132  35 106 123 118 134 127  33]\n",
      "Fold 9, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30  32  33  35  57   6 113 118  25  54]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143  0.02133614\n",
      " 0.01841816 0.01703708 0.01599764 0.01544085]\n",
      "Fold 10, VarNum 10, Method RF, Model RF, Selected Features: [ 30  32  33  35  57   6 113 118  25  54]\n",
      "Fold 10, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [30 33 32 35 21 37 39 22 57 25]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159 0.01897151\n",
      " 0.01866083 0.01702243 0.01647859 0.01574145]\n",
      "Fold 11, VarNum 10, Method RF, Model RF, Selected Features: [30 33 32 35 21 37 39 22 57 25]\n",
      "Fold 11, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [33 35 30 21 32 22  3 39 25 37]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812 0.01944994\n",
      " 0.01731347 0.0168168  0.01443683 0.01385973]\n",
      "Fold 12, VarNum 10, Method RF, Model RF, Selected Features: [33 35 30 21 32 22  3 39 25 37]\n",
      "Fold 12, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 33  30  32  21  35  37 136 101 100 115]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099   0.01780627\n",
      " 0.0133239  0.01289129 0.0127552  0.01254642]\n",
      "Fold 13, VarNum 10, Method RF, Model RF, Selected Features: [ 33  30  32  21  35  37 136 101 100 115]\n",
      "Fold 13, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 33  30  35  32 127  21 106  22 121 105]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125 0.01578645\n",
      " 0.01528147 0.01479727 0.01284964 0.01243961]\n",
      "Fold 14, VarNum 10, Method RF, Model RF, Selected Features: [ 33  30  35  32 127  21 106  22 121 105]\n",
      "Fold 14, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (RF): [ 33  30 110  21 101  32  35 136  37 114]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285 0.015737\n",
      " 0.01503168 0.0140918  0.01361461 0.01344505]\n",
      "Fold 15, VarNum 10, Method RF, Model RF, Selected Features: [ 33  30 110  21 101  32  35 136  37 114]\n",
      "Fold 15, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (RF): [ 33 105  30  21  32 136 107  22 114 102]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172 0.01886913\n",
      " 0.01532578 0.01504565 0.01427528 0.01374455]\n",
      "Fold 16, VarNum 10, Method RF, Model RF, Selected Features: [ 33 105  30  21  32 136 107  22 114 102]\n",
      "Fold 16, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (RF): [122 113  30  21 134 105   6 109  16  32]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434 0.01428145\n",
      " 0.01421295 0.01348654 0.01343482 0.01305475]\n",
      "Fold 17, VarNum 10, Method RF, Model RF, Selected Features: [122 113  30  21 134 105   6 109  16  32]\n",
      "Fold 17, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (RF): [122 113  91 114  33 134  32 110  92  46]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344 0.01700055\n",
      " 0.01696383 0.01685772 0.016267   0.01624445]\n",
      "Fold 18, VarNum 10, Method RF, Model RF, Selected Features: [122 113  91 114  33 134  32 110  92  46]\n",
      "Fold 18, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [134  92 122  32  33 109  73 114 113 132]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174 0.01769571\n",
      " 0.01744237 0.01549418 0.01513342 0.01443422]\n",
      "Fold 19, VarNum 10, Method RF, Model RF, Selected Features: [134  92 122  32  33 109  73 114 113 132]\n",
      "Fold 19, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (RF): [ 30  32 134  33  91 114 119 112  85 130]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132  0.01416553\n",
      " 0.01375293 0.01374425 0.01317677 0.0129457 ]\n",
      "Fold 20, VarNum 10, Method RF, Model RF, Selected Features: [ 30  32 134  33  91 114 119 112  85 130]\n",
      "Fold 20, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [ 30 136 100  73  98  32  31  21  33   6]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984 0.01694408\n",
      " 0.01582854 0.01479282 0.01409128 0.01360734]\n",
      "Fold 21, VarNum 10, Method RF, Model RF, Selected Features: [ 30 136 100  73  98  32  31  21  33   6]\n",
      "Fold 21, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252]\n",
      "Fold 0, VarNum 10, Method RF, Model XGB, Selected Features: [129 134 106  30 107 105  43 123 100  96]\n",
      "Fold 0, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [134 107 106 100 129  30 131 123 102 127]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054 0.02062351\n",
      " 0.02036939 0.02001713 0.01831205 0.01752115]\n",
      "Fold 1, VarNum 10, Method RF, Model XGB, Selected Features: [134 107 106 100 129  30 131 123 102 127]\n",
      "Fold 1, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [107  30 105  33 129 106 100 127 102  98]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624 0.01825179\n",
      " 0.01749679 0.01692175 0.01671952 0.0158184 ]\n",
      "Fold 2, VarNum 10, Method RF, Model XGB, Selected Features: [107  30 105  33 129 106 100 127 102  98]\n",
      "Fold 2, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [107 106 122 127  30  33 104 120 105 100]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713 0.02271285\n",
      " 0.02224426 0.01820072 0.01750092 0.01735569]\n",
      "Fold 3, VarNum 10, Method RF, Model XGB, Selected Features: [107 106 122 127  30  33 104 120 105 100]\n",
      "Fold 3, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.782608695652174\n",
      "Selected features (RF): [106 107  32  30  33 129 109 120 127 100]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174   0.02665422\n",
      " 0.02360296 0.01937688 0.01756391 0.01635806]\n",
      "Fold 4, VarNum 10, Method RF, Model XGB, Selected Features: [106 107  32  30  33 129 109 120 127 100]\n",
      "Fold 4, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.47619047619047616\n",
      "Selected features (RF): [107 127 106 109  30 129 136 134  32  33]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567 0.01914487\n",
      " 0.01570817 0.0154738  0.01532336 0.01522347]\n",
      "Fold 5, VarNum 10, Method RF, Model XGB, Selected Features: [107 127 106 109  30 129 136 134  32  33]\n",
      "Fold 5, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [106 107 127 109  32 118 122 134 131  89]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486 0.02450973\n",
      " 0.02411738 0.02402983 0.01708098 0.0164193 ]\n",
      "Fold 6, VarNum 10, Method RF, Model XGB, Selected Features: [106 107 127 109  32 118 122 134 131  89]\n",
      "Fold 6, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [107 106 127  30 118 134  90 123 113 109]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774 0.02518253\n",
      " 0.02050878 0.0164142  0.01534288 0.01532239]\n",
      "Fold 7, VarNum 10, Method RF, Model XGB, Selected Features: [107 106 127  30 118 134  90 123 113 109]\n",
      "Fold 7, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [107 106  30 109  33 134 122 123  57  35]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996 0.02325124\n",
      " 0.02306146 0.02232644 0.02068571 0.02016427]\n",
      "Fold 8, VarNum 10, Method RF, Model XGB, Selected Features: [107 106  30 109  33 134 122 123  57  35]\n",
      "Fold 8, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 132  35 106 123 118 134 127  33]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646 0.02035143\n",
      " 0.01837401 0.01697944 0.01689733 0.01622381]\n",
      "Fold 9, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 132  35 106 123 118 134 127  33]\n",
      "Fold 9, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30  32  33  35  57   6 113 118  25  54]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143  0.02133614\n",
      " 0.01841816 0.01703708 0.01599764 0.01544085]\n",
      "Fold 10, VarNum 10, Method RF, Model XGB, Selected Features: [ 30  32  33  35  57   6 113 118  25  54]\n",
      "Fold 10, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [30 33 32 35 21 37 39 22 57 25]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159 0.01897151\n",
      " 0.01866083 0.01702243 0.01647859 0.01574145]\n",
      "Fold 11, VarNum 10, Method RF, Model XGB, Selected Features: [30 33 32 35 21 37 39 22 57 25]\n",
      "Fold 11, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [33 35 30 21 32 22  3 39 25 37]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812 0.01944994\n",
      " 0.01731347 0.0168168  0.01443683 0.01385973]\n",
      "Fold 12, VarNum 10, Method RF, Model XGB, Selected Features: [33 35 30 21 32 22  3 39 25 37]\n",
      "Fold 12, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 33  30  32  21  35  37 136 101 100 115]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099   0.01780627\n",
      " 0.0133239  0.01289129 0.0127552  0.01254642]\n",
      "Fold 13, VarNum 10, Method RF, Model XGB, Selected Features: [ 33  30  32  21  35  37 136 101 100 115]\n",
      "Fold 13, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (RF): [ 33  30  35  32 127  21 106  22 121 105]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125 0.01578645\n",
      " 0.01528147 0.01479727 0.01284964 0.01243961]\n",
      "Fold 14, VarNum 10, Method RF, Model XGB, Selected Features: [ 33  30  35  32 127  21 106  22 121 105]\n",
      "Fold 14, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (RF): [ 33  30 110  21 101  32  35 136  37 114]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285 0.015737\n",
      " 0.01503168 0.0140918  0.01361461 0.01344505]\n",
      "Fold 15, VarNum 10, Method RF, Model XGB, Selected Features: [ 33  30 110  21 101  32  35 136  37 114]\n",
      "Fold 15, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (RF): [ 33 105  30  21  32 136 107  22 114 102]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172 0.01886913\n",
      " 0.01532578 0.01504565 0.01427528 0.01374455]\n",
      "Fold 16, VarNum 10, Method RF, Model XGB, Selected Features: [ 33 105  30  21  32 136 107  22 114 102]\n",
      "Fold 16, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (RF): [122 113  30  21 134 105   6 109  16  32]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434 0.01428145\n",
      " 0.01421295 0.01348654 0.01343482 0.01305475]\n",
      "Fold 17, VarNum 10, Method RF, Model XGB, Selected Features: [122 113  30  21 134 105   6 109  16  32]\n",
      "Fold 17, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (RF): [122 113  91 114  33 134  32 110  92  46]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344 0.01700055\n",
      " 0.01696383 0.01685772 0.016267   0.01624445]\n",
      "Fold 18, VarNum 10, Method RF, Model XGB, Selected Features: [122 113  91 114  33 134  32 110  92  46]\n",
      "Fold 18, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [134  92 122  32  33 109  73 114 113 132]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174 0.01769571\n",
      " 0.01744237 0.01549418 0.01513342 0.01443422]\n",
      "Fold 19, VarNum 10, Method RF, Model XGB, Selected Features: [134  92 122  32  33 109  73 114 113 132]\n",
      "Fold 19, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (RF): [ 30  32 134  33  91 114 119 112  85 130]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132  0.01416553\n",
      " 0.01375293 0.01374425 0.01317677 0.0129457 ]\n",
      "Fold 20, VarNum 10, Method RF, Model XGB, Selected Features: [ 30  32 134  33  91 114 119 112  85 130]\n",
      "Fold 20, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [ 30 136 100  73  98  32  31  21  33   6]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984 0.01694408\n",
      " 0.01582854 0.01479282 0.01409128 0.01360734]\n",
      "Fold 21, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 136 100  73  98  32  31  21  33   6]\n",
      "Fold 21, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252]\n",
      "Fold 0, VarNum 10, Method RF, Model LDA, Selected Features: [129 134 106  30 107 105  43 123 100  96]\n",
      "Fold 0, VarNum 10, Method RF, Model LDA, Train Acc: 0.72, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [134 107 106 100 129  30 131 123 102 127]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054 0.02062351\n",
      " 0.02036939 0.02001713 0.01831205 0.01752115]\n",
      "Fold 1, VarNum 10, Method RF, Model LDA, Selected Features: [134 107 106 100 129  30 131 123 102 127]\n",
      "Fold 1, VarNum 10, Method RF, Model LDA, Train Acc: 0.7338709677419355, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [107  30 105  33 129 106 100 127 102  98]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624 0.01825179\n",
      " 0.01749679 0.01692175 0.01671952 0.0158184 ]\n",
      "Fold 2, VarNum 10, Method RF, Model LDA, Selected Features: [107  30 105  33 129 106 100 127 102  98]\n",
      "Fold 2, VarNum 10, Method RF, Model LDA, Train Acc: 0.7578125, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [107 106 122 127  30  33 104 120 105 100]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713 0.02271285\n",
      " 0.02224426 0.01820072 0.01750092 0.01735569]\n",
      "Fold 3, VarNum 10, Method RF, Model LDA, Selected Features: [107 106 122 127  30  33 104 120 105 100]\n",
      "Fold 3, VarNum 10, Method RF, Model LDA, Train Acc: 0.776, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [106 107  32  30  33 129 109 120 127 100]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174   0.02665422\n",
      " 0.02360296 0.01937688 0.01756391 0.01635806]\n",
      "Fold 4, VarNum 10, Method RF, Model LDA, Selected Features: [106 107  32  30  33 129 109 120 127 100]\n",
      "Fold 4, VarNum 10, Method RF, Model LDA, Train Acc: 0.78125, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [107 127 106 109  30 129 136 134  32  33]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567 0.01914487\n",
      " 0.01570817 0.0154738  0.01532336 0.01522347]\n",
      "Fold 5, VarNum 10, Method RF, Model LDA, Selected Features: [107 127 106 109  30 129 136 134  32  33]\n",
      "Fold 5, VarNum 10, Method RF, Model LDA, Train Acc: 0.7795275590551181, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [106 107 127 109  32 118 122 134 131  89]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486 0.02450973\n",
      " 0.02411738 0.02402983 0.01708098 0.0164193 ]\n",
      "Fold 6, VarNum 10, Method RF, Model LDA, Selected Features: [106 107 127 109  32 118 122 134 131  89]\n",
      "Fold 6, VarNum 10, Method RF, Model LDA, Train Acc: 0.7698412698412699, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [107 106 127  30 118 134  90 123 113 109]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774 0.02518253\n",
      " 0.02050878 0.0164142  0.01534288 0.01532239]\n",
      "Fold 7, VarNum 10, Method RF, Model LDA, Selected Features: [107 106 127  30 118 134  90 123 113 109]\n",
      "Fold 7, VarNum 10, Method RF, Model LDA, Train Acc: 0.7637795275590551, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [107 106  30 109  33 134 122 123  57  35]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996 0.02325124\n",
      " 0.02306146 0.02232644 0.02068571 0.02016427]\n",
      "Fold 8, VarNum 10, Method RF, Model LDA, Selected Features: [107 106  30 109  33 134 122 123  57  35]\n",
      "Fold 8, VarNum 10, Method RF, Model LDA, Train Acc: 0.8130081300813008, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30 107 132  35 106 123 118 134 127  33]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646 0.02035143\n",
      " 0.01837401 0.01697944 0.01689733 0.01622381]\n",
      "Fold 9, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 132  35 106 123 118 134 127  33]\n",
      "Fold 9, VarNum 10, Method RF, Model LDA, Train Acc: 0.76, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30  32  33  35  57   6 113 118  25  54]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143  0.02133614\n",
      " 0.01841816 0.01703708 0.01599764 0.01544085]\n",
      "Fold 10, VarNum 10, Method RF, Model LDA, Selected Features: [ 30  32  33  35  57   6 113 118  25  54]\n",
      "Fold 10, VarNum 10, Method RF, Model LDA, Train Acc: 0.7804878048780488, Test Acc: 0.8181818181818182\n",
      "Selected features (RF): [30 33 32 35 21 37 39 22 57 25]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159 0.01897151\n",
      " 0.01866083 0.01702243 0.01647859 0.01574145]\n",
      "Fold 11, VarNum 10, Method RF, Model LDA, Selected Features: [30 33 32 35 21 37 39 22 57 25]\n",
      "Fold 11, VarNum 10, Method RF, Model LDA, Train Acc: 0.7903225806451613, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [33 35 30 21 32 22  3 39 25 37]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812 0.01944994\n",
      " 0.01731347 0.0168168  0.01443683 0.01385973]\n",
      "Fold 12, VarNum 10, Method RF, Model LDA, Selected Features: [33 35 30 21 32 22  3 39 25 37]\n",
      "Fold 12, VarNum 10, Method RF, Model LDA, Train Acc: 0.8064516129032258, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 33  30  32  21  35  37 136 101 100 115]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099   0.01780627\n",
      " 0.0133239  0.01289129 0.0127552  0.01254642]\n",
      "Fold 13, VarNum 10, Method RF, Model LDA, Selected Features: [ 33  30  32  21  35  37 136 101 100 115]\n",
      "Fold 13, VarNum 10, Method RF, Model LDA, Train Acc: 0.7338709677419355, Test Acc: 0.4090909090909091\n",
      "Selected features (RF): [ 33  30  35  32 127  21 106  22 121 105]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125 0.01578645\n",
      " 0.01528147 0.01479727 0.01284964 0.01243961]\n",
      "Fold 14, VarNum 10, Method RF, Model LDA, Selected Features: [ 33  30  35  32 127  21 106  22 121 105]\n",
      "Fold 14, VarNum 10, Method RF, Model LDA, Train Acc: 0.7007874015748031, Test Acc: 0.6\n",
      "Selected features (RF): [ 33  30 110  21 101  32  35 136  37 114]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285 0.015737\n",
      " 0.01503168 0.0140918  0.01361461 0.01344505]\n",
      "Fold 15, VarNum 10, Method RF, Model LDA, Selected Features: [ 33  30 110  21 101  32  35 136  37 114]\n",
      "Fold 15, VarNum 10, Method RF, Model LDA, Train Acc: 0.7142857142857143, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [ 33 105  30  21  32 136 107  22 114 102]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172 0.01886913\n",
      " 0.01532578 0.01504565 0.01427528 0.01374455]\n",
      "Fold 16, VarNum 10, Method RF, Model LDA, Selected Features: [ 33 105  30  21  32 136 107  22 114 102]\n",
      "Fold 16, VarNum 10, Method RF, Model LDA, Train Acc: 0.6953125, Test Acc: 0.55\n",
      "Selected features (RF): [122 113  30  21 134 105   6 109  16  32]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434 0.01428145\n",
      " 0.01421295 0.01348654 0.01343482 0.01305475]\n",
      "Fold 17, VarNum 10, Method RF, Model LDA, Selected Features: [122 113  30  21 134 105   6 109  16  32]\n",
      "Fold 17, VarNum 10, Method RF, Model LDA, Train Acc: 0.6984126984126984, Test Acc: 0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [122 113  91 114  33 134  32 110  92  46]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344 0.01700055\n",
      " 0.01696383 0.01685772 0.016267   0.01624445]\n",
      "Fold 18, VarNum 10, Method RF, Model LDA, Selected Features: [122 113  91 114  33 134  32 110  92  46]\n",
      "Fold 18, VarNum 10, Method RF, Model LDA, Train Acc: 0.7322834645669292, Test Acc: 0.36363636363636365\n",
      "Selected features (RF): [134  92 122  32  33 109  73 114 113 132]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174 0.01769571\n",
      " 0.01744237 0.01549418 0.01513342 0.01443422]\n",
      "Fold 19, VarNum 10, Method RF, Model LDA, Selected Features: [134  92 122  32  33 109  73 114 113 132]\n",
      "Fold 19, VarNum 10, Method RF, Model LDA, Train Acc: 0.7007874015748031, Test Acc: 0.42105263157894735\n",
      "Selected features (RF): [ 30  32 134  33  91 114 119 112  85 130]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132  0.01416553\n",
      " 0.01375293 0.01374425 0.01317677 0.0129457 ]\n",
      "Fold 20, VarNum 10, Method RF, Model LDA, Selected Features: [ 30  32 134  33  91 114 119 112  85 130]\n",
      "Fold 20, VarNum 10, Method RF, Model LDA, Train Acc: 0.6532258064516129, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 136 100  73  98  32  31  21  33   6]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984 0.01694408\n",
      " 0.01582854 0.01479282 0.01409128 0.01360734]\n",
      "Fold 21, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 136 100  73  98  32  31  21  33   6]\n",
      "Fold 21, VarNum 10, Method RF, Model LDA, Train Acc: 0.76, Test Acc: 0.7272727272727273\n",
      "[15:22:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594]\n",
      "Fold 0, VarNum 10, Method XGB, Model CART, Selected Features: [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Fold 0, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:22:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1  52  49  43  97 134  65 106 101  95]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752 0.03626129\n",
      " 0.02896795 0.02821424 0.02584204 0.02422096]\n",
      "Fold 1, VarNum 10, Method XGB, Model CART, Selected Features: [  1  52  49  43  97 134  65 106 101  95]\n",
      "Fold 1, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "[15:22:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35  32  71 106  97 109]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024 0.03934522\n",
      " 0.03732697 0.03702934 0.03554991 0.02754998]\n",
      "Fold 2, VarNum 10, Method XGB, Model CART, Selected Features: [ 54  93  92 127  35  32  71 106  97 109]\n",
      "Fold 2, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71  97  29 131 105  37]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887 0.03330989\n",
      " 0.03321506 0.03237444 0.03195391 0.0294292 ]\n",
      "Fold 3, VarNum 10, Method XGB, Model CART, Selected Features: [127  19 112 111  71  97  29 131 105  37]\n",
      "Fold 3, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81  92 105  30 104  95]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301 0.03296668\n",
      " 0.03100923 0.02906076 0.02756156 0.02643843]\n",
      "Fold 4, VarNum 10, Method XGB, Model CART, Selected Features: [ 32  61 106  29  81  92 105  30 104  95]\n",
      "Fold 4, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 37  63  23  72   6 107  68 106 131   0]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091 0.03251973\n",
      " 0.02970542 0.02945096 0.02717068 0.02702343]\n",
      "Fold 5, VarNum 10, Method XGB, Model CART, Selected Features: [ 37  63  23  72   6 107  68 106 131   0]\n",
      "Fold 5, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63  57 127   5  17  32]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347 0.04153319\n",
      " 0.04102546 0.03950502 0.03855329 0.03679097]\n",
      "Fold 6, VarNum 10, Method XGB, Model CART, Selected Features: [107 105 122  52  63  57 127   5  17  32]\n",
      "Fold 6, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37 127  35  51  63  73]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871 0.03215658\n",
      " 0.03175694 0.0314934  0.02671719 0.0256332 ]\n",
      "Fold 7, VarNum 10, Method XGB, Model CART, Selected Features: [  1 107  80 115  37 127  35  51  63  73]\n",
      "Fold 7, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107  33  17  30  75  54]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188 0.04792891\n",
      " 0.03837226 0.03728842 0.0318395  0.03169723]\n",
      "Fold 8, VarNum 10, Method XGB, Model CART, Selected Features: [108  41 110  35 107  33  17  30  75  54]\n",
      "Fold 8, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35  30  17 121  15 109]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769 0.04237923\n",
      " 0.0411956  0.03040076 0.0301668  0.02735102]\n",
      "Fold 9, VarNum 10, Method XGB, Model CART, Selected Features: [126 112  27  70  35  30  17 121  15 109]\n",
      "Fold 9, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69  56  42 111  65  27]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013 0.04062516\n",
      " 0.04024033 0.03461215 0.03025046 0.02787456]\n",
      "Fold 10, VarNum 10, Method XGB, Model CART, Selected Features: [ 35  50 132  33  69  56  42 111  65  27]\n",
      "Fold 10, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:22:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 33  92  24 131  28 128  30  93  87  64]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683 0.03494062\n",
      " 0.03379305 0.03272084 0.03186584 0.03093796]\n",
      "Fold 11, VarNum 10, Method XGB, Model CART, Selected Features: [ 33  92  24 131  28 128  30  93  87  64]\n",
      "Fold 11, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.3157894736842105\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [33 25 17  8 21 98 35 74 32  9]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572 0.03954697\n",
      " 0.0381116  0.03652066 0.03586444 0.03134957]\n",
      "Fold 12, VarNum 10, Method XGB, Model CART, Selected Features: [33 25 17  8 21 98 35 74 32  9]\n",
      "Fold 12, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65 124  32 100  95 128]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192 0.0375411\n",
      " 0.0296344  0.028289   0.02809397 0.02647186]\n",
      "Fold 13, VarNum 10, Method XGB, Model CART, Selected Features: [125  33  47  26  65 124  32 100  95 128]\n",
      "Fold 13, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51   5  67  70 132 122]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805 0.03213321\n",
      " 0.03205619 0.02940755 0.02676875 0.02317305]\n",
      "Fold 14, VarNum 10, Method XGB, Model CART, Selected Features: [ 33  63  42 108  51   5  67  70 132 122]\n",
      "Fold 14, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [89 30 73 69 22 29  2 28  5 47]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143   0.03310274\n",
      " 0.03033257 0.02782268 0.0271608  0.02617155]\n",
      "Fold 15, VarNum 10, Method XGB, Model CART, Selected Features: [89 30 73 69 22 29  2 28  5 47]\n",
      "Fold 15, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7  35   6 107 120   8]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619 0.03208331\n",
      " 0.03144437 0.02985148 0.0276808  0.02635875]\n",
      "Fold 16, VarNum 10, Method XGB, Model CART, Selected Features: [115  88  32 109   7  35   6 107 120   8]\n",
      "Fold 16, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74  15  11  21  90  65]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458 0.02770768\n",
      " 0.02723359 0.02665008 0.02389776 0.02333322]\n",
      "Fold 17, VarNum 10, Method XGB, Model CART, Selected Features: [112   2  36 113  74  15  11  21  90  65]\n",
      "Fold 17, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32 118  48 113  73 131]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386 0.03553027\n",
      " 0.03012871 0.02946849 0.02732415 0.02354965]\n",
      "Fold 18, VarNum 10, Method XGB, Model CART, Selected Features: [112  31 110 130  32 118  48 113  73 131]\n",
      "Fold 18, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131 115 106  32  30  42]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978 0.02677654\n",
      " 0.02664938 0.02544507 0.02473148 0.02245516]\n",
      "Fold 19, VarNum 10, Method XGB, Model CART, Selected Features: [ 16  12  89  80 131 115 106  32  30  42]\n",
      "Fold 19, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:22:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135 105  44  30  91  43]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441 0.0345871\n",
      " 0.03290356 0.02876632 0.02770249 0.02665301]\n",
      "Fold 20, VarNum 10, Method XGB, Model CART, Selected Features: [119  48   8  25 135 105  44  30  91  43]\n",
      "Fold 20, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47619047619047616\n",
      "[15:22:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109  44 100  32 124  25]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843 0.03087765\n",
      " 0.0301982  0.02749008 0.02651894 0.02543698]\n",
      "Fold 21, VarNum 10, Method XGB, Model CART, Selected Features: [ 30  15 129 105 109  44 100  32 124  25]\n",
      "Fold 21, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:22:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594]\n",
      "Fold 0, VarNum 10, Method XGB, Model RF, Selected Features: [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Fold 0, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:22:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1  52  49  43  97 134  65 106 101  95]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752 0.03626129\n",
      " 0.02896795 0.02821424 0.02584204 0.02422096]\n",
      "Fold 1, VarNum 10, Method XGB, Model RF, Selected Features: [  1  52  49  43  97 134  65 106 101  95]\n",
      "Fold 1, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "[15:22:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35  32  71 106  97 109]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024 0.03934522\n",
      " 0.03732697 0.03702934 0.03554991 0.02754998]\n",
      "Fold 2, VarNum 10, Method XGB, Model RF, Selected Features: [ 54  93  92 127  35  32  71 106  97 109]\n",
      "Fold 2, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "[15:22:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71  97  29 131 105  37]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887 0.03330989\n",
      " 0.03321506 0.03237444 0.03195391 0.0294292 ]\n",
      "Fold 3, VarNum 10, Method XGB, Model RF, Selected Features: [127  19 112 111  71  97  29 131 105  37]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "[15:22:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81  92 105  30 104  95]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301 0.03296668\n",
      " 0.03100923 0.02906076 0.02756156 0.02643843]\n",
      "Fold 4, VarNum 10, Method XGB, Model RF, Selected Features: [ 32  61 106  29  81  92 105  30 104  95]\n",
      "Fold 4, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:22:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 37  63  23  72   6 107  68 106 131   0]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091 0.03251973\n",
      " 0.02970542 0.02945096 0.02717068 0.02702343]\n",
      "Fold 5, VarNum 10, Method XGB, Model RF, Selected Features: [ 37  63  23  72   6 107  68 106 131   0]\n",
      "Fold 5, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:22:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63  57 127   5  17  32]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347 0.04153319\n",
      " 0.04102546 0.03950502 0.03855329 0.03679097]\n",
      "Fold 6, VarNum 10, Method XGB, Model RF, Selected Features: [107 105 122  52  63  57 127   5  17  32]\n",
      "Fold 6, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:22:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37 127  35  51  63  73]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871 0.03215658\n",
      " 0.03175694 0.0314934  0.02671719 0.0256332 ]\n",
      "Fold 7, VarNum 10, Method XGB, Model RF, Selected Features: [  1 107  80 115  37 127  35  51  63  73]\n",
      "Fold 7, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:22:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107  33  17  30  75  54]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188 0.04792891\n",
      " 0.03837226 0.03728842 0.0318395  0.03169723]\n",
      "Fold 8, VarNum 10, Method XGB, Model RF, Selected Features: [108  41 110  35 107  33  17  30  75  54]\n",
      "Fold 8, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:22:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35  30  17 121  15 109]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769 0.04237923\n",
      " 0.0411956  0.03040076 0.0301668  0.02735102]\n",
      "Fold 9, VarNum 10, Method XGB, Model RF, Selected Features: [126 112  27  70  35  30  17 121  15 109]\n",
      "Fold 9, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:22:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69  56  42 111  65  27]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013 0.04062516\n",
      " 0.04024033 0.03461215 0.03025046 0.02787456]\n",
      "Fold 10, VarNum 10, Method XGB, Model RF, Selected Features: [ 35  50 132  33  69  56  42 111  65  27]\n",
      "Fold 10, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:22:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28 128  30  93  87  64]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683 0.03494062\n",
      " 0.03379305 0.03272084 0.03186584 0.03093796]\n",
      "Fold 11, VarNum 10, Method XGB, Model RF, Selected Features: [ 33  92  24 131  28 128  30  93  87  64]\n",
      "Fold 11, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:22:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [33 25 17  8 21 98 35 74 32  9]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572 0.03954697\n",
      " 0.0381116  0.03652066 0.03586444 0.03134957]\n",
      "Fold 12, VarNum 10, Method XGB, Model RF, Selected Features: [33 25 17  8 21 98 35 74 32  9]\n",
      "Fold 12, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:22:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65 124  32 100  95 128]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192 0.0375411\n",
      " 0.0296344  0.028289   0.02809397 0.02647186]\n",
      "Fold 13, VarNum 10, Method XGB, Model RF, Selected Features: [125  33  47  26  65 124  32 100  95 128]\n",
      "Fold 13, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:22:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51   5  67  70 132 122]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805 0.03213321\n",
      " 0.03205619 0.02940755 0.02676875 0.02317305]\n",
      "Fold 14, VarNum 10, Method XGB, Model RF, Selected Features: [ 33  63  42 108  51   5  67  70 132 122]\n",
      "Fold 14, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:22:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [89 30 73 69 22 29  2 28  5 47]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143   0.03310274\n",
      " 0.03033257 0.02782268 0.0271608  0.02617155]\n",
      "Fold 15, VarNum 10, Method XGB, Model RF, Selected Features: [89 30 73 69 22 29  2 28  5 47]\n",
      "Fold 15, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:22:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7  35   6 107 120   8]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619 0.03208331\n",
      " 0.03144437 0.02985148 0.0276808  0.02635875]\n",
      "Fold 16, VarNum 10, Method XGB, Model RF, Selected Features: [115  88  32 109   7  35   6 107 120   8]\n",
      "Fold 16, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:22:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74  15  11  21  90  65]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458 0.02770768\n",
      " 0.02723359 0.02665008 0.02389776 0.02333322]\n",
      "Fold 17, VarNum 10, Method XGB, Model RF, Selected Features: [112   2  36 113  74  15  11  21  90  65]\n",
      "Fold 17, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:22:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32 118  48 113  73 131]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386 0.03553027\n",
      " 0.03012871 0.02946849 0.02732415 0.02354965]\n",
      "Fold 18, VarNum 10, Method XGB, Model RF, Selected Features: [112  31 110 130  32 118  48 113  73 131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 18, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:22:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131 115 106  32  30  42]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978 0.02677654\n",
      " 0.02664938 0.02544507 0.02473148 0.02245516]\n",
      "Fold 19, VarNum 10, Method XGB, Model RF, Selected Features: [ 16  12  89  80 131 115 106  32  30  42]\n",
      "Fold 19, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:22:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135 105  44  30  91  43]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441 0.0345871\n",
      " 0.03290356 0.02876632 0.02770249 0.02665301]\n",
      "Fold 20, VarNum 10, Method XGB, Model RF, Selected Features: [119  48   8  25 135 105  44  30  91  43]\n",
      "Fold 20, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "[15:22:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109  44 100  32 124  25]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843 0.03087765\n",
      " 0.0301982  0.02749008 0.02651894 0.02543698]\n",
      "Fold 21, VarNum 10, Method XGB, Model RF, Selected Features: [ 30  15 129 105 109  44 100  32 124  25]\n",
      "Fold 21, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "[15:22:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594]\n",
      "Fold 0, VarNum 10, Method XGB, Model XGB, Selected Features: [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Fold 0, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:22:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1  52  49  43  97 134  65 106 101  95]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752 0.03626129\n",
      " 0.02896795 0.02821424 0.02584204 0.02422096]\n",
      "Fold 1, VarNum 10, Method XGB, Model XGB, Selected Features: [  1  52  49  43  97 134  65 106 101  95]\n",
      "Fold 1, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "[15:22:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35  32  71 106  97 109]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024 0.03934522\n",
      " 0.03732697 0.03702934 0.03554991 0.02754998]\n",
      "Fold 2, VarNum 10, Method XGB, Model XGB, Selected Features: [ 54  93  92 127  35  32  71 106  97 109]\n",
      "Fold 2, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "[15:22:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71  97  29 131 105  37]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887 0.03330989\n",
      " 0.03321506 0.03237444 0.03195391 0.0294292 ]\n",
      "Fold 3, VarNum 10, Method XGB, Model XGB, Selected Features: [127  19 112 111  71  97  29 131 105  37]\n",
      "Fold 3, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.8695652173913043\n",
      "[15:22:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81  92 105  30 104  95]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301 0.03296668\n",
      " 0.03100923 0.02906076 0.02756156 0.02643843]\n",
      "Fold 4, VarNum 10, Method XGB, Model XGB, Selected Features: [ 32  61 106  29  81  92 105  30 104  95]\n",
      "Fold 4, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:22:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 37  63  23  72   6 107  68 106 131   0]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091 0.03251973\n",
      " 0.02970542 0.02945096 0.02717068 0.02702343]\n",
      "Fold 5, VarNum 10, Method XGB, Model XGB, Selected Features: [ 37  63  23  72   6 107  68 106 131   0]\n",
      "Fold 5, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:22:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63  57 127   5  17  32]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347 0.04153319\n",
      " 0.04102546 0.03950502 0.03855329 0.03679097]\n",
      "Fold 6, VarNum 10, Method XGB, Model XGB, Selected Features: [107 105 122  52  63  57 127   5  17  32]\n",
      "Fold 6, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "[15:22:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37 127  35  51  63  73]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871 0.03215658\n",
      " 0.03175694 0.0314934  0.02671719 0.0256332 ]\n",
      "Fold 7, VarNum 10, Method XGB, Model XGB, Selected Features: [  1 107  80 115  37 127  35  51  63  73]\n",
      "Fold 7, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:22:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107  33  17  30  75  54]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188 0.04792891\n",
      " 0.03837226 0.03728842 0.0318395  0.03169723]\n",
      "Fold 8, VarNum 10, Method XGB, Model XGB, Selected Features: [108  41 110  35 107  33  17  30  75  54]\n",
      "Fold 8, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:22:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35  30  17 121  15 109]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769 0.04237923\n",
      " 0.0411956  0.03040076 0.0301668  0.02735102]\n",
      "Fold 9, VarNum 10, Method XGB, Model XGB, Selected Features: [126 112  27  70  35  30  17 121  15 109]\n",
      "Fold 9, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:22:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69  56  42 111  65  27]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013 0.04062516\n",
      " 0.04024033 0.03461215 0.03025046 0.02787456]\n",
      "Fold 10, VarNum 10, Method XGB, Model XGB, Selected Features: [ 35  50 132  33  69  56  42 111  65  27]\n",
      "Fold 10, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:22:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28 128  30  93  87  64]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683 0.03494062\n",
      " 0.03379305 0.03272084 0.03186584 0.03093796]\n",
      "Fold 11, VarNum 10, Method XGB, Model XGB, Selected Features: [ 33  92  24 131  28 128  30  93  87  64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 11, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:22:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [33 25 17  8 21 98 35 74 32  9]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572 0.03954697\n",
      " 0.0381116  0.03652066 0.03586444 0.03134957]\n",
      "Fold 12, VarNum 10, Method XGB, Model XGB, Selected Features: [33 25 17  8 21 98 35 74 32  9]\n",
      "Fold 12, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:22:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65 124  32 100  95 128]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192 0.0375411\n",
      " 0.0296344  0.028289   0.02809397 0.02647186]\n",
      "Fold 13, VarNum 10, Method XGB, Model XGB, Selected Features: [125  33  47  26  65 124  32 100  95 128]\n",
      "Fold 13, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:22:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51   5  67  70 132 122]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805 0.03213321\n",
      " 0.03205619 0.02940755 0.02676875 0.02317305]\n",
      "Fold 14, VarNum 10, Method XGB, Model XGB, Selected Features: [ 33  63  42 108  51   5  67  70 132 122]\n",
      "Fold 14, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:22:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [89 30 73 69 22 29  2 28  5 47]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143   0.03310274\n",
      " 0.03033257 0.02782268 0.0271608  0.02617155]\n",
      "Fold 15, VarNum 10, Method XGB, Model XGB, Selected Features: [89 30 73 69 22 29  2 28  5 47]\n",
      "Fold 15, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "[15:22:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7  35   6 107 120   8]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619 0.03208331\n",
      " 0.03144437 0.02985148 0.0276808  0.02635875]\n",
      "Fold 16, VarNum 10, Method XGB, Model XGB, Selected Features: [115  88  32 109   7  35   6 107 120   8]\n",
      "Fold 16, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "[15:22:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74  15  11  21  90  65]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458 0.02770768\n",
      " 0.02723359 0.02665008 0.02389776 0.02333322]\n",
      "Fold 17, VarNum 10, Method XGB, Model XGB, Selected Features: [112   2  36 113  74  15  11  21  90  65]\n",
      "Fold 17, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.45\n",
      "[15:22:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32 118  48 113  73 131]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386 0.03553027\n",
      " 0.03012871 0.02946849 0.02732415 0.02354965]\n",
      "Fold 18, VarNum 10, Method XGB, Model XGB, Selected Features: [112  31 110 130  32 118  48 113  73 131]\n",
      "Fold 18, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:22:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131 115 106  32  30  42]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978 0.02677654\n",
      " 0.02664938 0.02544507 0.02473148 0.02245516]\n",
      "Fold 19, VarNum 10, Method XGB, Model XGB, Selected Features: [ 16  12  89  80 131 115 106  32  30  42]\n",
      "Fold 19, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:22:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135 105  44  30  91  43]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441 0.0345871\n",
      " 0.03290356 0.02876632 0.02770249 0.02665301]\n",
      "Fold 20, VarNum 10, Method XGB, Model XGB, Selected Features: [119  48   8  25 135 105  44  30  91  43]\n",
      "Fold 20, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "[15:22:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109  44 100  32 124  25]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843 0.03087765\n",
      " 0.0301982  0.02749008 0.02651894 0.02543698]\n",
      "Fold 21, VarNum 10, Method XGB, Model XGB, Selected Features: [ 30  15 129 105 109  44 100  32 124  25]\n",
      "Fold 21, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:22:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594]\n",
      "Fold 0, VarNum 10, Method XGB, Model LDA, Selected Features: [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Fold 0, VarNum 10, Method XGB, Model LDA, Train Acc: 0.696, Test Acc: 0.5714285714285714\n",
      "[15:22:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1  52  49  43  97 134  65 106 101  95]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752 0.03626129\n",
      " 0.02896795 0.02821424 0.02584204 0.02422096]\n",
      "Fold 1, VarNum 10, Method XGB, Model LDA, Selected Features: [  1  52  49  43  97 134  65 106 101  95]\n",
      "Fold 1, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6774193548387096, Test Acc: 0.5217391304347826\n",
      "[15:22:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35  32  71 106  97 109]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024 0.03934522\n",
      " 0.03732697 0.03702934 0.03554991 0.02754998]\n",
      "Fold 2, VarNum 10, Method XGB, Model LDA, Selected Features: [ 54  93  92 127  35  32  71 106  97 109]\n",
      "Fold 2, VarNum 10, Method XGB, Model LDA, Train Acc: 0.71875, Test Acc: 0.8421052631578947\n",
      "[15:22:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71  97  29 131 105  37]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887 0.03330989\n",
      " 0.03321506 0.03237444 0.03195391 0.0294292 ]\n",
      "Fold 3, VarNum 10, Method XGB, Model LDA, Selected Features: [127  19 112 111  71  97  29 131 105  37]\n",
      "Fold 3, VarNum 10, Method XGB, Model LDA, Train Acc: 0.712, Test Acc: 0.6956521739130435\n",
      "[15:22:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81  92 105  30 104  95]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301 0.03296668\n",
      " 0.03100923 0.02906076 0.02756156 0.02643843]\n",
      "Fold 4, VarNum 10, Method XGB, Model LDA, Selected Features: [ 32  61 106  29  81  92 105  30 104  95]\n",
      "Fold 4, VarNum 10, Method XGB, Model LDA, Train Acc: 0.75, Test Acc: 0.7619047619047619\n",
      "[15:22:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 37  63  23  72   6 107  68 106 131   0]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091 0.03251973\n",
      " 0.02970542 0.02945096 0.02717068 0.02702343]\n",
      "Fold 5, VarNum 10, Method XGB, Model LDA, Selected Features: [ 37  63  23  72   6 107  68 106 131   0]\n",
      "Fold 5, VarNum 10, Method XGB, Model LDA, Train Acc: 0.8031496062992126, Test Acc: 0.5263157894736842\n",
      "[15:22:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63  57 127   5  17  32]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347 0.04153319\n",
      " 0.04102546 0.03950502 0.03855329 0.03679097]\n",
      "Fold 6, VarNum 10, Method XGB, Model LDA, Selected Features: [107 105 122  52  63  57 127   5  17  32]\n",
      "Fold 6, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7380952380952381, Test Acc: 0.5454545454545454\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37 127  35  51  63  73]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871 0.03215658\n",
      " 0.03175694 0.0314934  0.02671719 0.0256332 ]\n",
      "Fold 7, VarNum 10, Method XGB, Model LDA, Selected Features: [  1 107  80 115  37 127  35  51  63  73]\n",
      "Fold 7, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7637795275590551, Test Acc: 0.8421052631578947\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107  33  17  30  75  54]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188 0.04792891\n",
      " 0.03837226 0.03728842 0.0318395  0.03169723]\n",
      "Fold 8, VarNum 10, Method XGB, Model LDA, Selected Features: [108  41 110  35 107  33  17  30  75  54]\n",
      "Fold 8, VarNum 10, Method XGB, Model LDA, Train Acc: 0.8292682926829268, Test Acc: 0.7619047619047619\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35  30  17 121  15 109]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769 0.04237923\n",
      " 0.0411956  0.03040076 0.0301668  0.02735102]\n",
      "Fold 9, VarNum 10, Method XGB, Model LDA, Selected Features: [126 112  27  70  35  30  17 121  15 109]\n",
      "Fold 9, VarNum 10, Method XGB, Model LDA, Train Acc: 0.744, Test Acc: 0.5238095238095238\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69  56  42 111  65  27]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013 0.04062516\n",
      " 0.04024033 0.03461215 0.03025046 0.02787456]\n",
      "Fold 10, VarNum 10, Method XGB, Model LDA, Selected Features: [ 35  50 132  33  69  56  42 111  65  27]\n",
      "Fold 10, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7235772357723578, Test Acc: 0.5454545454545454\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28 128  30  93  87  64]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683 0.03494062\n",
      " 0.03379305 0.03272084 0.03186584 0.03093796]\n",
      "Fold 11, VarNum 10, Method XGB, Model LDA, Selected Features: [ 33  92  24 131  28 128  30  93  87  64]\n",
      "Fold 11, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7580645161290323, Test Acc: 0.5789473684210527\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [33 25 17  8 21 98 35 74 32  9]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572 0.03954697\n",
      " 0.0381116  0.03652066 0.03586444 0.03134957]\n",
      "Fold 12, VarNum 10, Method XGB, Model LDA, Selected Features: [33 25 17  8 21 98 35 74 32  9]\n",
      "Fold 12, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7741935483870968, Test Acc: 0.5909090909090909\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65 124  32 100  95 128]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192 0.0375411\n",
      " 0.0296344  0.028289   0.02809397 0.02647186]\n",
      "Fold 13, VarNum 10, Method XGB, Model LDA, Selected Features: [125  33  47  26  65 124  32 100  95 128]\n",
      "Fold 13, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6612903225806451, Test Acc: 0.45454545454545453\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51   5  67  70 132 122]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805 0.03213321\n",
      " 0.03205619 0.02940755 0.02676875 0.02317305]\n",
      "Fold 14, VarNum 10, Method XGB, Model LDA, Selected Features: [ 33  63  42 108  51   5  67  70 132 122]\n",
      "Fold 14, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6692913385826772, Test Acc: 0.6\n",
      "[15:22:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [89 30 73 69 22 29  2 28  5 47]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143   0.03310274\n",
      " 0.03033257 0.02782268 0.0271608  0.02617155]\n",
      "Fold 15, VarNum 10, Method XGB, Model LDA, Selected Features: [89 30 73 69 22 29  2 28  5 47]\n",
      "Fold 15, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6349206349206349, Test Acc: 0.6521739130434783\n",
      "[15:22:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7  35   6 107 120   8]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619 0.03208331\n",
      " 0.03144437 0.02985148 0.0276808  0.02635875]\n",
      "Fold 16, VarNum 10, Method XGB, Model LDA, Selected Features: [115  88  32 109   7  35   6 107 120   8]\n",
      "Fold 16, VarNum 10, Method XGB, Model LDA, Train Acc: 0.734375, Test Acc: 0.6\n",
      "[15:22:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74  15  11  21  90  65]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458 0.02770768\n",
      " 0.02723359 0.02665008 0.02389776 0.02333322]\n",
      "Fold 17, VarNum 10, Method XGB, Model LDA, Selected Features: [112   2  36 113  74  15  11  21  90  65]\n",
      "Fold 17, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6904761904761905, Test Acc: 0.75\n",
      "[15:22:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32 118  48 113  73 131]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386 0.03553027\n",
      " 0.03012871 0.02946849 0.02732415 0.02354965]\n",
      "Fold 18, VarNum 10, Method XGB, Model LDA, Selected Features: [112  31 110 130  32 118  48 113  73 131]\n",
      "Fold 18, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7165354330708661, Test Acc: 0.45454545454545453\n",
      "[15:22:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 16  12  89  80 131 115 106  32  30  42]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978 0.02677654\n",
      " 0.02664938 0.02544507 0.02473148 0.02245516]\n",
      "Fold 19, VarNum 10, Method XGB, Model LDA, Selected Features: [ 16  12  89  80 131 115 106  32  30  42]\n",
      "Fold 19, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6692913385826772, Test Acc: 0.3684210526315789\n",
      "[15:22:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135 105  44  30  91  43]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441 0.0345871\n",
      " 0.03290356 0.02876632 0.02770249 0.02665301]\n",
      "Fold 20, VarNum 10, Method XGB, Model LDA, Selected Features: [119  48   8  25 135 105  44  30  91  43]\n",
      "Fold 20, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6854838709677419, Test Acc: 0.47619047619047616\n",
      "[15:22:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109  44 100  32 124  25]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843 0.03087765\n",
      " 0.0301982  0.02749008 0.02651894 0.02543698]\n",
      "Fold 21, VarNum 10, Method XGB, Model LDA, Selected Features: [ 30  15 129 105 109  44 100  32 124  25]\n",
      "Fold 21, VarNum 10, Method XGB, Model LDA, Train Acc: 0.68, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model LDA, Train Acc: 0.776, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model LDA, Train Acc: 0.782258064516129, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7734375, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model LDA, Train Acc: 0.768, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model LDA, Train Acc: 0.8046875, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7795275590551181, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7857142857142857, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7952755905511811, Test Acc: 0.7368421052631579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7967479674796748, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model LDA, Train Acc: 0.8, Test Acc: 0.5238095238095238\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7642276422764228, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7983870967741935, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model LDA, Train Acc: 0.8145161290322581, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7580645161290323, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7559055118110236, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model LDA, Train Acc: 0.746031746031746, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7421875, Test Acc: 0.8\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7142857142857143, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model LDA, Train Acc: 0.6929133858267716, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7165354330708661, Test Acc: 0.42105263157894735\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model LDA, Train Acc: 0.6612903225806451, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model LDA, Train Acc: 0.688, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model CART, Selected Features: [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  55  87 100 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  55  87 100 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.782608695652174\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  87 100 105 106 107 127 131]\n",
      "Fold 2, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  25  30  31  32  33  35  87 100 105 106 107 127 131]\n",
      "Fold 2, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  39  87 106 107 121 127 129 131 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  39  87 106 107 121 127 129 131 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37  39 106 107 122 127 129 131 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  37  39 106 107 122 127 129 131 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 106 107 109 122 123 127 129 131 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35 106 107 109 122 123 127 129 131 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [ 25  30  31  32  33  35  50 106 107 109 122 123 127 131 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model CART, Selected Features: [ 25  30  31  32  33  35  50 106 107 109 122 123 127 131 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39  55  73  76 106 107 127]\n",
      "Fold 7, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  25  30  31  32  33  35  37  39  55  73  76 106 107 127]\n",
      "Fold 7, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  36  37  38  39  55 106 107 127]\n",
      "Fold 8, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  30  31  32  33  35  36  37  38  39  55 106 107 127]\n",
      "Fold 8, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [  1   3   4  21  25  30  31  32  33  35  36  38  55 109 127]\n",
      "Fold 9, VarNum 15, Method CHI2, Model CART, Selected Features: [  1   3   4  21  25  30  31  32  33  35  36  38  55 109 127]\n",
      "Fold 9, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [17 18 21 22 25 30 31 32 33 35 36 37 38 39 55]\n",
      "Fold 10, VarNum 15, Method CHI2, Model CART, Selected Features: [17 18 21 22 25 30 31 32 33 35 36 37 38 39 55]\n",
      "Fold 10, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 11, VarNum 15, Method CHI2, Model CART, Selected Features: [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 11, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 12, VarNum 15, Method CHI2, Model CART, Selected Features: [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 12, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 2  3 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 13, VarNum 15, Method CHI2, Model CART, Selected Features: [ 2  3 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 13, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (ChiSquareFS): [17 20 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 14, VarNum 15, Method CHI2, Model CART, Selected Features: [17 20 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 14, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [20 21 22 25 30 32 33 35 36 38 39 46 51 53 56]\n",
      "Fold 15, VarNum 15, Method CHI2, Model CART, Selected Features: [20 21 22 25 30 32 33 35 36 38 39 46 51 53 56]\n",
      "Fold 15, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (ChiSquareFS): [ 20  21  22  25  30  31  32  33  35  36  37  38  39 106 107]\n",
      "Fold 16, VarNum 15, Method CHI2, Model CART, Selected Features: [ 20  21  22  25  30  31  32  33  35  36  37  38  39 106 107]\n",
      "Fold 16, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [  1   2  21  22  25  30  32  33  35  36  38  87 106 107 109]\n",
      "Fold 17, VarNum 15, Method CHI2, Model CART, Selected Features: [  1   2  21  22  25  30  32  33  35  36  38  87 106 107 109]\n",
      "Fold 17, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  51 106 107 110 114 118 122 127 134]\n",
      "Fold 18, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  51 106 107 110 114 118 122 127 134]\n",
      "Fold 18, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.36363636363636365\n",
      "Selected features (ChiSquareFS): [ 18  21  22  25  30  31  32  33  35  36  38 106 107 122 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model CART, Selected Features: [ 18  21  22  25  30  31  32  33  35  36  38 106 107 122 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "Selected features (ChiSquareFS): [ 9 10 13 14 18 21 22 30 31 32 33 35 38 51 88]\n",
      "Fold 20, VarNum 15, Method CHI2, Model CART, Selected Features: [ 9 10 13 14 18 21 22 30 31 32 33 35 38 51 88]\n",
      "Fold 20, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (ChiSquareFS): [ 9 10 13 18 21 22 28 30 31 32 33 35 38 55 73]\n",
      "Fold 21, VarNum 15, Method CHI2, Model CART, Selected Features: [ 9 10 13 18 21 22 28 30 31 32 33 35 38 55 73]\n",
      "Fold 21, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.3181818181818182\n",
      "Selected features (ChiSquareFS): [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model RF, Selected Features: [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  55  87 100 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  55  87 100 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.782608695652174\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  87 100 105 106 107 127 131]\n",
      "Fold 2, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  87 100 105 106 107 127 131]\n",
      "Fold 2, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  39  87 106 107 121 127 129 131 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  39  87 106 107 121 127 129 131 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37  39 106 107 122 127 129 131 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  37  39 106 107 122 127 129 131 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 106 107 109 122 123 127 129 131 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35 106 107 109 122 123 127 129 131 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [ 25  30  31  32  33  35  50 106 107 109 122 123 127 131 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model RF, Selected Features: [ 25  30  31  32  33  35  50 106 107 109 122 123 127 131 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39  55  73  76 106 107 127]\n",
      "Fold 7, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  25  30  31  32  33  35  37  39  55  73  76 106 107 127]\n",
      "Fold 7, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  36  37  38  39  55 106 107 127]\n",
      "Fold 8, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  30  31  32  33  35  36  37  38  39  55 106 107 127]\n",
      "Fold 8, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [  1   3   4  21  25  30  31  32  33  35  36  38  55 109 127]\n",
      "Fold 9, VarNum 15, Method CHI2, Model RF, Selected Features: [  1   3   4  21  25  30  31  32  33  35  36  38  55 109 127]\n",
      "Fold 9, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (ChiSquareFS): [17 18 21 22 25 30 31 32 33 35 36 37 38 39 55]\n",
      "Fold 10, VarNum 15, Method CHI2, Model RF, Selected Features: [17 18 21 22 25 30 31 32 33 35 36 37 38 39 55]\n",
      "Fold 10, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 11, VarNum 15, Method CHI2, Model RF, Selected Features: [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 11, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 12, VarNum 15, Method CHI2, Model RF, Selected Features: [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 12, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [ 2  3 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 13, VarNum 15, Method CHI2, Model RF, Selected Features: [ 2  3 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 13, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [17 20 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 14, VarNum 15, Method CHI2, Model RF, Selected Features: [17 20 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 14, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (ChiSquareFS): [20 21 22 25 30 32 33 35 36 38 39 46 51 53 56]\n",
      "Fold 15, VarNum 15, Method CHI2, Model RF, Selected Features: [20 21 22 25 30 32 33 35 36 38 39 46 51 53 56]\n",
      "Fold 15, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.782608695652174\n",
      "Selected features (ChiSquareFS): [ 20  21  22  25  30  31  32  33  35  36  37  38  39 106 107]\n",
      "Fold 16, VarNum 15, Method CHI2, Model RF, Selected Features: [ 20  21  22  25  30  31  32  33  35  36  37  38  39 106 107]\n",
      "Fold 16, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [  1   2  21  22  25  30  32  33  35  36  38  87 106 107 109]\n",
      "Fold 17, VarNum 15, Method CHI2, Model RF, Selected Features: [  1   2  21  22  25  30  32  33  35  36  38  87 106 107 109]\n",
      "Fold 17, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  51 106 107 110 114 118 122 127 134]\n",
      "Fold 18, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  51 106 107 110 114 118 122 127 134]\n",
      "Fold 18, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (ChiSquareFS): [ 18  21  22  25  30  31  32  33  35  36  38 106 107 122 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model RF, Selected Features: [ 18  21  22  25  30  31  32  33  35  36  38 106 107 122 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.3684210526315789\n",
      "Selected features (ChiSquareFS): [ 9 10 13 14 18 21 22 30 31 32 33 35 38 51 88]\n",
      "Fold 20, VarNum 15, Method CHI2, Model RF, Selected Features: [ 9 10 13 14 18 21 22 30 31 32 33 35 38 51 88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 20, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 9 10 13 18 21 22 28 30 31 32 33 35 38 55 73]\n",
      "Fold 21, VarNum 15, Method CHI2, Model RF, Selected Features: [ 9 10 13 18 21 22 28 30 31 32 33 35 38 55 73]\n",
      "Fold 21, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.36363636363636365\n",
      "Selected features (ChiSquareFS): [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  55  87 100 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  55  87 100 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  87 100 105 106 107 127 131]\n",
      "Fold 2, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  87 100 105 106 107 127 131]\n",
      "Fold 2, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  39  87 106 107 121 127 129 131 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  39  87 106 107 121 127 129 131 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37  39 106 107 122 127 129 131 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  37  39 106 107 122 127 129 131 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 106 107 109 122 123 127 129 131 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35 106 107 109 122 123 127 129 131 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 25  30  31  32  33  35  50 106 107 109 122 123 127 131 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 25  30  31  32  33  35  50 106 107 109 122 123 127 131 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39  55  73  76 106 107 127]\n",
      "Fold 7, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  25  30  31  32  33  35  37  39  55  73  76 106 107 127]\n",
      "Fold 7, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  36  37  38  39  55 106 107 127]\n",
      "Fold 8, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  30  31  32  33  35  36  37  38  39  55 106 107 127]\n",
      "Fold 8, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [  1   3   4  21  25  30  31  32  33  35  36  38  55 109 127]\n",
      "Fold 9, VarNum 15, Method CHI2, Model XGB, Selected Features: [  1   3   4  21  25  30  31  32  33  35  36  38  55 109 127]\n",
      "Fold 9, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [17 18 21 22 25 30 31 32 33 35 36 37 38 39 55]\n",
      "Fold 10, VarNum 15, Method CHI2, Model XGB, Selected Features: [17 18 21 22 25 30 31 32 33 35 36 37 38 39 55]\n",
      "Fold 10, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 11, VarNum 15, Method CHI2, Model XGB, Selected Features: [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 11, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 12, VarNum 15, Method CHI2, Model XGB, Selected Features: [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 12, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [ 2  3 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 13, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 2  3 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 13, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [17 20 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 14, VarNum 15, Method CHI2, Model XGB, Selected Features: [17 20 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 14, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [20 21 22 25 30 32 33 35 36 38 39 46 51 53 56]\n",
      "Fold 15, VarNum 15, Method CHI2, Model XGB, Selected Features: [20 21 22 25 30 32 33 35 36 38 39 46 51 53 56]\n",
      "Fold 15, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 20  21  22  25  30  31  32  33  35  36  37  38  39 106 107]\n",
      "Fold 16, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 20  21  22  25  30  31  32  33  35  36  37  38  39 106 107]\n",
      "Fold 16, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [  1   2  21  22  25  30  32  33  35  36  38  87 106 107 109]\n",
      "Fold 17, VarNum 15, Method CHI2, Model XGB, Selected Features: [  1   2  21  22  25  30  32  33  35  36  38  87 106 107 109]\n",
      "Fold 17, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  51 106 107 110 114 118 122 127 134]\n",
      "Fold 18, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  51 106 107 110 114 118 122 127 134]\n",
      "Fold 18, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 18  21  22  25  30  31  32  33  35  36  38 106 107 122 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 18  21  22  25  30  31  32  33  35  36  38 106 107 122 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (ChiSquareFS): [ 9 10 13 14 18 21 22 30 31 32 33 35 38 51 88]\n",
      "Fold 20, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 9 10 13 14 18 21 22 30 31 32 33 35 38 51 88]\n",
      "Fold 20, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 9 10 13 18 21 22 28 30 31 32 33 35 38 55 73]\n",
      "Fold 21, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 9 10 13 18 21 22 28 30 31 32 33 35 38 55 73]\n",
      "Fold 21, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.736, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  55  87 100 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  55  87 100 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.8064516129032258, Test Acc: 0.8260869565217391\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  87 100 105 106 107 127 131]\n",
      "Fold 2, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  87 100 105 106 107 127 131]\n",
      "Fold 2, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.8359375, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  39  87 106 107 121 127 129 131 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  39  87 106 107 121 127 129 131 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.76, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37  39 106 107 122 127 129 131 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  37  39 106 107 122 127 129 131 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.765625, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 106 107 109 122 123 127 129 131 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35 106 107 109 122 123 127 129 131 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7637795275590551, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 25  30  31  32  33  35  50 106 107 109 122 123 127 131 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 25  30  31  32  33  35  50 106 107 109 122 123 127 131 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7619047619047619, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39  55  73  76 106 107 127]\n",
      "Fold 7, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  25  30  31  32  33  35  37  39  55  73  76 106 107 127]\n",
      "Fold 7, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7952755905511811, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  36  37  38  39  55 106 107 127]\n",
      "Fold 8, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  30  31  32  33  35  36  37  38  39  55 106 107 127]\n",
      "Fold 8, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.8211382113821138, Test Acc: 0.8095238095238095\n",
      "Selected features (ChiSquareFS): [  1   3   4  21  25  30  31  32  33  35  36  38  55 109 127]\n",
      "Fold 9, VarNum 15, Method CHI2, Model LDA, Selected Features: [  1   3   4  21  25  30  31  32  33  35  36  38  55 109 127]\n",
      "Fold 9, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.784, Test Acc: 0.3333333333333333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (ChiSquareFS): [17 18 21 22 25 30 31 32 33 35 36 37 38 39 55]\n",
      "Fold 10, VarNum 15, Method CHI2, Model LDA, Selected Features: [17 18 21 22 25 30 31 32 33 35 36 37 38 39 55]\n",
      "Fold 10, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7886178861788617, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 11, VarNum 15, Method CHI2, Model LDA, Selected Features: [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 11, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.8064516129032258, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 12, VarNum 15, Method CHI2, Model LDA, Selected Features: [17 21 22 25 30 31 32 33 35 36 37 38 39 40 57]\n",
      "Fold 12, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.8064516129032258, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 2  3 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 13, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 2  3 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 13, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7661290322580645, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [17 20 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 14, VarNum 15, Method CHI2, Model LDA, Selected Features: [17 20 21 22 25 30 31 32 33 35 36 37 38 39 56]\n",
      "Fold 14, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7322834645669292, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [20 21 22 25 30 32 33 35 36 38 39 46 51 53 56]\n",
      "Fold 15, VarNum 15, Method CHI2, Model LDA, Selected Features: [20 21 22 25 30 32 33 35 36 38 39 46 51 53 56]\n",
      "Fold 15, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7301587301587301, Test Acc: 0.43478260869565216\n",
      "Selected features (ChiSquareFS): [ 20  21  22  25  30  31  32  33  35  36  37  38  39 106 107]\n",
      "Fold 16, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 20  21  22  25  30  31  32  33  35  36  37  38  39 106 107]\n",
      "Fold 16, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7578125, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [  1   2  21  22  25  30  32  33  35  36  38  87 106 107 109]\n",
      "Fold 17, VarNum 15, Method CHI2, Model LDA, Selected Features: [  1   2  21  22  25  30  32  33  35  36  38  87 106 107 109]\n",
      "Fold 17, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.6825396825396826, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  51 106 107 110 114 118 122 127 134]\n",
      "Fold 18, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  51 106 107 110 114 118 122 127 134]\n",
      "Fold 18, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7401574803149606, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 18  21  22  25  30  31  32  33  35  36  38 106 107 122 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 18  21  22  25  30  31  32  33  35  36  38 106 107 122 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7007874015748031, Test Acc: 0.5263157894736842\n",
      "Selected features (ChiSquareFS): [ 9 10 13 14 18 21 22 30 31 32 33 35 38 51 88]\n",
      "Fold 20, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 9 10 13 14 18 21 22 30 31 32 33 35 38 51 88]\n",
      "Fold 20, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7096774193548387, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 9 10 13 18 21 22 28 30 31 32 33 35 38 55 73]\n",
      "Fold 21, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 9 10 13 18 21 22 28 30 31 32 33 35 38 55 73]\n",
      "Fold 21, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.688, Test Acc: 0.5\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252 0.01557481 0.01491991\n",
      " 0.01489731 0.01448407 0.01424924]\n",
      "Fold 0, VarNum 15, Method RF, Model CART, Selected Features: [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Fold 0, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [134 107 106 100 129  30 131 123 102 127  94 118 122 124  87]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054 0.02062351\n",
      " 0.02036939 0.02001713 0.01831205 0.01752115 0.01505652 0.01438327\n",
      " 0.01378803 0.01268187 0.01239187]\n",
      "Fold 1, VarNum 15, Method RF, Model CART, Selected Features: [134 107 106 100 129  30 131 123 102 127  94 118 122 124  87]\n",
      "Fold 1, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [107  30 105  33 129 106 100 127 102  98 134  32  93 109 101]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624 0.01825179\n",
      " 0.01749679 0.01692175 0.01671952 0.0158184  0.0155876  0.01517898\n",
      " 0.01467856 0.01402575 0.01341131]\n",
      "Fold 2, VarNum 15, Method RF, Model CART, Selected Features: [107  30 105  33 129 106 100 127 102  98 134  32  93 109 101]\n",
      "Fold 2, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [107 106 122 127  30  33 104 120 105 100  98   6 129 109 102]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713 0.02271285\n",
      " 0.02224426 0.01820072 0.01750092 0.01735569 0.01731945 0.01703932\n",
      " 0.01619163 0.0157258  0.01570642]\n",
      "Fold 3, VarNum 15, Method RF, Model CART, Selected Features: [107 106 122 127  30  33 104 120 105 100  98   6 129 109 102]\n",
      "Fold 3, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [106 107  32  30  33 129 109 120 127 100  39  83  98  97  37]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174   0.02665422\n",
      " 0.02360296 0.01937688 0.01756391 0.01635806 0.0153292  0.01398757\n",
      " 0.0135894  0.01347108 0.01311887]\n",
      "Fold 4, VarNum 15, Method RF, Model CART, Selected Features: [106 107  32  30  33 129 109 120 127 100  39  83  98  97  37]\n",
      "Fold 4, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [107 127 106 109  30 129 136 134  32  33 122  92 105 123 121]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567 0.01914487\n",
      " 0.01570817 0.0154738  0.01532336 0.01522347 0.01391631 0.01380123\n",
      " 0.01365285 0.01327018 0.01291883]\n",
      "Fold 5, VarNum 15, Method RF, Model CART, Selected Features: [107 127 106 109  30 129 136 134  32  33 122  92 105 123 121]\n",
      "Fold 5, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [106 107 127 109  32 118 122 134 131  89 132 123  90 133  33]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486 0.02450973\n",
      " 0.02411738 0.02402983 0.01708098 0.0164193  0.0162579  0.01446688\n",
      " 0.0124501  0.01211474 0.01210385]\n",
      "Fold 6, VarNum 15, Method RF, Model CART, Selected Features: [106 107 127 109  32 118 122 134 131  89 132 123  90 133  33]\n",
      "Fold 6, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [107 106 127  30 118 134  90 123 113 109 132 122  89  60  21]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774 0.02518253\n",
      " 0.02050878 0.0164142  0.01534288 0.01532239 0.01517528 0.01427025\n",
      " 0.01362907 0.01320932 0.01236264]\n",
      "Fold 7, VarNum 15, Method RF, Model CART, Selected Features: [107 106 127  30 118 134  90 123 113 109 132 122  89  60  21]\n",
      "Fold 7, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [107 106  30 109  33 134 122 123  57  35 118 132  31 127  71]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996 0.02325124\n",
      " 0.02306146 0.02232644 0.02068571 0.02016427 0.01968458 0.01627144\n",
      " 0.01481066 0.0136572  0.01333423]\n",
      "Fold 8, VarNum 15, Method RF, Model CART, Selected Features: [107 106  30 109  33 134 122 123  57  35 118 132  31 127  71]\n",
      "Fold 8, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107 132  35 106 123 118 134 127  33  32  12   1  57 133]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646 0.02035143\n",
      " 0.01837401 0.01697944 0.01689733 0.01622381 0.01464765 0.01423333\n",
      " 0.01394567 0.01301881 0.01254948]\n",
      "Fold 9, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 132  35 106 123 118 134 127  33  32  12   1  57 133]\n",
      "Fold 9, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7619047619047619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [ 30  32  33  35  57   6 113 118  25  54  21 106  29  70  73]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143  0.02133614\n",
      " 0.01841816 0.01703708 0.01599764 0.01544085 0.01512738 0.01430247\n",
      " 0.01390334 0.01380115 0.01363089]\n",
      "Fold 10, VarNum 15, Method RF, Model CART, Selected Features: [ 30  32  33  35  57   6 113 118  25  54  21 106  29  70  73]\n",
      "Fold 10, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 30  33  32  35  21  37  39  22  57  25  36 118  11 106  73]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159 0.01897151\n",
      " 0.01866083 0.01702243 0.01647859 0.01574145 0.01460072 0.01368665\n",
      " 0.01362177 0.01319057 0.01234622]\n",
      "Fold 11, VarNum 15, Method RF, Model CART, Selected Features: [ 30  33  32  35  21  37  39  22  57  25  36 118  11 106  73]\n",
      "Fold 11, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [ 33  35  30  21  32  22   3  39  25  37   6 107  57 106  36]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812 0.01944994\n",
      " 0.01731347 0.0168168  0.01443683 0.01385973 0.01244558 0.01190744\n",
      " 0.01164713 0.01156317 0.0113951 ]\n",
      "Fold 12, VarNum 15, Method RF, Model CART, Selected Features: [ 33  35  30  21  32  22   3  39  25  37   6 107  57 106  36]\n",
      "Fold 12, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [ 33  30  32  21  35  37 136 101 100 115  22  58  39  36  95]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099   0.01780627\n",
      " 0.0133239  0.01289129 0.0127552  0.01254642 0.01166191 0.01131257\n",
      " 0.01130641 0.01103137 0.01098019]\n",
      "Fold 13, VarNum 15, Method RF, Model CART, Selected Features: [ 33  30  32  21  35  37 136 101 100 115  22  58  39  36  95]\n",
      "Fold 13, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [ 33  30  35  32 127  21 106  22 121 105  57  40 115 101 129]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125 0.01578645\n",
      " 0.01528147 0.01479727 0.01284964 0.01243961 0.01229255 0.01227687\n",
      " 0.01201724 0.01201053 0.01200573]\n",
      "Fold 14, VarNum 15, Method RF, Model CART, Selected Features: [ 33  30  35  32 127  21 106  22 121 105  57  40 115 101 129]\n",
      "Fold 14, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.45\n",
      "Selected features (RF): [ 33  30 110  21 101  32  35 136  37 114 132   0  39  22 133]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285 0.015737\n",
      " 0.01503168 0.0140918  0.01361461 0.01344505 0.01343731 0.01327275\n",
      " 0.01313235 0.01247191 0.01237332]\n",
      "Fold 15, VarNum 15, Method RF, Model CART, Selected Features: [ 33  30 110  21 101  32  35 136  37 114 132   0  39  22 133]\n",
      "Fold 15, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (RF): [ 33 105  30  21  32 136 107  22 114 102  25 132   6   5 110]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172 0.01886913\n",
      " 0.01532578 0.01504565 0.01427528 0.01374455 0.0132633  0.01294786\n",
      " 0.01240264 0.01225477 0.01214028]\n",
      "Fold 16, VarNum 15, Method RF, Model CART, Selected Features: [ 33 105  30  21  32 136 107  22 114 102  25 132   6   5 110]\n",
      "Fold 16, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [122 113  30  21 134 105   6 109  16  32  25  33  22  89 101]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434 0.01428145\n",
      " 0.01421295 0.01348654 0.01343482 0.01305475 0.01281797 0.01266458\n",
      " 0.01259589 0.0125712  0.01216599]\n",
      "Fold 17, VarNum 15, Method RF, Model CART, Selected Features: [122 113  30  21 134 105   6 109  16  32  25  33  22  89 101]\n",
      "Fold 17, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (RF): [122 113  91 114  33 134  32 110  92  46 100 106 118  30 115]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344 0.01700055\n",
      " 0.01696383 0.01685772 0.016267   0.01624445 0.01546131 0.01529183\n",
      " 0.01484006 0.01473522 0.01353428]\n",
      "Fold 18, VarNum 15, Method RF, Model CART, Selected Features: [122 113  91 114  33 134  32 110  92  46 100 106 118  30 115]\n",
      "Fold 18, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [134  92 122  32  33 109  73 114 113 132 110  35 115 111 130]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174 0.01769571\n",
      " 0.01744237 0.01549418 0.01513342 0.01443422 0.01426394 0.01276479\n",
      " 0.01265933 0.01214103 0.01204304]\n",
      "Fold 19, VarNum 15, Method RF, Model CART, Selected Features: [134  92 122  32  33 109  73 114 113 132 110  35 115 111 130]\n",
      "Fold 19, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (RF): [ 30  32 134  33  91 114 119 112  85 130  36  38  59  35  58]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132  0.01416553\n",
      " 0.01375293 0.01374425 0.01317677 0.0129457  0.01195973 0.01192029\n",
      " 0.01190177 0.01163069 0.01161039]\n",
      "Fold 20, VarNum 15, Method RF, Model CART, Selected Features: [ 30  32 134  33  91 114 119 112  85 130  36  38  59  35  58]\n",
      "Fold 20, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.3333333333333333\n",
      "Selected features (RF): [ 30 136 100  73  98  32  31  21  33   6 123 134 105  72 110]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984 0.01694408\n",
      " 0.01582854 0.01479282 0.01409128 0.01360734 0.01359641 0.01359467\n",
      " 0.01315045 0.0128937  0.01257166]\n",
      "Fold 21, VarNum 15, Method RF, Model CART, Selected Features: [ 30 136 100  73  98  32  31  21  33   6 123 134 105  72 110]\n",
      "Fold 21, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252 0.01557481 0.01491991\n",
      " 0.01489731 0.01448407 0.01424924]\n",
      "Fold 0, VarNum 15, Method RF, Model RF, Selected Features: [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Fold 0, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [134 107 106 100 129  30 131 123 102 127  94 118 122 124  87]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054 0.02062351\n",
      " 0.02036939 0.02001713 0.01831205 0.01752115 0.01505652 0.01438327\n",
      " 0.01378803 0.01268187 0.01239187]\n",
      "Fold 1, VarNum 15, Method RF, Model RF, Selected Features: [134 107 106 100 129  30 131 123 102 127  94 118 122 124  87]\n",
      "Fold 1, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [107  30 105  33 129 106 100 127 102  98 134  32  93 109 101]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624 0.01825179\n",
      " 0.01749679 0.01692175 0.01671952 0.0158184  0.0155876  0.01517898\n",
      " 0.01467856 0.01402575 0.01341131]\n",
      "Fold 2, VarNum 15, Method RF, Model RF, Selected Features: [107  30 105  33 129 106 100 127 102  98 134  32  93 109 101]\n",
      "Fold 2, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [107 106 122 127  30  33 104 120 105 100  98   6 129 109 102]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713 0.02271285\n",
      " 0.02224426 0.01820072 0.01750092 0.01735569 0.01731945 0.01703932\n",
      " 0.01619163 0.0157258  0.01570642]\n",
      "Fold 3, VarNum 15, Method RF, Model RF, Selected Features: [107 106 122 127  30  33 104 120 105 100  98   6 129 109 102]\n",
      "Fold 3, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [106 107  32  30  33 129 109 120 127 100  39  83  98  97  37]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174   0.02665422\n",
      " 0.02360296 0.01937688 0.01756391 0.01635806 0.0153292  0.01398757\n",
      " 0.0135894  0.01347108 0.01311887]\n",
      "Fold 4, VarNum 15, Method RF, Model RF, Selected Features: [106 107  32  30  33 129 109 120 127 100  39  83  98  97  37]\n",
      "Fold 4, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.42857142857142855\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [107 127 106 109  30 129 136 134  32  33 122  92 105 123 121]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567 0.01914487\n",
      " 0.01570817 0.0154738  0.01532336 0.01522347 0.01391631 0.01380123\n",
      " 0.01365285 0.01327018 0.01291883]\n",
      "Fold 5, VarNum 15, Method RF, Model RF, Selected Features: [107 127 106 109  30 129 136 134  32  33 122  92 105 123 121]\n",
      "Fold 5, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [106 107 127 109  32 118 122 134 131  89 132 123  90 133  33]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486 0.02450973\n",
      " 0.02411738 0.02402983 0.01708098 0.0164193  0.0162579  0.01446688\n",
      " 0.0124501  0.01211474 0.01210385]\n",
      "Fold 6, VarNum 15, Method RF, Model RF, Selected Features: [106 107 127 109  32 118 122 134 131  89 132 123  90 133  33]\n",
      "Fold 6, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [107 106 127  30 118 134  90 123 113 109 132 122  89  60  21]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774 0.02518253\n",
      " 0.02050878 0.0164142  0.01534288 0.01532239 0.01517528 0.01427025\n",
      " 0.01362907 0.01320932 0.01236264]\n",
      "Fold 7, VarNum 15, Method RF, Model RF, Selected Features: [107 106 127  30 118 134  90 123 113 109 132 122  89  60  21]\n",
      "Fold 7, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [107 106  30 109  33 134 122 123  57  35 118 132  31 127  71]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996 0.02325124\n",
      " 0.02306146 0.02232644 0.02068571 0.02016427 0.01968458 0.01627144\n",
      " 0.01481066 0.0136572  0.01333423]\n",
      "Fold 8, VarNum 15, Method RF, Model RF, Selected Features: [107 106  30 109  33 134 122 123  57  35 118 132  31 127  71]\n",
      "Fold 8, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 132  35 106 123 118 134 127  33  32  12   1  57 133]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646 0.02035143\n",
      " 0.01837401 0.01697944 0.01689733 0.01622381 0.01464765 0.01423333\n",
      " 0.01394567 0.01301881 0.01254948]\n",
      "Fold 9, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 132  35 106 123 118 134 127  33  32  12   1  57 133]\n",
      "Fold 9, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30  32  33  35  57   6 113 118  25  54  21 106  29  70  73]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143  0.02133614\n",
      " 0.01841816 0.01703708 0.01599764 0.01544085 0.01512738 0.01430247\n",
      " 0.01390334 0.01380115 0.01363089]\n",
      "Fold 10, VarNum 15, Method RF, Model RF, Selected Features: [ 30  32  33  35  57   6 113 118  25  54  21 106  29  70  73]\n",
      "Fold 10, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30  33  32  35  21  37  39  22  57  25  36 118  11 106  73]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159 0.01897151\n",
      " 0.01866083 0.01702243 0.01647859 0.01574145 0.01460072 0.01368665\n",
      " 0.01362177 0.01319057 0.01234622]\n",
      "Fold 11, VarNum 15, Method RF, Model RF, Selected Features: [ 30  33  32  35  21  37  39  22  57  25  36 118  11 106  73]\n",
      "Fold 11, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (RF): [ 33  35  30  21  32  22   3  39  25  37   6 107  57 106  36]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812 0.01944994\n",
      " 0.01731347 0.0168168  0.01443683 0.01385973 0.01244558 0.01190744\n",
      " 0.01164713 0.01156317 0.0113951 ]\n",
      "Fold 12, VarNum 15, Method RF, Model RF, Selected Features: [ 33  35  30  21  32  22   3  39  25  37   6 107  57 106  36]\n",
      "Fold 12, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 33  30  32  21  35  37 136 101 100 115  22  58  39  36  95]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099   0.01780627\n",
      " 0.0133239  0.01289129 0.0127552  0.01254642 0.01166191 0.01131257\n",
      " 0.01130641 0.01103137 0.01098019]\n",
      "Fold 13, VarNum 15, Method RF, Model RF, Selected Features: [ 33  30  32  21  35  37 136 101 100 115  22  58  39  36  95]\n",
      "Fold 13, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 33  30  35  32 127  21 106  22 121 105  57  40 115 101 129]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125 0.01578645\n",
      " 0.01528147 0.01479727 0.01284964 0.01243961 0.01229255 0.01227687\n",
      " 0.01201724 0.01201053 0.01200573]\n",
      "Fold 14, VarNum 15, Method RF, Model RF, Selected Features: [ 33  30  35  32 127  21 106  22 121 105  57  40 115 101 129]\n",
      "Fold 14, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [ 33  30 110  21 101  32  35 136  37 114 132   0  39  22 133]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285 0.015737\n",
      " 0.01503168 0.0140918  0.01361461 0.01344505 0.01343731 0.01327275\n",
      " 0.01313235 0.01247191 0.01237332]\n",
      "Fold 15, VarNum 15, Method RF, Model RF, Selected Features: [ 33  30 110  21 101  32  35 136  37 114 132   0  39  22 133]\n",
      "Fold 15, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "Selected features (RF): [ 33 105  30  21  32 136 107  22 114 102  25 132   6   5 110]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172 0.01886913\n",
      " 0.01532578 0.01504565 0.01427528 0.01374455 0.0132633  0.01294786\n",
      " 0.01240264 0.01225477 0.01214028]\n",
      "Fold 16, VarNum 15, Method RF, Model RF, Selected Features: [ 33 105  30  21  32 136 107  22 114 102  25 132   6   5 110]\n",
      "Fold 16, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.75\n",
      "Selected features (RF): [122 113  30  21 134 105   6 109  16  32  25  33  22  89 101]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434 0.01428145\n",
      " 0.01421295 0.01348654 0.01343482 0.01305475 0.01281797 0.01266458\n",
      " 0.01259589 0.0125712  0.01216599]\n",
      "Fold 17, VarNum 15, Method RF, Model RF, Selected Features: [122 113  30  21 134 105   6 109  16  32  25  33  22  89 101]\n",
      "Fold 17, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (RF): [122 113  91 114  33 134  32 110  92  46 100 106 118  30 115]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344 0.01700055\n",
      " 0.01696383 0.01685772 0.016267   0.01624445 0.01546131 0.01529183\n",
      " 0.01484006 0.01473522 0.01353428]\n",
      "Fold 18, VarNum 15, Method RF, Model RF, Selected Features: [122 113  91 114  33 134  32 110  92  46 100 106 118  30 115]\n",
      "Fold 18, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [134  92 122  32  33 109  73 114 113 132 110  35 115 111 130]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174 0.01769571\n",
      " 0.01744237 0.01549418 0.01513342 0.01443422 0.01426394 0.01276479\n",
      " 0.01265933 0.01214103 0.01204304]\n",
      "Fold 19, VarNum 15, Method RF, Model RF, Selected Features: [134  92 122  32  33 109  73 114 113 132 110  35 115 111 130]\n",
      "Fold 19, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.3684210526315789\n",
      "Selected features (RF): [ 30  32 134  33  91 114 119 112  85 130  36  38  59  35  58]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132  0.01416553\n",
      " 0.01375293 0.01374425 0.01317677 0.0129457  0.01195973 0.01192029\n",
      " 0.01190177 0.01163069 0.01161039]\n",
      "Fold 20, VarNum 15, Method RF, Model RF, Selected Features: [ 30  32 134  33  91 114 119 112  85 130  36  38  59  35  58]\n",
      "Fold 20, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "Selected features (RF): [ 30 136 100  73  98  32  31  21  33   6 123 134 105  72 110]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984 0.01694408\n",
      " 0.01582854 0.01479282 0.01409128 0.01360734 0.01359641 0.01359467\n",
      " 0.01315045 0.0128937  0.01257166]\n",
      "Fold 21, VarNum 15, Method RF, Model RF, Selected Features: [ 30 136 100  73  98  32  31  21  33   6 123 134 105  72 110]\n",
      "Fold 21, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252 0.01557481 0.01491991\n",
      " 0.01489731 0.01448407 0.01424924]\n",
      "Fold 0, VarNum 15, Method RF, Model XGB, Selected Features: [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Fold 0, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [134 107 106 100 129  30 131 123 102 127  94 118 122 124  87]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054 0.02062351\n",
      " 0.02036939 0.02001713 0.01831205 0.01752115 0.01505652 0.01438327\n",
      " 0.01378803 0.01268187 0.01239187]\n",
      "Fold 1, VarNum 15, Method RF, Model XGB, Selected Features: [134 107 106 100 129  30 131 123 102 127  94 118 122 124  87]\n",
      "Fold 1, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [107  30 105  33 129 106 100 127 102  98 134  32  93 109 101]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624 0.01825179\n",
      " 0.01749679 0.01692175 0.01671952 0.0158184  0.0155876  0.01517898\n",
      " 0.01467856 0.01402575 0.01341131]\n",
      "Fold 2, VarNum 15, Method RF, Model XGB, Selected Features: [107  30 105  33 129 106 100 127 102  98 134  32  93 109 101]\n",
      "Fold 2, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.8947368421052632\n",
      "Selected features (RF): [107 106 122 127  30  33 104 120 105 100  98   6 129 109 102]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713 0.02271285\n",
      " 0.02224426 0.01820072 0.01750092 0.01735569 0.01731945 0.01703932\n",
      " 0.01619163 0.0157258  0.01570642]\n",
      "Fold 3, VarNum 15, Method RF, Model XGB, Selected Features: [107 106 122 127  30  33 104 120 105 100  98   6 129 109 102]\n",
      "Fold 3, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [106 107  32  30  33 129 109 120 127 100  39  83  98  97  37]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174   0.02665422\n",
      " 0.02360296 0.01937688 0.01756391 0.01635806 0.0153292  0.01398757\n",
      " 0.0135894  0.01347108 0.01311887]\n",
      "Fold 4, VarNum 15, Method RF, Model XGB, Selected Features: [106 107  32  30  33 129 109 120 127 100  39  83  98  97  37]\n",
      "Fold 4, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "Selected features (RF): [107 127 106 109  30 129 136 134  32  33 122  92 105 123 121]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567 0.01914487\n",
      " 0.01570817 0.0154738  0.01532336 0.01522347 0.01391631 0.01380123\n",
      " 0.01365285 0.01327018 0.01291883]\n",
      "Fold 5, VarNum 15, Method RF, Model XGB, Selected Features: [107 127 106 109  30 129 136 134  32  33 122  92 105 123 121]\n",
      "Fold 5, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [106 107 127 109  32 118 122 134 131  89 132 123  90 133  33]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486 0.02450973\n",
      " 0.02411738 0.02402983 0.01708098 0.0164193  0.0162579  0.01446688\n",
      " 0.0124501  0.01211474 0.01210385]\n",
      "Fold 6, VarNum 15, Method RF, Model XGB, Selected Features: [106 107 127 109  32 118 122 134 131  89 132 123  90 133  33]\n",
      "Fold 6, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [107 106 127  30 118 134  90 123 113 109 132 122  89  60  21]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774 0.02518253\n",
      " 0.02050878 0.0164142  0.01534288 0.01532239 0.01517528 0.01427025\n",
      " 0.01362907 0.01320932 0.01236264]\n",
      "Fold 7, VarNum 15, Method RF, Model XGB, Selected Features: [107 106 127  30 118 134  90 123 113 109 132 122  89  60  21]\n",
      "Fold 7, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [107 106  30 109  33 134 122 123  57  35 118 132  31 127  71]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996 0.02325124\n",
      " 0.02306146 0.02232644 0.02068571 0.02016427 0.01968458 0.01627144\n",
      " 0.01481066 0.0136572  0.01333423]\n",
      "Fold 8, VarNum 15, Method RF, Model XGB, Selected Features: [107 106  30 109  33 134 122 123  57  35 118 132  31 127  71]\n",
      "Fold 8, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [ 30 107 132  35 106 123 118 134 127  33  32  12   1  57 133]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646 0.02035143\n",
      " 0.01837401 0.01697944 0.01689733 0.01622381 0.01464765 0.01423333\n",
      " 0.01394567 0.01301881 0.01254948]\n",
      "Fold 9, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 132  35 106 123 118 134 127  33  32  12   1  57 133]\n",
      "Fold 9, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30  32  33  35  57   6 113 118  25  54  21 106  29  70  73]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143  0.02133614\n",
      " 0.01841816 0.01703708 0.01599764 0.01544085 0.01512738 0.01430247\n",
      " 0.01390334 0.01380115 0.01363089]\n",
      "Fold 10, VarNum 15, Method RF, Model XGB, Selected Features: [ 30  32  33  35  57   6 113 118  25  54  21 106  29  70  73]\n",
      "Fold 10, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [ 30  33  32  35  21  37  39  22  57  25  36 118  11 106  73]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159 0.01897151\n",
      " 0.01866083 0.01702243 0.01647859 0.01574145 0.01460072 0.01368665\n",
      " 0.01362177 0.01319057 0.01234622]\n",
      "Fold 11, VarNum 15, Method RF, Model XGB, Selected Features: [ 30  33  32  35  21  37  39  22  57  25  36 118  11 106  73]\n",
      "Fold 11, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 33  35  30  21  32  22   3  39  25  37   6 107  57 106  36]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812 0.01944994\n",
      " 0.01731347 0.0168168  0.01443683 0.01385973 0.01244558 0.01190744\n",
      " 0.01164713 0.01156317 0.0113951 ]\n",
      "Fold 12, VarNum 15, Method RF, Model XGB, Selected Features: [ 33  35  30  21  32  22   3  39  25  37   6 107  57 106  36]\n",
      "Fold 12, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 33  30  32  21  35  37 136 101 100 115  22  58  39  36  95]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099   0.01780627\n",
      " 0.0133239  0.01289129 0.0127552  0.01254642 0.01166191 0.01131257\n",
      " 0.01130641 0.01103137 0.01098019]\n",
      "Fold 13, VarNum 15, Method RF, Model XGB, Selected Features: [ 33  30  32  21  35  37 136 101 100 115  22  58  39  36  95]\n",
      "Fold 13, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 33  30  35  32 127  21 106  22 121 105  57  40 115 101 129]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125 0.01578645\n",
      " 0.01528147 0.01479727 0.01284964 0.01243961 0.01229255 0.01227687\n",
      " 0.01201724 0.01201053 0.01200573]\n",
      "Fold 14, VarNum 15, Method RF, Model XGB, Selected Features: [ 33  30  35  32 127  21 106  22 121 105  57  40 115 101 129]\n",
      "Fold 14, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.45\n",
      "Selected features (RF): [ 33  30 110  21 101  32  35 136  37 114 132   0  39  22 133]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285 0.015737\n",
      " 0.01503168 0.0140918  0.01361461 0.01344505 0.01343731 0.01327275\n",
      " 0.01313235 0.01247191 0.01237332]\n",
      "Fold 15, VarNum 15, Method RF, Model XGB, Selected Features: [ 33  30 110  21 101  32  35 136  37 114 132   0  39  22 133]\n",
      "Fold 15, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (RF): [ 33 105  30  21  32 136 107  22 114 102  25 132   6   5 110]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172 0.01886913\n",
      " 0.01532578 0.01504565 0.01427528 0.01374455 0.0132633  0.01294786\n",
      " 0.01240264 0.01225477 0.01214028]\n",
      "Fold 16, VarNum 15, Method RF, Model XGB, Selected Features: [ 33 105  30  21  32 136 107  22 114 102  25 132   6   5 110]\n",
      "Fold 16, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.75\n",
      "Selected features (RF): [122 113  30  21 134 105   6 109  16  32  25  33  22  89 101]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434 0.01428145\n",
      " 0.01421295 0.01348654 0.01343482 0.01305475 0.01281797 0.01266458\n",
      " 0.01259589 0.0125712  0.01216599]\n",
      "Fold 17, VarNum 15, Method RF, Model XGB, Selected Features: [122 113  30  21 134 105   6 109  16  32  25  33  22  89 101]\n",
      "Fold 17, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (RF): [122 113  91 114  33 134  32 110  92  46 100 106 118  30 115]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344 0.01700055\n",
      " 0.01696383 0.01685772 0.016267   0.01624445 0.01546131 0.01529183\n",
      " 0.01484006 0.01473522 0.01353428]\n",
      "Fold 18, VarNum 15, Method RF, Model XGB, Selected Features: [122 113  91 114  33 134  32 110  92  46 100 106 118  30 115]\n",
      "Fold 18, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [134  92 122  32  33 109  73 114 113 132 110  35 115 111 130]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174 0.01769571\n",
      " 0.01744237 0.01549418 0.01513342 0.01443422 0.01426394 0.01276479\n",
      " 0.01265933 0.01214103 0.01204304]\n",
      "Fold 19, VarNum 15, Method RF, Model XGB, Selected Features: [134  92 122  32  33 109  73 114 113 132 110  35 115 111 130]\n",
      "Fold 19, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (RF): [ 30  32 134  33  91 114 119 112  85 130  36  38  59  35  58]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132  0.01416553\n",
      " 0.01375293 0.01374425 0.01317677 0.0129457  0.01195973 0.01192029\n",
      " 0.01190177 0.01163069 0.01161039]\n",
      "Fold 20, VarNum 15, Method RF, Model XGB, Selected Features: [ 30  32 134  33  91 114 119 112  85 130  36  38  59  35  58]\n",
      "Fold 20, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 136 100  73  98  32  31  21  33   6 123 134 105  72 110]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984 0.01694408\n",
      " 0.01582854 0.01479282 0.01409128 0.01360734 0.01359641 0.01359467\n",
      " 0.01315045 0.0128937  0.01257166]\n",
      "Fold 21, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 136 100  73  98  32  31  21  33   6 123 134 105  72 110]\n",
      "Fold 21, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252 0.01557481 0.01491991\n",
      " 0.01489731 0.01448407 0.01424924]\n",
      "Fold 0, VarNum 15, Method RF, Model LDA, Selected Features: [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Fold 0, VarNum 15, Method RF, Model LDA, Train Acc: 0.728, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [134 107 106 100 129  30 131 123 102 127  94 118 122 124  87]\n",
      "Feature importances: [0.03356268 0.03101136 0.02707835 0.02215516 0.02142054 0.02062351\n",
      " 0.02036939 0.02001713 0.01831205 0.01752115 0.01505652 0.01438327\n",
      " 0.01378803 0.01268187 0.01239187]\n",
      "Fold 1, VarNum 15, Method RF, Model LDA, Selected Features: [134 107 106 100 129  30 131 123 102 127  94 118 122 124  87]\n",
      "Fold 1, VarNum 15, Method RF, Model LDA, Train Acc: 0.7258064516129032, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [107  30 105  33 129 106 100 127 102  98 134  32  93 109 101]\n",
      "Feature importances: [0.03421967 0.02853743 0.02400341 0.0223075  0.01970624 0.01825179\n",
      " 0.01749679 0.01692175 0.01671952 0.0158184  0.0155876  0.01517898\n",
      " 0.01467856 0.01402575 0.01341131]\n",
      "Fold 2, VarNum 15, Method RF, Model LDA, Selected Features: [107  30 105  33 129 106 100 127 102  98 134  32  93 109 101]\n",
      "Fold 2, VarNum 15, Method RF, Model LDA, Train Acc: 0.734375, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [107 106 122 127  30  33 104 120 105 100  98   6 129 109 102]\n",
      "Feature importances: [0.0358706  0.03419063 0.02516566 0.0240222  0.02334713 0.02271285\n",
      " 0.02224426 0.01820072 0.01750092 0.01735569 0.01731945 0.01703932\n",
      " 0.01619163 0.0157258  0.01570642]\n",
      "Fold 3, VarNum 15, Method RF, Model LDA, Selected Features: [107 106 122 127  30  33 104 120 105 100  98   6 129 109 102]\n",
      "Fold 3, VarNum 15, Method RF, Model LDA, Train Acc: 0.776, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [106 107  32  30  33 129 109 120 127 100  39  83  98  97  37]\n",
      "Feature importances: [0.04446563 0.03816146 0.029448   0.02832367 0.028174   0.02665422\n",
      " 0.02360296 0.01937688 0.01756391 0.01635806 0.0153292  0.01398757\n",
      " 0.0135894  0.01347108 0.01311887]\n",
      "Fold 4, VarNum 15, Method RF, Model LDA, Selected Features: [106 107  32  30  33 129 109 120 127 100  39  83  98  97  37]\n",
      "Fold 4, VarNum 15, Method RF, Model LDA, Train Acc: 0.7890625, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [107 127 106 109  30 129 136 134  32  33 122  92 105 123 121]\n",
      "Feature importances: [0.04665078 0.04386423 0.0422409  0.0317109  0.02308567 0.01914487\n",
      " 0.01570817 0.0154738  0.01532336 0.01522347 0.01391631 0.01380123\n",
      " 0.01365285 0.01327018 0.01291883]\n",
      "Fold 5, VarNum 15, Method RF, Model LDA, Selected Features: [107 127 106 109  30 129 136 134  32  33 122  92 105 123 121]\n",
      "Fold 5, VarNum 15, Method RF, Model LDA, Train Acc: 0.8188976377952756, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [106 107 127 109  32 118 122 134 131  89 132 123  90 133  33]\n",
      "Feature importances: [0.04110622 0.03826086 0.03302889 0.02757463 0.02565486 0.02450973\n",
      " 0.02411738 0.02402983 0.01708098 0.0164193  0.0162579  0.01446688\n",
      " 0.0124501  0.01211474 0.01210385]\n",
      "Fold 6, VarNum 15, Method RF, Model LDA, Selected Features: [106 107 127 109  32 118 122 134 131  89 132 123  90 133  33]\n",
      "Fold 6, VarNum 15, Method RF, Model LDA, Train Acc: 0.8253968253968254, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [107 106 127  30 118 134  90 123 113 109 132 122  89  60  21]\n",
      "Feature importances: [0.06009301 0.04714606 0.04646858 0.0322664  0.02702774 0.02518253\n",
      " 0.02050878 0.0164142  0.01534288 0.01532239 0.01517528 0.01427025\n",
      " 0.01362907 0.01320932 0.01236264]\n",
      "Fold 7, VarNum 15, Method RF, Model LDA, Selected Features: [107 106 127  30 118 134  90 123 113 109 132 122  89  60  21]\n",
      "Fold 7, VarNum 15, Method RF, Model LDA, Train Acc: 0.7874015748031497, Test Acc: 0.8947368421052632\n",
      "Selected features (RF): [107 106  30 109  33 134 122 123  57  35 118 132  31 127  71]\n",
      "Feature importances: [0.05000378 0.04506237 0.03123143 0.02906954 0.02500996 0.02325124\n",
      " 0.02306146 0.02232644 0.02068571 0.02016427 0.01968458 0.01627144\n",
      " 0.01481066 0.0136572  0.01333423]\n",
      "Fold 8, VarNum 15, Method RF, Model LDA, Selected Features: [107 106  30 109  33 134 122 123  57  35 118 132  31 127  71]\n",
      "Fold 8, VarNum 15, Method RF, Model LDA, Train Acc: 0.8048780487804879, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107 132  35 106 123 118 134 127  33  32  12   1  57 133]\n",
      "Feature importances: [0.04047922 0.03216058 0.02106391 0.02104438 0.02073646 0.02035143\n",
      " 0.01837401 0.01697944 0.01689733 0.01622381 0.01464765 0.01423333\n",
      " 0.01394567 0.01301881 0.01254948]\n",
      "Fold 9, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 132  35 106 123 118 134 127  33  32  12   1  57 133]\n",
      "Fold 9, VarNum 15, Method RF, Model LDA, Train Acc: 0.76, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30  32  33  35  57   6 113 118  25  54  21 106  29  70  73]\n",
      "Feature importances: [0.04275383 0.03797433 0.02967797 0.02526055 0.0227143  0.02133614\n",
      " 0.01841816 0.01703708 0.01599764 0.01544085 0.01512738 0.01430247\n",
      " 0.01390334 0.01380115 0.01363089]\n",
      "Fold 10, VarNum 15, Method RF, Model LDA, Selected Features: [ 30  32  33  35  57   6 113 118  25  54  21 106  29  70  73]\n",
      "Fold 10, VarNum 15, Method RF, Model LDA, Train Acc: 0.7560975609756098, Test Acc: 0.7727272727272727\n",
      "Selected features (RF): [ 30  33  32  35  21  37  39  22  57  25  36 118  11 106  73]\n",
      "Feature importances: [0.0514621  0.03750305 0.02587261 0.02494307 0.02461159 0.01897151\n",
      " 0.01866083 0.01702243 0.01647859 0.01574145 0.01460072 0.01368665\n",
      " 0.01362177 0.01319057 0.01234622]\n",
      "Fold 11, VarNum 15, Method RF, Model LDA, Selected Features: [ 30  33  32  35  21  37  39  22  57  25  36 118  11 106  73]\n",
      "Fold 11, VarNum 15, Method RF, Model LDA, Train Acc: 0.7661290322580645, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [ 33  35  30  21  32  22   3  39  25  37   6 107  57 106  36]\n",
      "Feature importances: [0.04524554 0.04512591 0.04181096 0.04093323 0.02589812 0.01944994\n",
      " 0.01731347 0.0168168  0.01443683 0.01385973 0.01244558 0.01190744\n",
      " 0.01164713 0.01156317 0.0113951 ]\n",
      "Fold 12, VarNum 15, Method RF, Model LDA, Selected Features: [ 33  35  30  21  32  22   3  39  25  37   6 107  57 106  36]\n",
      "Fold 12, VarNum 15, Method RF, Model LDA, Train Acc: 0.782258064516129, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 33  30  32  21  35  37 136 101 100 115  22  58  39  36  95]\n",
      "Feature importances: [0.04164435 0.03893279 0.033448   0.02842507 0.025099   0.01780627\n",
      " 0.0133239  0.01289129 0.0127552  0.01254642 0.01166191 0.01131257\n",
      " 0.01130641 0.01103137 0.01098019]\n",
      "Fold 13, VarNum 15, Method RF, Model LDA, Selected Features: [ 33  30  32  21  35  37 136 101 100 115  22  58  39  36  95]\n",
      "Fold 13, VarNum 15, Method RF, Model LDA, Train Acc: 0.7580645161290323, Test Acc: 0.4090909090909091\n",
      "Selected features (RF): [ 33  30  35  32 127  21 106  22 121 105  57  40 115 101 129]\n",
      "Feature importances: [0.03006635 0.02728075 0.02309845 0.02027255 0.01580125 0.01578645\n",
      " 0.01528147 0.01479727 0.01284964 0.01243961 0.01229255 0.01227687\n",
      " 0.01201724 0.01201053 0.01200573]\n",
      "Fold 14, VarNum 15, Method RF, Model LDA, Selected Features: [ 33  30  35  32 127  21 106  22 121 105  57  40 115 101 129]\n",
      "Fold 14, VarNum 15, Method RF, Model LDA, Train Acc: 0.6929133858267716, Test Acc: 0.55\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [ 33  30 110  21 101  32  35 136  37 114 132   0  39  22 133]\n",
      "Feature importances: [0.02378753 0.02271546 0.02099906 0.01733511 0.01705285 0.015737\n",
      " 0.01503168 0.0140918  0.01361461 0.01344505 0.01343731 0.01327275\n",
      " 0.01313235 0.01247191 0.01237332]\n",
      "Fold 15, VarNum 15, Method RF, Model LDA, Selected Features: [ 33  30 110  21 101  32  35 136  37 114 132   0  39  22 133]\n",
      "Fold 15, VarNum 15, Method RF, Model LDA, Train Acc: 0.6984126984126984, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [ 33 105  30  21  32 136 107  22 114 102  25 132   6   5 110]\n",
      "Feature importances: [0.0257224  0.02383966 0.0227334  0.02144866 0.01935172 0.01886913\n",
      " 0.01532578 0.01504565 0.01427528 0.01374455 0.0132633  0.01294786\n",
      " 0.01240264 0.01225477 0.01214028]\n",
      "Fold 16, VarNum 15, Method RF, Model LDA, Selected Features: [ 33 105  30  21  32 136 107  22 114 102  25 132   6   5 110]\n",
      "Fold 16, VarNum 15, Method RF, Model LDA, Train Acc: 0.71875, Test Acc: 0.35\n",
      "Selected features (RF): [122 113  30  21 134 105   6 109  16  32  25  33  22  89 101]\n",
      "Feature importances: [0.02283732 0.0200557  0.01858813 0.01492871 0.01486434 0.01428145\n",
      " 0.01421295 0.01348654 0.01343482 0.01305475 0.01281797 0.01266458\n",
      " 0.01259589 0.0125712  0.01216599]\n",
      "Fold 17, VarNum 15, Method RF, Model LDA, Selected Features: [122 113  30  21 134 105   6 109  16  32  25  33  22  89 101]\n",
      "Fold 17, VarNum 15, Method RF, Model LDA, Train Acc: 0.7142857142857143, Test Acc: 0.65\n",
      "Selected features (RF): [122 113  91 114  33 134  32 110  92  46 100 106 118  30 115]\n",
      "Feature importances: [0.02343943 0.02078319 0.01944439 0.0188368  0.01700344 0.01700055\n",
      " 0.01696383 0.01685772 0.016267   0.01624445 0.01546131 0.01529183\n",
      " 0.01484006 0.01473522 0.01353428]\n",
      "Fold 18, VarNum 15, Method RF, Model LDA, Selected Features: [122 113  91 114  33 134  32 110  92  46 100 106 118  30 115]\n",
      "Fold 18, VarNum 15, Method RF, Model LDA, Train Acc: 0.7244094488188977, Test Acc: 0.5\n",
      "Selected features (RF): [134  92 122  32  33 109  73 114 113 132 110  35 115 111 130]\n",
      "Feature importances: [0.02810113 0.02651087 0.02150162 0.0186888  0.01837174 0.01769571\n",
      " 0.01744237 0.01549418 0.01513342 0.01443422 0.01426394 0.01276479\n",
      " 0.01265933 0.01214103 0.01204304]\n",
      "Fold 19, VarNum 15, Method RF, Model LDA, Selected Features: [134  92 122  32  33 109  73 114 113 132 110  35 115 111 130]\n",
      "Fold 19, VarNum 15, Method RF, Model LDA, Train Acc: 0.7322834645669292, Test Acc: 0.47368421052631576\n",
      "Selected features (RF): [ 30  32 134  33  91 114 119 112  85 130  36  38  59  35  58]\n",
      "Feature importances: [0.02547259 0.0254014  0.01675952 0.01606476 0.0147132  0.01416553\n",
      " 0.01375293 0.01374425 0.01317677 0.0129457  0.01195973 0.01192029\n",
      " 0.01190177 0.01163069 0.01161039]\n",
      "Fold 20, VarNum 15, Method RF, Model LDA, Selected Features: [ 30  32 134  33  91 114 119 112  85 130  36  38  59  35  58]\n",
      "Fold 20, VarNum 15, Method RF, Model LDA, Train Acc: 0.7258064516129032, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 136 100  73  98  32  31  21  33   6 123 134 105  72 110]\n",
      "Feature importances: [0.03421025 0.02059931 0.01828678 0.01744054 0.01724984 0.01694408\n",
      " 0.01582854 0.01479282 0.01409128 0.01360734 0.01359641 0.01359467\n",
      " 0.01315045 0.0128937  0.01257166]\n",
      "Fold 21, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 136 100  73  98  32  31  21  33   6 123 134 105  72 110]\n",
      "Fold 21, VarNum 15, Method RF, Model LDA, Train Acc: 0.8, Test Acc: 0.5\n",
      "[15:23:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594 0.02863571 0.02685573\n",
      " 0.02517813 0.02097734 0.01903709]\n",
      "Fold 0, VarNum 15, Method XGB, Model CART, Selected Features: [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Fold 0, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:23:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1  52  49  43  97 134  65 106 101  95  91 131 123 111  54]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752 0.03626129\n",
      " 0.02896795 0.02821424 0.02584204 0.02422096 0.0226251  0.01989891\n",
      " 0.01893421 0.01867546 0.0186568 ]\n",
      "Fold 1, VarNum 15, Method XGB, Model CART, Selected Features: [  1  52  49  43  97 134  65 106 101  95  91 131 123 111  54]\n",
      "Fold 1, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "[15:23:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35  32  71 106  97 109  85  89  70 102  29]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024 0.03934522\n",
      " 0.03732697 0.03702934 0.03554991 0.02754998 0.02700797 0.02020467\n",
      " 0.01919138 0.0179514  0.0177955 ]\n",
      "Fold 2, VarNum 15, Method XGB, Model CART, Selected Features: [ 54  93  92 127  35  32  71 106  97 109  85  89  70 102  29]\n",
      "Fold 2, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.3157894736842105\n",
      "[15:23:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71  97  29 131 105  37  30 106 109  78  32]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887 0.03330989\n",
      " 0.03321506 0.03237444 0.03195391 0.0294292  0.02905012 0.02441263\n",
      " 0.02328251 0.02283769 0.02273776]\n",
      "Fold 3, VarNum 15, Method XGB, Model CART, Selected Features: [127  19 112 111  71  97  29 131 105  37  30 106 109  78  32]\n",
      "Fold 3, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "[15:23:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81  92 105  30 104  95  96 107 135  83 129]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301 0.03296668\n",
      " 0.03100923 0.02906076 0.02756156 0.02643843 0.02640826 0.02447952\n",
      " 0.0238907  0.02016696 0.01899442]\n",
      "Fold 4, VarNum 15, Method XGB, Model CART, Selected Features: [ 32  61 106  29  81  92 105  30 104  95  96 107 135  83 129]\n",
      "Fold 4, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:23:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 37  63  23  72   6 107  68 106 131   0  32  25  98 108 101]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091 0.03251973\n",
      " 0.02970542 0.02945096 0.02717068 0.02702343 0.02423287 0.0239141\n",
      " 0.02381011 0.01915811 0.01777912]\n",
      "Fold 5, VarNum 15, Method XGB, Model CART, Selected Features: [ 37  63  23  72   6 107  68 106 131   0  32  25  98 108 101]\n",
      "Fold 5, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.3157894736842105\n",
      "[15:23:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63  57 127   5  17  32  74  79 129  83  73]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347 0.04153319\n",
      " 0.04102546 0.03950502 0.03855329 0.03679097 0.02686568 0.02664023\n",
      " 0.02378465 0.01864977 0.01825309]\n",
      "Fold 6, VarNum 15, Method XGB, Model CART, Selected Features: [107 105 122  52  63  57 127   5  17  32  74  79 129  83  73]\n",
      "Fold 6, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [  1 107  80 115  37 127  35  51  63  73 126  62  20 133  48]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871 0.03215658\n",
      " 0.03175694 0.0314934  0.02671719 0.0256332  0.02488761 0.02397624\n",
      " 0.02261926 0.02068106 0.01882865]\n",
      "Fold 7, VarNum 15, Method XGB, Model CART, Selected Features: [  1 107  80 115  37 127  35  51  63  73 126  62  20 133  48]\n",
      "Fold 7, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107  33  17  30  75  54  28  83  11  68  97]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188 0.04792891\n",
      " 0.03837226 0.03728842 0.0318395  0.03169723 0.03014294 0.02831862\n",
      " 0.02652607 0.02524678 0.02493394]\n",
      "Fold 8, VarNum 15, Method XGB, Model CART, Selected Features: [108  41 110  35 107  33  17  30  75  54  28  83  11  68  97]\n",
      "Fold 8, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35  30  17 121  15 109  54   1  97  12 127]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769 0.04237923\n",
      " 0.0411956  0.03040076 0.0301668  0.02735102 0.02536665 0.02526023\n",
      " 0.02399222 0.02341142 0.02101137]\n",
      "Fold 9, VarNum 15, Method XGB, Model CART, Selected Features: [126 112  27  70  35  30  17 121  15 109  54   1  97  12 127]\n",
      "Fold 9, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69  56  42 111  65  27  91  98   6 128  44]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013 0.04062516\n",
      " 0.04024033 0.03461215 0.03025046 0.02787456 0.02475373 0.02299904\n",
      " 0.02182731 0.0201506  0.02010849]\n",
      "Fold 10, VarNum 15, Method XGB, Model CART, Selected Features: [ 35  50 132  33  69  56  42 111  65  27  91  98   6 128  44]\n",
      "Fold 10, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28 128  30  93  87  64  35  21 117 133 107]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683 0.03494062\n",
      " 0.03379305 0.03272084 0.03186584 0.03093796 0.02939206 0.02934098\n",
      " 0.02555138 0.0222534  0.02216419]\n",
      "Fold 11, VarNum 15, Method XGB, Model CART, Selected Features: [ 33  92  24 131  28 128  30  93  87  64  35  21 117 133 107]\n",
      "Fold 11, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.3157894736842105\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  25  17   8  21  98  35  74  32   9  80   0 104 125  70]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572 0.03954697\n",
      " 0.0381116  0.03652066 0.03586444 0.03134957 0.02263123 0.02206391\n",
      " 0.0216522  0.02148568 0.01925022]\n",
      "Fold 12, VarNum 15, Method XGB, Model CART, Selected Features: [ 33  25  17   8  21  98  35  74  32   9  80   0 104 125  70]\n",
      "Fold 12, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65 124  32 100  95 128  25  29  20 122 106]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192 0.0375411\n",
      " 0.0296344  0.028289   0.02809397 0.02647186 0.02582458 0.02543353\n",
      " 0.02503267 0.02181566 0.02006459]\n",
      "Fold 13, VarNum 15, Method XGB, Model CART, Selected Features: [125  33  47  26  65 124  32 100  95 128  25  29  20 122 106]\n",
      "Fold 13, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.36363636363636365\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51   5  67  70 132 122  32  79 107  75 102]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805 0.03213321\n",
      " 0.03205619 0.02940755 0.02676875 0.02317305 0.0227934  0.02200307\n",
      " 0.02149891 0.01855158 0.01802956]\n",
      "Fold 14, VarNum 15, Method XGB, Model CART, Selected Features: [ 33  63  42 108  51   5  67  70 132 122  32  79 107  75 102]\n",
      "Fold 14, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:23:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 89  30  73  69  22  29   2  28   5  47   0 133  98 107 106]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143   0.03310274\n",
      " 0.03033257 0.02782268 0.0271608  0.02617155 0.02578854 0.02359498\n",
      " 0.0233918  0.02226278 0.02205537]\n",
      "Fold 15, VarNum 15, Method XGB, Model CART, Selected Features: [ 89  30  73  69  22  29   2  28   5  47   0 133  98 107 106]\n",
      "Fold 15, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "[15:23:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7  35   6 107 120   8 118  52  15  29 131]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619 0.03208331\n",
      " 0.03144437 0.02985148 0.0276808  0.02635875 0.02586114 0.02488816\n",
      " 0.02475854 0.02421924 0.02399355]\n",
      "Fold 16, VarNum 15, Method XGB, Model CART, Selected Features: [115  88  32 109   7  35   6 107 120   8 118  52  15  29 131]\n",
      "Fold 16, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:23:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74  15  11  21  90  65 105  47  61 102  66]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458 0.02770768\n",
      " 0.02723359 0.02665008 0.02389776 0.02333322 0.02038334 0.01794489\n",
      " 0.01787452 0.01743047 0.01716154]\n",
      "Fold 17, VarNum 15, Method XGB, Model CART, Selected Features: [112   2  36 113  74  15  11  21  90  65 105  47  61 102  66]\n",
      "Fold 17, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:23:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32 118  48 113  73 131  15  33 121 125  60]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386 0.03553027\n",
      " 0.03012871 0.02946849 0.02732415 0.02354965 0.02305194 0.02255423\n",
      " 0.02130838 0.02006356 0.01973283]\n",
      "Fold 18, VarNum 15, Method XGB, Model CART, Selected Features: [112  31 110 130  32 118  48 113  73 131  15  33 121 125  60]\n",
      "Fold 18, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:23:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 16  12  89  80 131 115 106  32  30  42 110 128 100  21 116]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978 0.02677654\n",
      " 0.02664938 0.02544507 0.02473148 0.02245516 0.02243918 0.02156388\n",
      " 0.0213986  0.02118192 0.02031996]\n",
      "Fold 19, VarNum 15, Method XGB, Model CART, Selected Features: [ 16  12  89  80 131 115 106  32  30  42 110 128 100  21 116]\n",
      "Fold 19, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "[15:23:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135 105  44  30  91  43  90  12 112  87  32]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441 0.0345871\n",
      " 0.03290356 0.02876632 0.02770249 0.02665301 0.02484939 0.02302239\n",
      " 0.02285191 0.02265313 0.0211102 ]\n",
      "Fold 20, VarNum 15, Method XGB, Model CART, Selected Features: [119  48   8  25 135 105  44  30  91  43  90  12 112  87  32]\n",
      "Fold 20, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:23:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109  44 100  32 124  25 135  29 114  43 120]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843 0.03087765\n",
      " 0.0301982  0.02749008 0.02651894 0.02543698 0.0253746  0.02534382\n",
      " 0.02412271 0.02409962 0.02405064]\n",
      "Fold 21, VarNum 15, Method XGB, Model CART, Selected Features: [ 30  15 129 105 109  44 100  32 124  25 135  29 114  43 120]\n",
      "Fold 21, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:23:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594 0.02863571 0.02685573\n",
      " 0.02517813 0.02097734 0.01903709]\n",
      "Fold 0, VarNum 15, Method XGB, Model RF, Selected Features: [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Fold 0, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:23:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1  52  49  43  97 134  65 106 101  95  91 131 123 111  54]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752 0.03626129\n",
      " 0.02896795 0.02821424 0.02584204 0.02422096 0.0226251  0.01989891\n",
      " 0.01893421 0.01867546 0.0186568 ]\n",
      "Fold 1, VarNum 15, Method XGB, Model RF, Selected Features: [  1  52  49  43  97 134  65 106 101  95  91 131 123 111  54]\n",
      "Fold 1, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:23:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35  32  71 106  97 109  85  89  70 102  29]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024 0.03934522\n",
      " 0.03732697 0.03702934 0.03554991 0.02754998 0.02700797 0.02020467\n",
      " 0.01919138 0.0179514  0.0177955 ]\n",
      "Fold 2, VarNum 15, Method XGB, Model RF, Selected Features: [ 54  93  92 127  35  32  71 106  97 109  85  89  70 102  29]\n",
      "Fold 2, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "[15:23:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71  97  29 131 105  37  30 106 109  78  32]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887 0.03330989\n",
      " 0.03321506 0.03237444 0.03195391 0.0294292  0.02905012 0.02441263\n",
      " 0.02328251 0.02283769 0.02273776]\n",
      "Fold 3, VarNum 15, Method XGB, Model RF, Selected Features: [127  19 112 111  71  97  29 131 105  37  30 106 109  78  32]\n",
      "Fold 3, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "[15:23:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81  92 105  30 104  95  96 107 135  83 129]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301 0.03296668\n",
      " 0.03100923 0.02906076 0.02756156 0.02643843 0.02640826 0.02447952\n",
      " 0.0238907  0.02016696 0.01899442]\n",
      "Fold 4, VarNum 15, Method XGB, Model RF, Selected Features: [ 32  61 106  29  81  92 105  30 104  95  96 107 135  83 129]\n",
      "Fold 4, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:23:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 37  63  23  72   6 107  68 106 131   0  32  25  98 108 101]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091 0.03251973\n",
      " 0.02970542 0.02945096 0.02717068 0.02702343 0.02423287 0.0239141\n",
      " 0.02381011 0.01915811 0.01777912]\n",
      "Fold 5, VarNum 15, Method XGB, Model RF, Selected Features: [ 37  63  23  72   6 107  68 106 131   0  32  25  98 108 101]\n",
      "Fold 5, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:23:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63  57 127   5  17  32  74  79 129  83  73]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347 0.04153319\n",
      " 0.04102546 0.03950502 0.03855329 0.03679097 0.02686568 0.02664023\n",
      " 0.02378465 0.01864977 0.01825309]\n",
      "Fold 6, VarNum 15, Method XGB, Model RF, Selected Features: [107 105 122  52  63  57 127   5  17  32  74  79 129  83  73]\n",
      "Fold 6, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "[15:23:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37 127  35  51  63  73 126  62  20 133  48]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871 0.03215658\n",
      " 0.03175694 0.0314934  0.02671719 0.0256332  0.02488761 0.02397624\n",
      " 0.02261926 0.02068106 0.01882865]\n",
      "Fold 7, VarNum 15, Method XGB, Model RF, Selected Features: [  1 107  80 115  37 127  35  51  63  73 126  62  20 133  48]\n",
      "Fold 7, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:23:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107  33  17  30  75  54  28  83  11  68  97]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188 0.04792891\n",
      " 0.03837226 0.03728842 0.0318395  0.03169723 0.03014294 0.02831862\n",
      " 0.02652607 0.02524678 0.02493394]\n",
      "Fold 8, VarNum 15, Method XGB, Model RF, Selected Features: [108  41 110  35 107  33  17  30  75  54  28  83  11  68  97]\n",
      "Fold 8, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:23:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35  30  17 121  15 109  54   1  97  12 127]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769 0.04237923\n",
      " 0.0411956  0.03040076 0.0301668  0.02735102 0.02536665 0.02526023\n",
      " 0.02399222 0.02341142 0.02101137]\n",
      "Fold 9, VarNum 15, Method XGB, Model RF, Selected Features: [126 112  27  70  35  30  17 121  15 109  54   1  97  12 127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 9, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.47619047619047616\n",
      "[15:23:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69  56  42 111  65  27  91  98   6 128  44]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013 0.04062516\n",
      " 0.04024033 0.03461215 0.03025046 0.02787456 0.02475373 0.02299904\n",
      " 0.02182731 0.0201506  0.02010849]\n",
      "Fold 10, VarNum 15, Method XGB, Model RF, Selected Features: [ 35  50 132  33  69  56  42 111  65  27  91  98   6 128  44]\n",
      "Fold 10, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:23:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28 128  30  93  87  64  35  21 117 133 107]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683 0.03494062\n",
      " 0.03379305 0.03272084 0.03186584 0.03093796 0.02939206 0.02934098\n",
      " 0.02555138 0.0222534  0.02216419]\n",
      "Fold 11, VarNum 15, Method XGB, Model RF, Selected Features: [ 33  92  24 131  28 128  30  93  87  64  35  21 117 133 107]\n",
      "Fold 11, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:23:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  25  17   8  21  98  35  74  32   9  80   0 104 125  70]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572 0.03954697\n",
      " 0.0381116  0.03652066 0.03586444 0.03134957 0.02263123 0.02206391\n",
      " 0.0216522  0.02148568 0.01925022]\n",
      "Fold 12, VarNum 15, Method XGB, Model RF, Selected Features: [ 33  25  17   8  21  98  35  74  32   9  80   0 104 125  70]\n",
      "Fold 12, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:23:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65 124  32 100  95 128  25  29  20 122 106]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192 0.0375411\n",
      " 0.0296344  0.028289   0.02809397 0.02647186 0.02582458 0.02543353\n",
      " 0.02503267 0.02181566 0.02006459]\n",
      "Fold 13, VarNum 15, Method XGB, Model RF, Selected Features: [125  33  47  26  65 124  32 100  95 128  25  29  20 122 106]\n",
      "Fold 13, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:23:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51   5  67  70 132 122  32  79 107  75 102]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805 0.03213321\n",
      " 0.03205619 0.02940755 0.02676875 0.02317305 0.0227934  0.02200307\n",
      " 0.02149891 0.01855158 0.01802956]\n",
      "Fold 14, VarNum 15, Method XGB, Model RF, Selected Features: [ 33  63  42 108  51   5  67  70 132 122  32  79 107  75 102]\n",
      "Fold 14, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:23:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 89  30  73  69  22  29   2  28   5  47   0 133  98 107 106]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143   0.03310274\n",
      " 0.03033257 0.02782268 0.0271608  0.02617155 0.02578854 0.02359498\n",
      " 0.0233918  0.02226278 0.02205537]\n",
      "Fold 15, VarNum 15, Method XGB, Model RF, Selected Features: [ 89  30  73  69  22  29   2  28   5  47   0 133  98 107 106]\n",
      "Fold 15, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "[15:23:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7  35   6 107 120   8 118  52  15  29 131]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619 0.03208331\n",
      " 0.03144437 0.02985148 0.0276808  0.02635875 0.02586114 0.02488816\n",
      " 0.02475854 0.02421924 0.02399355]\n",
      "Fold 16, VarNum 15, Method XGB, Model RF, Selected Features: [115  88  32 109   7  35   6 107 120   8 118  52  15  29 131]\n",
      "Fold 16, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:23:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74  15  11  21  90  65 105  47  61 102  66]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458 0.02770768\n",
      " 0.02723359 0.02665008 0.02389776 0.02333322 0.02038334 0.01794489\n",
      " 0.01787452 0.01743047 0.01716154]\n",
      "Fold 17, VarNum 15, Method XGB, Model RF, Selected Features: [112   2  36 113  74  15  11  21  90  65 105  47  61 102  66]\n",
      "Fold 17, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:23:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32 118  48 113  73 131  15  33 121 125  60]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386 0.03553027\n",
      " 0.03012871 0.02946849 0.02732415 0.02354965 0.02305194 0.02255423\n",
      " 0.02130838 0.02006356 0.01973283]\n",
      "Fold 18, VarNum 15, Method XGB, Model RF, Selected Features: [112  31 110 130  32 118  48 113  73 131  15  33 121 125  60]\n",
      "Fold 18, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:23:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131 115 106  32  30  42 110 128 100  21 116]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978 0.02677654\n",
      " 0.02664938 0.02544507 0.02473148 0.02245516 0.02243918 0.02156388\n",
      " 0.0213986  0.02118192 0.02031996]\n",
      "Fold 19, VarNum 15, Method XGB, Model RF, Selected Features: [ 16  12  89  80 131 115 106  32  30  42 110 128 100  21 116]\n",
      "Fold 19, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:23:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135 105  44  30  91  43  90  12 112  87  32]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441 0.0345871\n",
      " 0.03290356 0.02876632 0.02770249 0.02665301 0.02484939 0.02302239\n",
      " 0.02285191 0.02265313 0.0211102 ]\n",
      "Fold 20, VarNum 15, Method XGB, Model RF, Selected Features: [119  48   8  25 135 105  44  30  91  43  90  12 112  87  32]\n",
      "Fold 20, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "[15:23:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109  44 100  32 124  25 135  29 114  43 120]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843 0.03087765\n",
      " 0.0301982  0.02749008 0.02651894 0.02543698 0.0253746  0.02534382\n",
      " 0.02412271 0.02409962 0.02405064]\n",
      "Fold 21, VarNum 15, Method XGB, Model RF, Selected Features: [ 30  15 129 105 109  44 100  32 124  25 135  29 114  43 120]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 21, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:23:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594 0.02863571 0.02685573\n",
      " 0.02517813 0.02097734 0.01903709]\n",
      "Fold 0, VarNum 15, Method XGB, Model XGB, Selected Features: [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Fold 0, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:23:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1  52  49  43  97 134  65 106 101  95  91 131 123 111  54]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752 0.03626129\n",
      " 0.02896795 0.02821424 0.02584204 0.02422096 0.0226251  0.01989891\n",
      " 0.01893421 0.01867546 0.0186568 ]\n",
      "Fold 1, VarNum 15, Method XGB, Model XGB, Selected Features: [  1  52  49  43  97 134  65 106 101  95  91 131 123 111  54]\n",
      "Fold 1, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "[15:23:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35  32  71 106  97 109  85  89  70 102  29]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024 0.03934522\n",
      " 0.03732697 0.03702934 0.03554991 0.02754998 0.02700797 0.02020467\n",
      " 0.01919138 0.0179514  0.0177955 ]\n",
      "Fold 2, VarNum 15, Method XGB, Model XGB, Selected Features: [ 54  93  92 127  35  32  71 106  97 109  85  89  70 102  29]\n",
      "Fold 2, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "[15:23:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71  97  29 131 105  37  30 106 109  78  32]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887 0.03330989\n",
      " 0.03321506 0.03237444 0.03195391 0.0294292  0.02905012 0.02441263\n",
      " 0.02328251 0.02283769 0.02273776]\n",
      "Fold 3, VarNum 15, Method XGB, Model XGB, Selected Features: [127  19 112 111  71  97  29 131 105  37  30 106 109  78  32]\n",
      "Fold 3, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "[15:23:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 32  61 106  29  81  92 105  30 104  95  96 107 135  83 129]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301 0.03296668\n",
      " 0.03100923 0.02906076 0.02756156 0.02643843 0.02640826 0.02447952\n",
      " 0.0238907  0.02016696 0.01899442]\n",
      "Fold 4, VarNum 15, Method XGB, Model XGB, Selected Features: [ 32  61 106  29  81  92 105  30 104  95  96 107 135  83 129]\n",
      "Fold 4, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:23:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 37  63  23  72   6 107  68 106 131   0  32  25  98 108 101]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091 0.03251973\n",
      " 0.02970542 0.02945096 0.02717068 0.02702343 0.02423287 0.0239141\n",
      " 0.02381011 0.01915811 0.01777912]\n",
      "Fold 5, VarNum 15, Method XGB, Model XGB, Selected Features: [ 37  63  23  72   6 107  68 106 131   0  32  25  98 108 101]\n",
      "Fold 5, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:23:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63  57 127   5  17  32  74  79 129  83  73]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347 0.04153319\n",
      " 0.04102546 0.03950502 0.03855329 0.03679097 0.02686568 0.02664023\n",
      " 0.02378465 0.01864977 0.01825309]\n",
      "Fold 6, VarNum 15, Method XGB, Model XGB, Selected Features: [107 105 122  52  63  57 127   5  17  32  74  79 129  83  73]\n",
      "Fold 6, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "[15:23:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37 127  35  51  63  73 126  62  20 133  48]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871 0.03215658\n",
      " 0.03175694 0.0314934  0.02671719 0.0256332  0.02488761 0.02397624\n",
      " 0.02261926 0.02068106 0.01882865]\n",
      "Fold 7, VarNum 15, Method XGB, Model XGB, Selected Features: [  1 107  80 115  37 127  35  51  63  73 126  62  20 133  48]\n",
      "Fold 7, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:23:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107  33  17  30  75  54  28  83  11  68  97]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188 0.04792891\n",
      " 0.03837226 0.03728842 0.0318395  0.03169723 0.03014294 0.02831862\n",
      " 0.02652607 0.02524678 0.02493394]\n",
      "Fold 8, VarNum 15, Method XGB, Model XGB, Selected Features: [108  41 110  35 107  33  17  30  75  54  28  83  11  68  97]\n",
      "Fold 8, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:23:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35  30  17 121  15 109  54   1  97  12 127]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769 0.04237923\n",
      " 0.0411956  0.03040076 0.0301668  0.02735102 0.02536665 0.02526023\n",
      " 0.02399222 0.02341142 0.02101137]\n",
      "Fold 9, VarNum 15, Method XGB, Model XGB, Selected Features: [126 112  27  70  35  30  17 121  15 109  54   1  97  12 127]\n",
      "Fold 9, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "[15:23:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69  56  42 111  65  27  91  98   6 128  44]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013 0.04062516\n",
      " 0.04024033 0.03461215 0.03025046 0.02787456 0.02475373 0.02299904\n",
      " 0.02182731 0.0201506  0.02010849]\n",
      "Fold 10, VarNum 15, Method XGB, Model XGB, Selected Features: [ 35  50 132  33  69  56  42 111  65  27  91  98   6 128  44]\n",
      "Fold 10, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:23:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28 128  30  93  87  64  35  21 117 133 107]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683 0.03494062\n",
      " 0.03379305 0.03272084 0.03186584 0.03093796 0.02939206 0.02934098\n",
      " 0.02555138 0.0222534  0.02216419]\n",
      "Fold 11, VarNum 15, Method XGB, Model XGB, Selected Features: [ 33  92  24 131  28 128  30  93  87  64  35  21 117 133 107]\n",
      "Fold 11, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:23:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  25  17   8  21  98  35  74  32   9  80   0 104 125  70]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572 0.03954697\n",
      " 0.0381116  0.03652066 0.03586444 0.03134957 0.02263123 0.02206391\n",
      " 0.0216522  0.02148568 0.01925022]\n",
      "Fold 12, VarNum 15, Method XGB, Model XGB, Selected Features: [ 33  25  17   8  21  98  35  74  32   9  80   0 104 125  70]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 12, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:23:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65 124  32 100  95 128  25  29  20 122 106]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192 0.0375411\n",
      " 0.0296344  0.028289   0.02809397 0.02647186 0.02582458 0.02543353\n",
      " 0.02503267 0.02181566 0.02006459]\n",
      "Fold 13, VarNum 15, Method XGB, Model XGB, Selected Features: [125  33  47  26  65 124  32 100  95 128  25  29  20 122 106]\n",
      "Fold 13, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:23:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51   5  67  70 132 122  32  79 107  75 102]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805 0.03213321\n",
      " 0.03205619 0.02940755 0.02676875 0.02317305 0.0227934  0.02200307\n",
      " 0.02149891 0.01855158 0.01802956]\n",
      "Fold 14, VarNum 15, Method XGB, Model XGB, Selected Features: [ 33  63  42 108  51   5  67  70 132 122  32  79 107  75 102]\n",
      "Fold 14, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:23:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 89  30  73  69  22  29   2  28   5  47   0 133  98 107 106]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143   0.03310274\n",
      " 0.03033257 0.02782268 0.0271608  0.02617155 0.02578854 0.02359498\n",
      " 0.0233918  0.02226278 0.02205537]\n",
      "Fold 15, VarNum 15, Method XGB, Model XGB, Selected Features: [ 89  30  73  69  22  29   2  28   5  47   0 133  98 107 106]\n",
      "Fold 15, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:23:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [115  88  32 109   7  35   6 107 120   8 118  52  15  29 131]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619 0.03208331\n",
      " 0.03144437 0.02985148 0.0276808  0.02635875 0.02586114 0.02488816\n",
      " 0.02475854 0.02421924 0.02399355]\n",
      "Fold 16, VarNum 15, Method XGB, Model XGB, Selected Features: [115  88  32 109   7  35   6 107 120   8 118  52  15  29 131]\n",
      "Fold 16, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "[15:23:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74  15  11  21  90  65 105  47  61 102  66]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458 0.02770768\n",
      " 0.02723359 0.02665008 0.02389776 0.02333322 0.02038334 0.01794489\n",
      " 0.01787452 0.01743047 0.01716154]\n",
      "Fold 17, VarNum 15, Method XGB, Model XGB, Selected Features: [112   2  36 113  74  15  11  21  90  65 105  47  61 102  66]\n",
      "Fold 17, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.55\n",
      "[15:23:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32 118  48 113  73 131  15  33 121 125  60]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386 0.03553027\n",
      " 0.03012871 0.02946849 0.02732415 0.02354965 0.02305194 0.02255423\n",
      " 0.02130838 0.02006356 0.01973283]\n",
      "Fold 18, VarNum 15, Method XGB, Model XGB, Selected Features: [112  31 110 130  32 118  48 113  73 131  15  33 121 125  60]\n",
      "Fold 18, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:23:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131 115 106  32  30  42 110 128 100  21 116]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978 0.02677654\n",
      " 0.02664938 0.02544507 0.02473148 0.02245516 0.02243918 0.02156388\n",
      " 0.0213986  0.02118192 0.02031996]\n",
      "Fold 19, VarNum 15, Method XGB, Model XGB, Selected Features: [ 16  12  89  80 131 115 106  32  30  42 110 128 100  21 116]\n",
      "Fold 19, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.3684210526315789\n",
      "[15:23:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135 105  44  30  91  43  90  12 112  87  32]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441 0.0345871\n",
      " 0.03290356 0.02876632 0.02770249 0.02665301 0.02484939 0.02302239\n",
      " 0.02285191 0.02265313 0.0211102 ]\n",
      "Fold 20, VarNum 15, Method XGB, Model XGB, Selected Features: [119  48   8  25 135 105  44  30  91  43  90  12 112  87  32]\n",
      "Fold 20, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "[15:23:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109  44 100  32 124  25 135  29 114  43 120]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843 0.03087765\n",
      " 0.0301982  0.02749008 0.02651894 0.02543698 0.0253746  0.02534382\n",
      " 0.02412271 0.02409962 0.02405064]\n",
      "Fold 21, VarNum 15, Method XGB, Model XGB, Selected Features: [ 30  15 129 105 109  44 100  32 124  25 135  29 114  43 120]\n",
      "Fold 21, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:23:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594 0.02863571 0.02685573\n",
      " 0.02517813 0.02097734 0.01903709]\n",
      "Fold 0, VarNum 15, Method XGB, Model LDA, Selected Features: [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Fold 0, VarNum 15, Method XGB, Model LDA, Train Acc: 0.68, Test Acc: 0.5714285714285714\n",
      "[15:23:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1  52  49  43  97 134  65 106 101  95  91 131 123 111  54]\n",
      "Feature importances: [0.09098215 0.04795565 0.04418239 0.0365594  0.03634752 0.03626129\n",
      " 0.02896795 0.02821424 0.02584204 0.02422096 0.0226251  0.01989891\n",
      " 0.01893421 0.01867546 0.0186568 ]\n",
      "Fold 1, VarNum 15, Method XGB, Model LDA, Selected Features: [  1  52  49  43  97 134  65 106 101  95  91 131 123 111  54]\n",
      "Fold 1, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6693548387096774, Test Acc: 0.43478260869565216\n",
      "[15:23:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  93  92 127  35  32  71 106  97 109  85  89  70 102  29]\n",
      "Feature importances: [0.06881668 0.04655104 0.04562842 0.0431707  0.03993024 0.03934522\n",
      " 0.03732697 0.03702934 0.03554991 0.02754998 0.02700797 0.02020467\n",
      " 0.01919138 0.0179514  0.0177955 ]\n",
      "Fold 2, VarNum 15, Method XGB, Model LDA, Selected Features: [ 54  93  92 127  35  32  71 106  97 109  85  89  70 102  29]\n",
      "Fold 2, VarNum 15, Method XGB, Model LDA, Train Acc: 0.78125, Test Acc: 0.631578947368421\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [127  19 112 111  71  97  29 131 105  37  30 106 109  78  32]\n",
      "Feature importances: [0.0677541  0.04787719 0.04161328 0.03636802 0.03365887 0.03330989\n",
      " 0.03321506 0.03237444 0.03195391 0.0294292  0.02905012 0.02441263\n",
      " 0.02328251 0.02283769 0.02273776]\n",
      "Fold 3, VarNum 15, Method XGB, Model LDA, Selected Features: [127  19 112 111  71  97  29 131 105  37  30 106 109  78  32]\n",
      "Fold 3, VarNum 15, Method XGB, Model LDA, Train Acc: 0.768, Test Acc: 0.7391304347826086\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 32  61 106  29  81  92 105  30 104  95  96 107 135  83 129]\n",
      "Feature importances: [0.06081986 0.06005809 0.05691498 0.05008206 0.04921301 0.03296668\n",
      " 0.03100923 0.02906076 0.02756156 0.02643843 0.02640826 0.02447952\n",
      " 0.0238907  0.02016696 0.01899442]\n",
      "Fold 4, VarNum 15, Method XGB, Model LDA, Selected Features: [ 32  61 106  29  81  92 105  30 104  95  96 107 135  83 129]\n",
      "Fold 4, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7421875, Test Acc: 0.6190476190476191\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 37  63  23  72   6 107  68 106 131   0  32  25  98 108 101]\n",
      "Feature importances: [0.12117104 0.08826667 0.0610269  0.05738462 0.03618091 0.03251973\n",
      " 0.02970542 0.02945096 0.02717068 0.02702343 0.02423287 0.0239141\n",
      " 0.02381011 0.01915811 0.01777912]\n",
      "Fold 5, VarNum 15, Method XGB, Model LDA, Selected Features: [ 37  63  23  72   6 107  68 106 131   0  32  25  98 108 101]\n",
      "Fold 5, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7874015748031497, Test Acc: 0.7368421052631579\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 105 122  52  63  57 127   5  17  32  74  79 129  83  73]\n",
      "Feature importances: [0.07236951 0.05657045 0.05448718 0.04686433 0.04511347 0.04153319\n",
      " 0.04102546 0.03950502 0.03855329 0.03679097 0.02686568 0.02664023\n",
      " 0.02378465 0.01864977 0.01825309]\n",
      "Fold 6, VarNum 15, Method XGB, Model LDA, Selected Features: [107 105 122  52  63  57 127   5  17  32  74  79 129  83  73]\n",
      "Fold 6, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7777777777777778, Test Acc: 0.6818181818181818\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 107  80 115  37 127  35  51  63  73 126  62  20 133  48]\n",
      "Feature importances: [0.07581432 0.06974842 0.05413421 0.05200139 0.04862871 0.03215658\n",
      " 0.03175694 0.0314934  0.02671719 0.0256332  0.02488761 0.02397624\n",
      " 0.02261926 0.02068106 0.01882865]\n",
      "Fold 7, VarNum 15, Method XGB, Model LDA, Selected Features: [  1 107  80 115  37 127  35  51  63  73 126  62  20 133  48]\n",
      "Fold 7, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7716535433070866, Test Acc: 0.631578947368421\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [108  41 110  35 107  33  17  30  75  54  28  83  11  68  97]\n",
      "Feature importances: [0.06090289 0.05803888 0.05315904 0.05156563 0.05000188 0.04792891\n",
      " 0.03837226 0.03728842 0.0318395  0.03169723 0.03014294 0.02831862\n",
      " 0.02652607 0.02524678 0.02493394]\n",
      "Fold 8, VarNum 15, Method XGB, Model LDA, Selected Features: [108  41 110  35 107  33  17  30  75  54  28  83  11  68  97]\n",
      "Fold 8, VarNum 15, Method XGB, Model LDA, Train Acc: 0.8130081300813008, Test Acc: 0.7142857142857143\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [126 112  27  70  35  30  17 121  15 109  54   1  97  12 127]\n",
      "Feature importances: [0.08476723 0.07523302 0.05240067 0.05085811 0.04241769 0.04237923\n",
      " 0.0411956  0.03040076 0.0301668  0.02735102 0.02536665 0.02526023\n",
      " 0.02399222 0.02341142 0.02101137]\n",
      "Fold 9, VarNum 15, Method XGB, Model LDA, Selected Features: [126 112  27  70  35  30  17 121  15 109  54   1  97  12 127]\n",
      "Fold 9, VarNum 15, Method XGB, Model LDA, Train Acc: 0.736, Test Acc: 0.47619047619047616\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 35  50 132  33  69  56  42 111  65  27  91  98   6 128  44]\n",
      "Feature importances: [0.08362651 0.04993495 0.04730373 0.04295497 0.04127013 0.04062516\n",
      " 0.04024033 0.03461215 0.03025046 0.02787456 0.02475373 0.02299904\n",
      " 0.02182731 0.0201506  0.02010849]\n",
      "Fold 10, VarNum 15, Method XGB, Model LDA, Selected Features: [ 35  50 132  33  69  56  42 111  65  27  91  98   6 128  44]\n",
      "Fold 10, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7073170731707317, Test Acc: 0.5909090909090909\n",
      "[15:23:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  92  24 131  28 128  30  93  87  64  35  21 117 133 107]\n",
      "Feature importances: [0.07099151 0.05919872 0.04016025 0.03611572 0.03536683 0.03494062\n",
      " 0.03379305 0.03272084 0.03186584 0.03093796 0.02939206 0.02934098\n",
      " 0.02555138 0.0222534  0.02216419]\n",
      "Fold 11, VarNum 15, Method XGB, Model LDA, Selected Features: [ 33  92  24 131  28 128  30  93  87  64  35  21 117 133 107]\n",
      "Fold 11, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7580645161290323, Test Acc: 0.5263157894736842\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  25  17   8  21  98  35  74  32   9  80   0 104 125  70]\n",
      "Feature importances: [0.11621111 0.09677556 0.06362217 0.05477572 0.04275572 0.03954697\n",
      " 0.0381116  0.03652066 0.03586444 0.03134957 0.02263123 0.02206391\n",
      " 0.0216522  0.02148568 0.01925022]\n",
      "Fold 12, VarNum 15, Method XGB, Model LDA, Selected Features: [ 33  25  17   8  21  98  35  74  32   9  80   0 104 125  70]\n",
      "Fold 12, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7741935483870968, Test Acc: 0.5454545454545454\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [125  33  47  26  65 124  32 100  95 128  25  29  20 122 106]\n",
      "Feature importances: [0.09067244 0.07497578 0.0657309  0.04896316 0.03977192 0.0375411\n",
      " 0.0296344  0.028289   0.02809397 0.02647186 0.02582458 0.02543353\n",
      " 0.02503267 0.02181566 0.02006459]\n",
      "Fold 13, VarNum 15, Method XGB, Model LDA, Selected Features: [125  33  47  26  65 124  32 100  95 128  25  29  20 122 106]\n",
      "Fold 13, VarNum 15, Method XGB, Model LDA, Train Acc: 0.75, Test Acc: 0.4090909090909091\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 33  63  42 108  51   5  67  70 132 122  32  79 107  75 102]\n",
      "Feature importances: [0.08733941 0.05797046 0.04142538 0.03783453 0.03698805 0.03213321\n",
      " 0.03205619 0.02940755 0.02676875 0.02317305 0.0227934  0.02200307\n",
      " 0.02149891 0.01855158 0.01802956]\n",
      "Fold 14, VarNum 15, Method XGB, Model LDA, Selected Features: [ 33  63  42 108  51   5  67  70 132 122  32  79 107  75 102]\n",
      "Fold 14, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7322834645669292, Test Acc: 0.6\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 89  30  73  69  22  29   2  28   5  47   0 133  98 107 106]\n",
      "Feature importances: [0.05774925 0.04600195 0.04250706 0.04129939 0.034143   0.03310274\n",
      " 0.03033257 0.02782268 0.0271608  0.02617155 0.02578854 0.02359498\n",
      " 0.0233918  0.02226278 0.02205537]\n",
      "Fold 15, VarNum 15, Method XGB, Model LDA, Selected Features: [ 89  30  73  69  22  29   2  28   5  47   0 133  98 107 106]\n",
      "Fold 15, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6666666666666666, Test Acc: 0.5652173913043478\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [115  88  32 109   7  35   6 107 120   8 118  52  15  29 131]\n",
      "Feature importances: [0.08838061 0.04788351 0.03626766 0.03557093 0.03512619 0.03208331\n",
      " 0.03144437 0.02985148 0.0276808  0.02635875 0.02586114 0.02488816\n",
      " 0.02475854 0.02421924 0.02399355]\n",
      "Fold 16, VarNum 15, Method XGB, Model LDA, Selected Features: [115  88  32 109   7  35   6 107 120   8 118  52  15  29 131]\n",
      "Fold 16, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7421875, Test Acc: 0.5\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112   2  36 113  74  15  11  21  90  65 105  47  61 102  66]\n",
      "Feature importances: [0.08225743 0.06962254 0.04561475 0.04289938 0.03476458 0.02770768\n",
      " 0.02723359 0.02665008 0.02389776 0.02333322 0.02038334 0.01794489\n",
      " 0.01787452 0.01743047 0.01716154]\n",
      "Fold 17, VarNum 15, Method XGB, Model LDA, Selected Features: [112   2  36 113  74  15  11  21  90  65 105  47  61 102  66]\n",
      "Fold 17, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7063492063492064, Test Acc: 0.9\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [112  31 110 130  32 118  48 113  73 131  15  33 121 125  60]\n",
      "Feature importances: [0.07218721 0.05009717 0.03989909 0.03818038 0.03626386 0.03553027\n",
      " 0.03012871 0.02946849 0.02732415 0.02354965 0.02305194 0.02255423\n",
      " 0.02130838 0.02006356 0.01973283]\n",
      "Fold 18, VarNum 15, Method XGB, Model LDA, Selected Features: [112  31 110 130  32 118  48 113  73 131  15  33 121 125  60]\n",
      "Fold 18, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7480314960629921, Test Acc: 0.45454545454545453\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 16  12  89  80 131 115 106  32  30  42 110 128 100  21 116]\n",
      "Feature importances: [0.0431761  0.04248863 0.03941054 0.03734741 0.03250978 0.02677654\n",
      " 0.02664938 0.02544507 0.02473148 0.02245516 0.02243918 0.02156388\n",
      " 0.0213986  0.02118192 0.02031996]\n",
      "Fold 19, VarNum 15, Method XGB, Model LDA, Selected Features: [ 16  12  89  80 131 115 106  32  30  42 110 128 100  21 116]\n",
      "Fold 19, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7007874015748031, Test Acc: 0.5263157894736842\n",
      "[15:23:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [119  48   8  25 135 105  44  30  91  43  90  12 112  87  32]\n",
      "Feature importances: [0.05592975 0.04594304 0.04335502 0.03693078 0.03619441 0.0345871\n",
      " 0.03290356 0.02876632 0.02770249 0.02665301 0.02484939 0.02302239\n",
      " 0.02285191 0.02265313 0.0211102 ]\n",
      "Fold 20, VarNum 15, Method XGB, Model LDA, Selected Features: [119  48   8  25 135 105  44  30  91  43  90  12 112  87  32]\n",
      "Fold 20, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7258064516129032, Test Acc: 0.47619047619047616\n",
      "[15:23:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 30  15 129 105 109  44 100  32 124  25 135  29 114  43 120]\n",
      "Feature importances: [0.03986085 0.03936125 0.03737655 0.03500066 0.03359843 0.03087765\n",
      " 0.0301982  0.02749008 0.02651894 0.02543698 0.0253746  0.02534382\n",
      " 0.02412271 0.02409962 0.02405064]\n",
      "Fold 21, VarNum 15, Method XGB, Model LDA, Selected Features: [ 30  15 129 105 109  44 100  32 124  25 135  29 114  43 120]\n",
      "Fold 21, VarNum 15, Method XGB, Model LDA, Train Acc: 0.696, Test Acc: 0.6818181818181818\n"
     ]
    }
   ],
   "source": [
    "data = data_main.drop(columns=['Target_BASE'])\n",
    "\n",
    "# Defining feature and target\n",
    "X = data.drop(columns=['Target'])\n",
    "Y = data['Target']\n",
    "\n",
    "# Normalization of data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Feature selection functions\n",
    "\n",
    "def select_features_corr(X_train, Y_train, varnum):\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    corr_with_target = corr_matrix.iloc[:-1, -1]\n",
    "\n",
    "    sorted_corr = corr_with_target.abs().sort_values(ascending=False)\n",
    "\n",
    "    selected_features_name = sorted_corr.head(varnum).index\n",
    "\n",
    "    selected_features = [X.columns.get_loc(feature) for feature in selected_features_name]\n",
    "    print(f\"Selected features (CorrelationFS): {selected_features}\")\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "def select_features_chi_square(X_train, Y_train, varnum):\n",
    "    selector = SelectKBest(chi2, k=varnum).fit(X_train, Y_train)\n",
    "\n",
    "    selected_features = selector.get_support(indices=True)\n",
    "\n",
    "    print(f\"Selected features (ChiSquareFS): {selected_features}\")\n",
    "    return selected_features\n",
    "\n",
    "def select_features_rf(X_train, Y_train, varnumn_estimators=50):\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, Y_train)\n",
    "    model_selector = SelectFromModel(rf, max_features=varnum, prefit=True)\n",
    "    selected_features = model_selector.get_support(indices=True)\n",
    "    feature_importances = rf.feature_importances_\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "    # Select the most important 'varnum' number feature\n",
    "    selected_features = sorted_indices[:varnum]\n",
    "    print(f\"Selected features (RF): {selected_features}\")\n",
    "    print(f\"Feature importances: {feature_importances[selected_features]}\")\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "def select_features_xgb(X_train, Y_train, varnum):\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42,n_estimator=100,max_depth=6,learning_rate=0.3)\n",
    "    xgb.fit(X_train, Y_train)\n",
    "    model_selector = SelectFromModel(xgb, max_features=varnum, prefit=True)\n",
    "    selected_features = model_selector.get_support(indices=True)\n",
    "    feature_importances = xgb.feature_importances_\n",
    "    \n",
    "    # Select the most important 'varnum' number feature\n",
    "    \n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    selected_features = sorted_indices[:varnum]\n",
    "    print(f\"Selected features (XGB): {selected_features}\")\n",
    "    print(f\"Feature importances: {feature_importances[selected_features]}\")\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Model functions\n",
    "def create_cart_model():\n",
    "    return DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "def create_rf_model():\n",
    "    return RandomForestClassifier(random_state=42)\n",
    "\n",
    "def create_xgb_model():\n",
    "    return XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "def create_lda_model():\n",
    "    return LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "# Custom time series splitting function\n",
    "def custom_time_series_split(data, train_period_months=6, test_period_months=1):\n",
    "    periods = []\n",
    "    start_date = data.index.min()\n",
    "    end_date = data.index.max()\n",
    "\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date + relativedelta(months=train_period_months + test_period_months) <= end_date:\n",
    "        train_start = current_date\n",
    "        train_end = train_start + relativedelta(months=train_period_months) - pd.DateOffset(days=1)\n",
    "        test_start = train_end + pd.DateOffset(days=1)\n",
    "        test_end = test_start + relativedelta(months=test_period_months) - pd.DateOffset(days=1)\n",
    "\n",
    "        train_indices = data.loc[train_start:train_end].index\n",
    "        test_indices = data.loc[test_start:test_end].index\n",
    "\n",
    "        periods.append((train_indices, test_indices))\n",
    "\n",
    "        current_date = current_date + relativedelta(months=1)\n",
    "\n",
    "    return periods\n",
    "\n",
    "def expanding_time_series_split(data, initial_train_period_months=6, test_period_months=1):\n",
    "    periods = []\n",
    "    start_date = data.index.min()\n",
    "    end_date = data.index.max()\n",
    "\n",
    "    current_date = start_date + relativedelta(months=initial_train_period_months)\n",
    "\n",
    "    while current_date + relativedelta(months=test_period_months) <= end_date:\n",
    "        train_start = start_date\n",
    "        train_end = current_date - pd.DateOffset(days=1)\n",
    "        test_start = train_end + pd.DateOffset(days=1)\n",
    "        test_end = test_start + relativedelta(months=test_period_months) - pd.DateOffset(days=1)\n",
    "\n",
    "        train_indices = data.loc[train_start:train_end].index\n",
    "        test_indices = data.loc[test_start:test_end].index\n",
    "\n",
    "        periods.append((train_indices, test_indices))\n",
    "\n",
    "        current_date = current_date + relativedelta(months=1)\n",
    "\n",
    "    return periods\n",
    "\n",
    "# Return calculation function\n",
    "def calculate_returns(data_main, predictions, start_date, end_date):\n",
    "    period_data = data_main.loc[start_date:end_date]\n",
    "    period_returns = period_data['Target_BASE']\n",
    "\n",
    "    correct_predictions = (predictions == period_data['Target'])\n",
    "    incorrect_predictions = ~correct_predictions\n",
    "\n",
    "    returns = np.where(correct_predictions, abs(period_returns), -abs(period_returns))\n",
    "    return returns.sum()\n",
    "\n",
    "def calculate_metrics(data_main, predictions, start_date, end_date):\n",
    "    period_data = data_main.loc[start_date:end_date]\n",
    "    period_returns = period_data['Target_BASE']\n",
    "\n",
    "    correct_predictions = (predictions == period_data['Target'])\n",
    "    incorrect_predictions = ~correct_predictions\n",
    "\n",
    "    returns = np.where(correct_predictions, abs(period_returns), -abs(period_returns))\n",
    "\n",
    "    return_df = pd.DataFrame(returns, index=period_data.index)\n",
    "\n",
    "    return_monthly= return_df.resample('M').sum()\n",
    "    min_return_monthly =return_monthly.min().iloc[0]\n",
    "    neg_months = (return_monthly < 0).sum().iloc[0].sum()\n",
    "\n",
    "\n",
    "    return_weekly = return_df.resample('W').sum()\n",
    "    min_return_weekly = return_weekly.min().iloc[0]\n",
    "    neg_weeks = (return_weekly < 0).sum().iloc[0].sum()\n",
    "\n",
    "\n",
    "    long_preds=np.where(predictions > 0.5,1,0)\n",
    "    short_preds=np.where(predictions < 0.5,1,0)\n",
    "\n",
    "    long_f1=f1_score(period_data['Target'],long_preds)\n",
    "    short_f1=f1_score(period_data['Target'],short_preds)\n",
    "\n",
    "    return return_monthly,min_return_monthly,neg_months,return_weekly,min_return_weekly,neg_weeks, long_f1, short_f1\n",
    "\n",
    "\n",
    "# Trend calculation function\n",
    "def calculate_trend(values):\n",
    "    if len(values) < 2:\n",
    "        return np.nan\n",
    "    x = np.arange(len(values))\n",
    "    slope, _, _, _, _ = linregress(x, values)\n",
    "    return slope\n",
    "\n",
    "# Evaluation\n",
    "varnums = [5,10,15]\n",
    "selection_methods = {\n",
    "    'CORR': select_features_corr,\n",
    "     'CHI2': select_features_chi_square,\n",
    "     'RF': select_features_rf,\n",
    "     'XGB': select_features_xgb,\n",
    "}\n",
    "results_df_roll = pd.DataFrame(columns=['Fold', 'Model', 'Feature Selection Method', 'VarNum', 'Train Period', 'Test Period', 'Train Accuracy', 'Test Accuracy', 'Train Return Mean', 'Test Return Mean', 'Selected Features'])\n",
    "\n",
    "#periods = expanding_time_series_split(data, initial_train_period_months=6, test_period_months=1)\n",
    "periods = custom_time_series_split(data, train_period_months=6, test_period_months=1)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "selected_features_count = {}\n",
    "\n",
    "for varnum in varnums:\n",
    "    for method_name, selection_method in selection_methods.items():\n",
    "        for model_name, create_model in [('CART', create_cart_model), ('RF', create_rf_model), ('XGB', create_xgb_model),('LDA', create_lda_model)]:\n",
    "            fold = 0\n",
    "            for train_indices, test_indices in periods:\n",
    "                train_indices = data.index.get_indexer(train_indices)\n",
    "                test_indices = data.index.get_indexer(test_indices)\n",
    "                                \n",
    "                X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "                Y_train, Y_test = Y.iloc[train_indices], Y.iloc[test_indices]\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "                \n",
    "                selected_features = selection_method(X_train, Y_train, varnum)\n",
    "                print(f\"Fold {fold}, VarNum {varnum}, Method {method_name}, Model {model_name}, Selected Features: {selected_features}\")\n",
    "\n",
    "                X_train_selected = X_train[:, selected_features]\n",
    "                X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "                selected_feature_names = X.columns[selected_features].tolist()\n",
    "\n",
    "                for feature in selected_feature_names:\n",
    "                    if feature not in selected_features_count:\n",
    "                        selected_features_count[feature] = 0\n",
    "                    selected_features_count[feature] += 1\n",
    "\n",
    "                model = create_model()\n",
    "                model.fit(X_train_selected, Y_train)\n",
    "\n",
    "                train_predictions = model.predict(X_train_selected)\n",
    "                test_predictions = model.predict(X_test_selected)\n",
    "\n",
    "                train_acc = np.mean(train_predictions == Y_train)\n",
    "                test_acc = np.mean(test_predictions == Y_test)\n",
    "\n",
    "                print(f\"Fold {fold}, VarNum {varnum}, Method {method_name}, Model {model_name}, Train Acc: {train_acc}, Test Acc: {test_acc}\")\n",
    "\n",
    "                train_start_date = data.index[train_indices].min()\n",
    "                train_end_date = data.index[train_indices].max()\n",
    "                test_start_date = data.index[test_indices].min()\n",
    "                test_end_date = data.index[test_indices].max()\n",
    "\n",
    "                train_return_mean = calculate_returns(data_main, train_predictions, train_start_date, train_end_date)\n",
    "                test_return_mean = calculate_returns(data_main, test_predictions, test_start_date, test_end_date)\n",
    "                return_monthly,min_return_monthly,neg_months,return_weekly,min_return_weekly,neg_weeks,long_f1,short_f1 = calculate_metrics(data_main, test_predictions, test_start_date, test_end_date)\n",
    "\n",
    "\n",
    "                fold += 1\n",
    "                results_df_roll = pd.concat([results_df_roll, pd.DataFrame({\n",
    "                    'Fold': [fold],\n",
    "                    'Model': [model_name],\n",
    "                    'Feature Selection Method': [method_name],\n",
    "                    'VarNum': [varnum],\n",
    "                    'Train Period': [f'{train_start_date.date()} - {train_end_date.date()}'],\n",
    "                    'Test Period': [f'{test_start_date.date()} - {test_end_date.date()}'],\n",
    "                    'Train Accuracy': [train_acc],\n",
    "                    'Test Accuracy': [test_acc],\n",
    "                    'Train Return Mean': [train_return_mean],\n",
    "                    'Test Return Mean': [test_return_mean],\n",
    "                    'Selected Features': [selected_feature_names],\n",
    "                    'Monthly Min Return':[min_return_monthly],\n",
    "                    'Negative Month Number':[neg_months],\n",
    "                    'Weekly Minimum Return':[min_return_weekly],\n",
    "                    'Negative Week Number':[neg_weeks],\n",
    "                    'Long F1':[long_f1],\n",
    "                    'Short F1':[short_f1]\n",
    "\n",
    "                })], ignore_index=True)\n",
    "\n",
    "# Calculate the number of negative return periods\n",
    "results_df_roll['Negative Train Return'] = results_df_roll['Train Return Mean'] < 0\n",
    "results_df_roll['Negative Test Return'] = results_df_roll['Test Return Mean'] < 0\n",
    "\n",
    "# Trend calculation\n",
    "results_df_roll['Test Accuracy Trend'] = results_df_roll.groupby(['Model', 'Feature Selection Method', 'VarNum'])['Test Accuracy'].transform(lambda x: calculate_trend(x[-25:]))\n",
    "results_df_roll['Test Return Mean Trend'] = results_df_roll.groupby(['Model', 'Feature Selection Method', 'VarNum'])['Test Return Mean'].transform(lambda x: calculate_trend(x[-25:]))\n",
    "\n",
    "# Calculate the mean and std of test accuracies\n",
    "mean_test_acc = results_df_roll.groupby(['Model', 'Feature Selection Method', 'VarNum'])['Test Accuracy'].mean()\n",
    "std_test_acc = results_df_roll.groupby(['Model', 'Feature Selection Method', 'VarNum'])['Test Accuracy'].std()\n",
    "\n",
    "# Create a pivot table for the results\n",
    "pivot_table_roll = results_df_roll.pivot_table(\n",
    "    index=['Model', 'Feature Selection Method', 'VarNum'],\n",
    "\n",
    "    values=['Test Accuracy', 'Test Return Mean', 'Negative Test Return', 'Test Accuracy Trend', 'Test Return Mean Trend','Monthly Min Return','Negative Month Number',\n",
    "            'Weekly Minimum Return','Negative Week Number','Long F1','Short F1'],\n",
    "    aggfunc={\n",
    "        'Test Accuracy': [np.mean, np.std, np.min],\n",
    "        'Test Return Mean': [np.mean, np.std],\n",
    "        'Negative Test Return': np.sum,\n",
    "        'Test Accuracy Trend': np.mean,\n",
    "        'Test Return Mean Trend': np.mean,\n",
    "        'Monthly Min Return':np.mean,\n",
    "        'Negative Month Number':np.mean,\n",
    "        'Weekly Minimum Return':np.mean,\n",
    "        'Negative Week Number':np.mean,\n",
    "        'Long F1':np.mean,\n",
    "        'Short F1':np.mean\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "pivot_table_roll.columns = [' '.join(col).strip() for col in pivot_table_roll.columns.values]\n",
    "pivot_table_roll['Periods'] = len(periods)\n",
    "\n",
    "# Create placeholder for Top Features\n",
    "pivot_table_roll[['Top Feature 1', 'Top Feature 2', 'Top Feature 3', 'Top Feature 4', 'Top Feature 5']] = ''\n",
    "\n",
    "# Find the 5 most frequently used variables in the result_df table and add them to the pivot_table\n",
    "for idx, row in pivot_table_roll.iterrows():\n",
    "    model_name = row['Model']\n",
    "    method_name = row['Feature Selection Method']\n",
    "    varnum = row['VarNum']\n",
    "\n",
    "    filtered_df = results_df_roll[(results_df_roll['Model'] == model_name) &\n",
    "                             (results_df_roll['Feature Selection Method'] == method_name) &\n",
    "                             (results_df_roll['VarNum'] == varnum)]\n",
    "\n",
    "    feature_counts = {}\n",
    "    for features in filtered_df['Selected Features']:\n",
    "        for feature in features:\n",
    "            if feature not in feature_counts:\n",
    "                feature_counts[feature] = 0\n",
    "            feature_counts[feature] += 1\n",
    "\n",
    "    top_features = pd.Series(feature_counts).nlargest(5).index.tolist()\n",
    "\n",
    "    for i, feature in enumerate(top_features):\n",
    "        pivot_table_roll.at[idx, f'Top Feature {i+1}'] = feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "fdQKBILCTBsQ",
    "outputId": "8c8c4ea9-8a6f-4c71-8217-7e3649e7fd3b"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Selection Method</th>\n",
       "      <th>VarNum</th>\n",
       "      <th>Long F1 mean</th>\n",
       "      <th>Monthly Min Return mean</th>\n",
       "      <th>Negative Month Number mean</th>\n",
       "      <th>Negative Test Return sum</th>\n",
       "      <th>Negative Week Number mean</th>\n",
       "      <th>Short F1 mean</th>\n",
       "      <th>Test Accuracy amin</th>\n",
       "      <th>Test Accuracy mean</th>\n",
       "      <th>Test Accuracy std</th>\n",
       "      <th>Test Accuracy Trend mean</th>\n",
       "      <th>Test Return Mean mean</th>\n",
       "      <th>Test Return Mean std</th>\n",
       "      <th>Test Return Mean Trend mean</th>\n",
       "      <th>Weekly Minimum Return mean</th>\n",
       "      <th>Periods</th>\n",
       "      <th>Top Feature 1</th>\n",
       "      <th>Top Feature 2</th>\n",
       "      <th>Top Feature 3</th>\n",
       "      <th>Top Feature 4</th>\n",
       "      <th>Top Feature 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CART</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.582865</td>\n",
       "      <td>0.219092</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>4</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>0.404762</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.575547</td>\n",
       "      <td>0.099888</td>\n",
       "      <td>-0.006348</td>\n",
       "      <td>2.527727</td>\n",
       "      <td>2.510014</td>\n",
       "      <td>-0.091807</td>\n",
       "      <td>-1.110000</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CART</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.615319</td>\n",
       "      <td>1.165910</td>\n",
       "      <td>0.227273</td>\n",
       "      <td>1</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.369625</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.623306</td>\n",
       "      <td>0.083067</td>\n",
       "      <td>-0.003039</td>\n",
       "      <td>3.916818</td>\n",
       "      <td>2.954734</td>\n",
       "      <td>-0.080672</td>\n",
       "      <td>-0.668183</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CART</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.572240</td>\n",
       "      <td>0.858182</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>4</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.407248</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.591092</td>\n",
       "      <td>0.123076</td>\n",
       "      <td>-0.014090</td>\n",
       "      <td>2.768639</td>\n",
       "      <td>3.219808</td>\n",
       "      <td>-0.384647</td>\n",
       "      <td>-0.860454</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CART</td>\n",
       "      <td>CORR</td>\n",
       "      <td>5</td>\n",
       "      <td>0.546121</td>\n",
       "      <td>-0.067272</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>6</td>\n",
       "      <td>1.681818</td>\n",
       "      <td>0.442854</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.570561</td>\n",
       "      <td>0.094060</td>\n",
       "      <td>-0.004886</td>\n",
       "      <td>2.151366</td>\n",
       "      <td>2.588190</td>\n",
       "      <td>-0.099215</td>\n",
       "      <td>-1.242727</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CART</td>\n",
       "      <td>CORR</td>\n",
       "      <td>10</td>\n",
       "      <td>0.592467</td>\n",
       "      <td>0.692727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>4</td>\n",
       "      <td>1.409091</td>\n",
       "      <td>0.388001</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.605465</td>\n",
       "      <td>0.122911</td>\n",
       "      <td>-0.003695</td>\n",
       "      <td>3.026820</td>\n",
       "      <td>2.835909</td>\n",
       "      <td>-0.019497</td>\n",
       "      <td>-0.852727</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CART</td>\n",
       "      <td>CORR</td>\n",
       "      <td>15</td>\n",
       "      <td>0.604854</td>\n",
       "      <td>0.671365</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>0.394080</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.596827</td>\n",
       "      <td>0.091604</td>\n",
       "      <td>-0.007736</td>\n",
       "      <td>2.556820</td>\n",
       "      <td>2.715199</td>\n",
       "      <td>-0.284285</td>\n",
       "      <td>-0.912727</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CART</td>\n",
       "      <td>RF</td>\n",
       "      <td>5</td>\n",
       "      <td>0.575937</td>\n",
       "      <td>0.383637</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>4</td>\n",
       "      <td>1.681818</td>\n",
       "      <td>0.414290</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.562428</td>\n",
       "      <td>0.100915</td>\n",
       "      <td>-0.003966</td>\n",
       "      <td>2.369546</td>\n",
       "      <td>2.616207</td>\n",
       "      <td>-0.142874</td>\n",
       "      <td>-0.933636</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CART</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.567338</td>\n",
       "      <td>0.037271</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>7</td>\n",
       "      <td>1.772727</td>\n",
       "      <td>0.409857</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.572598</td>\n",
       "      <td>0.140063</td>\n",
       "      <td>-0.010372</td>\n",
       "      <td>1.998635</td>\n",
       "      <td>3.897937</td>\n",
       "      <td>-0.253343</td>\n",
       "      <td>-1.106819</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CART</td>\n",
       "      <td>RF</td>\n",
       "      <td>15</td>\n",
       "      <td>0.620156</td>\n",
       "      <td>0.627727</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>5</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>0.362147</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.590089</td>\n",
       "      <td>0.123785</td>\n",
       "      <td>-0.010156</td>\n",
       "      <td>2.599545</td>\n",
       "      <td>3.873750</td>\n",
       "      <td>-0.303992</td>\n",
       "      <td>-0.891818</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^TWII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CART</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.480400</td>\n",
       "      <td>-0.931364</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>7</td>\n",
       "      <td>2.181818</td>\n",
       "      <td>0.460053</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.521328</td>\n",
       "      <td>0.108768</td>\n",
       "      <td>-0.002030</td>\n",
       "      <td>0.474092</td>\n",
       "      <td>3.376659</td>\n",
       "      <td>-0.046940</td>\n",
       "      <td>-1.621363</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_^DJI</td>\n",
       "      <td>Return_ADBE</td>\n",
       "      <td>Return_TXN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CART</td>\n",
       "      <td>XGB</td>\n",
       "      <td>10</td>\n",
       "      <td>0.544626</td>\n",
       "      <td>-0.130910</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>6</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>0.397976</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.553694</td>\n",
       "      <td>0.103986</td>\n",
       "      <td>0.000571</td>\n",
       "      <td>1.997726</td>\n",
       "      <td>3.077036</td>\n",
       "      <td>0.043269</td>\n",
       "      <td>-1.079545</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_TXN</td>\n",
       "      <td>CCI_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CART</td>\n",
       "      <td>XGB</td>\n",
       "      <td>15</td>\n",
       "      <td>0.521908</td>\n",
       "      <td>-0.478636</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>8</td>\n",
       "      <td>2.136364</td>\n",
       "      <td>0.416640</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.529707</td>\n",
       "      <td>0.115702</td>\n",
       "      <td>0.001688</td>\n",
       "      <td>1.255909</td>\n",
       "      <td>3.016739</td>\n",
       "      <td>0.116584</td>\n",
       "      <td>-1.298637</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>WilliamsR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.669777</td>\n",
       "      <td>1.428636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.292229</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.672827</td>\n",
       "      <td>0.114776</td>\n",
       "      <td>-0.011106</td>\n",
       "      <td>5.098635</td>\n",
       "      <td>2.388732</td>\n",
       "      <td>-0.153016</td>\n",
       "      <td>-0.415909</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.698646</td>\n",
       "      <td>1.324545</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>1</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.241551</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.669926</td>\n",
       "      <td>0.125428</td>\n",
       "      <td>-0.008010</td>\n",
       "      <td>4.825001</td>\n",
       "      <td>2.848791</td>\n",
       "      <td>-0.155398</td>\n",
       "      <td>-0.515455</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.663344</td>\n",
       "      <td>1.438636</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>3</td>\n",
       "      <td>1.227273</td>\n",
       "      <td>0.288240</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.648731</td>\n",
       "      <td>0.134404</td>\n",
       "      <td>-0.010220</td>\n",
       "      <td>4.584092</td>\n",
       "      <td>3.444145</td>\n",
       "      <td>-0.204241</td>\n",
       "      <td>-0.621363</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CORR</td>\n",
       "      <td>5</td>\n",
       "      <td>0.609144</td>\n",
       "      <td>0.677272</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333172</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.636455</td>\n",
       "      <td>0.088882</td>\n",
       "      <td>-0.003708</td>\n",
       "      <td>3.980454</td>\n",
       "      <td>2.234978</td>\n",
       "      <td>0.008182</td>\n",
       "      <td>-0.690000</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CORR</td>\n",
       "      <td>10</td>\n",
       "      <td>0.659436</td>\n",
       "      <td>1.228181</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.287772</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.667243</td>\n",
       "      <td>0.102069</td>\n",
       "      <td>-0.009218</td>\n",
       "      <td>5.341363</td>\n",
       "      <td>2.888665</td>\n",
       "      <td>-0.151649</td>\n",
       "      <td>-0.461819</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CORR</td>\n",
       "      <td>15</td>\n",
       "      <td>0.625337</td>\n",
       "      <td>1.381818</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.328929</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.651714</td>\n",
       "      <td>0.106609</td>\n",
       "      <td>-0.006378</td>\n",
       "      <td>5.089546</td>\n",
       "      <td>3.062947</td>\n",
       "      <td>-0.033331</td>\n",
       "      <td>-0.440454</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LDA</td>\n",
       "      <td>RF</td>\n",
       "      <td>5</td>\n",
       "      <td>0.658328</td>\n",
       "      <td>0.847273</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>1</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>0.336841</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.653158</td>\n",
       "      <td>0.105587</td>\n",
       "      <td>-0.006657</td>\n",
       "      <td>4.583181</td>\n",
       "      <td>2.728268</td>\n",
       "      <td>-0.041474</td>\n",
       "      <td>-0.499546</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LDA</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.643717</td>\n",
       "      <td>1.104091</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.355609</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.635726</td>\n",
       "      <td>0.134469</td>\n",
       "      <td>-0.008222</td>\n",
       "      <td>4.608635</td>\n",
       "      <td>2.871597</td>\n",
       "      <td>-0.099565</td>\n",
       "      <td>-0.445000</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LDA</td>\n",
       "      <td>RF</td>\n",
       "      <td>15</td>\n",
       "      <td>0.658430</td>\n",
       "      <td>0.755908</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>3</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>0.314036</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.631012</td>\n",
       "      <td>0.136385</td>\n",
       "      <td>-0.012239</td>\n",
       "      <td>4.026817</td>\n",
       "      <td>3.787992</td>\n",
       "      <td>-0.233049</td>\n",
       "      <td>-0.877273</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^TWII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LDA</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.568515</td>\n",
       "      <td>0.622273</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>6</td>\n",
       "      <td>1.863636</td>\n",
       "      <td>0.298397</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.587689</td>\n",
       "      <td>0.117280</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>2.327729</td>\n",
       "      <td>3.403955</td>\n",
       "      <td>0.000864</td>\n",
       "      <td>-1.233636</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_^DJI</td>\n",
       "      <td>Return_ADBE</td>\n",
       "      <td>Return_TXN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LDA</td>\n",
       "      <td>XGB</td>\n",
       "      <td>10</td>\n",
       "      <td>0.584599</td>\n",
       "      <td>0.585454</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.681818</td>\n",
       "      <td>0.304066</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.604544</td>\n",
       "      <td>0.127395</td>\n",
       "      <td>-0.007087</td>\n",
       "      <td>3.312273</td>\n",
       "      <td>3.198349</td>\n",
       "      <td>-0.063924</td>\n",
       "      <td>-1.096818</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_TXN</td>\n",
       "      <td>CCI_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LDA</td>\n",
       "      <td>XGB</td>\n",
       "      <td>15</td>\n",
       "      <td>0.576131</td>\n",
       "      <td>0.172272</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>5</td>\n",
       "      <td>1.681818</td>\n",
       "      <td>0.313116</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.591479</td>\n",
       "      <td>0.118882</td>\n",
       "      <td>-0.002574</td>\n",
       "      <td>2.798637</td>\n",
       "      <td>3.212251</td>\n",
       "      <td>0.040954</td>\n",
       "      <td>-1.183182</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>WilliamsR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RF</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.646643</td>\n",
       "      <td>1.299091</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>0.318114</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.654314</td>\n",
       "      <td>0.104799</td>\n",
       "      <td>-0.005161</td>\n",
       "      <td>4.841365</td>\n",
       "      <td>2.171303</td>\n",
       "      <td>-0.058933</td>\n",
       "      <td>-0.389546</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RF</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.651633</td>\n",
       "      <td>1.227273</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.315258</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.651853</td>\n",
       "      <td>0.102472</td>\n",
       "      <td>-0.005192</td>\n",
       "      <td>4.665000</td>\n",
       "      <td>1.960421</td>\n",
       "      <td>-0.104557</td>\n",
       "      <td>-0.405000</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RF</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.625509</td>\n",
       "      <td>0.803182</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>2</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>0.359618</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.609236</td>\n",
       "      <td>0.125419</td>\n",
       "      <td>-0.010218</td>\n",
       "      <td>3.477728</td>\n",
       "      <td>3.273762</td>\n",
       "      <td>-0.240864</td>\n",
       "      <td>-0.583637</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RF</td>\n",
       "      <td>CORR</td>\n",
       "      <td>5</td>\n",
       "      <td>0.613576</td>\n",
       "      <td>0.708183</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>1</td>\n",
       "      <td>1.409091</td>\n",
       "      <td>0.345136</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.613680</td>\n",
       "      <td>0.095686</td>\n",
       "      <td>-0.003540</td>\n",
       "      <td>3.598639</td>\n",
       "      <td>2.472801</td>\n",
       "      <td>-0.024658</td>\n",
       "      <td>-0.834545</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RF</td>\n",
       "      <td>CORR</td>\n",
       "      <td>10</td>\n",
       "      <td>0.653107</td>\n",
       "      <td>1.026363</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>0.323250</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.653295</td>\n",
       "      <td>0.090096</td>\n",
       "      <td>-0.005420</td>\n",
       "      <td>4.509546</td>\n",
       "      <td>2.680927</td>\n",
       "      <td>-0.124082</td>\n",
       "      <td>-0.614545</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RF</td>\n",
       "      <td>CORR</td>\n",
       "      <td>15</td>\n",
       "      <td>0.673151</td>\n",
       "      <td>0.779091</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1</td>\n",
       "      <td>1.409091</td>\n",
       "      <td>0.280740</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.647619</td>\n",
       "      <td>0.091219</td>\n",
       "      <td>-0.005579</td>\n",
       "      <td>3.987728</td>\n",
       "      <td>2.342737</td>\n",
       "      <td>-0.065110</td>\n",
       "      <td>-0.823182</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>5</td>\n",
       "      <td>0.636344</td>\n",
       "      <td>0.987726</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>0.335614</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.623446</td>\n",
       "      <td>0.071185</td>\n",
       "      <td>-0.005987</td>\n",
       "      <td>3.684999</td>\n",
       "      <td>1.960920</td>\n",
       "      <td>-0.116844</td>\n",
       "      <td>-0.570455</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.632178</td>\n",
       "      <td>0.910454</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.352259</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.625608</td>\n",
       "      <td>0.085126</td>\n",
       "      <td>-0.006907</td>\n",
       "      <td>3.818635</td>\n",
       "      <td>2.357471</td>\n",
       "      <td>-0.092078</td>\n",
       "      <td>-0.656364</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>15</td>\n",
       "      <td>0.593008</td>\n",
       "      <td>0.624546</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>1</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>0.384446</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.601235</td>\n",
       "      <td>0.114693</td>\n",
       "      <td>-0.009617</td>\n",
       "      <td>3.691364</td>\n",
       "      <td>2.613367</td>\n",
       "      <td>-0.142671</td>\n",
       "      <td>-0.726818</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^TWII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>RF</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.581433</td>\n",
       "      <td>-0.003637</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>6</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>0.373741</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.587524</td>\n",
       "      <td>0.117078</td>\n",
       "      <td>0.002025</td>\n",
       "      <td>1.879546</td>\n",
       "      <td>3.027608</td>\n",
       "      <td>0.067369</td>\n",
       "      <td>-1.602727</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_^DJI</td>\n",
       "      <td>Return_ADBE</td>\n",
       "      <td>Return_TXN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RF</td>\n",
       "      <td>XGB</td>\n",
       "      <td>10</td>\n",
       "      <td>0.605625</td>\n",
       "      <td>0.633636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>0.348710</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.609115</td>\n",
       "      <td>0.076309</td>\n",
       "      <td>-0.000590</td>\n",
       "      <td>3.288635</td>\n",
       "      <td>1.998248</td>\n",
       "      <td>0.036336</td>\n",
       "      <td>-0.757727</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_TXN</td>\n",
       "      <td>CCI_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RF</td>\n",
       "      <td>XGB</td>\n",
       "      <td>15</td>\n",
       "      <td>0.623116</td>\n",
       "      <td>1.032272</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>0.339518</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.623991</td>\n",
       "      <td>0.102804</td>\n",
       "      <td>-0.002705</td>\n",
       "      <td>4.276818</td>\n",
       "      <td>2.501344</td>\n",
       "      <td>0.030700</td>\n",
       "      <td>-0.792728</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>WilliamsR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.633452</td>\n",
       "      <td>1.273637</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.349779</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.634082</td>\n",
       "      <td>0.122114</td>\n",
       "      <td>-0.005990</td>\n",
       "      <td>4.374091</td>\n",
       "      <td>3.125094</td>\n",
       "      <td>-0.182931</td>\n",
       "      <td>-0.432274</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.629836</td>\n",
       "      <td>1.165909</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>3</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.336700</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.600890</td>\n",
       "      <td>0.093766</td>\n",
       "      <td>-0.006332</td>\n",
       "      <td>3.555909</td>\n",
       "      <td>2.826949</td>\n",
       "      <td>-0.140784</td>\n",
       "      <td>-0.745456</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.654632</td>\n",
       "      <td>1.405909</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>2</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.320580</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.634979</td>\n",
       "      <td>0.091412</td>\n",
       "      <td>-0.008714</td>\n",
       "      <td>3.894092</td>\n",
       "      <td>2.730311</td>\n",
       "      <td>-0.233614</td>\n",
       "      <td>-0.879546</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CORR</td>\n",
       "      <td>5</td>\n",
       "      <td>0.610460</td>\n",
       "      <td>0.610001</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>4</td>\n",
       "      <td>1.181818</td>\n",
       "      <td>0.373294</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.614143</td>\n",
       "      <td>0.113998</td>\n",
       "      <td>-0.002200</td>\n",
       "      <td>2.866821</td>\n",
       "      <td>3.484799</td>\n",
       "      <td>-0.013377</td>\n",
       "      <td>-0.894091</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CORR</td>\n",
       "      <td>10</td>\n",
       "      <td>0.642776</td>\n",
       "      <td>1.097272</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>2</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>0.340467</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.643539</td>\n",
       "      <td>0.115794</td>\n",
       "      <td>-0.008648</td>\n",
       "      <td>4.045911</td>\n",
       "      <td>2.764069</td>\n",
       "      <td>-0.196381</td>\n",
       "      <td>-0.726819</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CORR</td>\n",
       "      <td>15</td>\n",
       "      <td>0.645665</td>\n",
       "      <td>1.097727</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>2</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.339454</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.632719</td>\n",
       "      <td>0.095240</td>\n",
       "      <td>-0.006846</td>\n",
       "      <td>3.979546</td>\n",
       "      <td>2.408774</td>\n",
       "      <td>-0.184195</td>\n",
       "      <td>-0.672273</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>XGB</td>\n",
       "      <td>RF</td>\n",
       "      <td>5</td>\n",
       "      <td>0.621143</td>\n",
       "      <td>1.053636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>3</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>0.359465</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.611466</td>\n",
       "      <td>0.096500</td>\n",
       "      <td>-0.004748</td>\n",
       "      <td>3.387728</td>\n",
       "      <td>2.927613</td>\n",
       "      <td>-0.175850</td>\n",
       "      <td>-0.650001</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>XGB</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.598938</td>\n",
       "      <td>0.775909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.590909</td>\n",
       "      <td>0.393042</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.599285</td>\n",
       "      <td>0.103083</td>\n",
       "      <td>-0.007287</td>\n",
       "      <td>3.050455</td>\n",
       "      <td>2.722697</td>\n",
       "      <td>-0.164026</td>\n",
       "      <td>-0.775909</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>XGB</td>\n",
       "      <td>RF</td>\n",
       "      <td>15</td>\n",
       "      <td>0.619767</td>\n",
       "      <td>1.094999</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>2</td>\n",
       "      <td>1.363636</td>\n",
       "      <td>0.372000</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.613235</td>\n",
       "      <td>0.114442</td>\n",
       "      <td>-0.006755</td>\n",
       "      <td>3.966817</td>\n",
       "      <td>2.968113</td>\n",
       "      <td>-0.065076</td>\n",
       "      <td>-0.626819</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^TWII</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>XGB</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.560921</td>\n",
       "      <td>-0.132275</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>8</td>\n",
       "      <td>2.045455</td>\n",
       "      <td>0.371484</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.572388</td>\n",
       "      <td>0.099475</td>\n",
       "      <td>-0.002064</td>\n",
       "      <td>1.428636</td>\n",
       "      <td>2.861168</td>\n",
       "      <td>-0.052270</td>\n",
       "      <td>-1.721363</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_^DJI</td>\n",
       "      <td>Return_ADBE</td>\n",
       "      <td>Return_TXN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>XGB</td>\n",
       "      <td>XGB</td>\n",
       "      <td>10</td>\n",
       "      <td>0.599630</td>\n",
       "      <td>0.616363</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>2</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>0.334237</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.613036</td>\n",
       "      <td>0.110285</td>\n",
       "      <td>-0.005459</td>\n",
       "      <td>3.344091</td>\n",
       "      <td>2.548680</td>\n",
       "      <td>-0.075183</td>\n",
       "      <td>-0.688181</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_TXN</td>\n",
       "      <td>CCI_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>XGB</td>\n",
       "      <td>XGB</td>\n",
       "      <td>15</td>\n",
       "      <td>0.614535</td>\n",
       "      <td>0.882273</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>3</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>0.333849</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.612670</td>\n",
       "      <td>0.113761</td>\n",
       "      <td>-0.001124</td>\n",
       "      <td>3.599546</td>\n",
       "      <td>2.909947</td>\n",
       "      <td>-0.017510</td>\n",
       "      <td>-0.910455</td>\n",
       "      <td>22</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>WilliamsR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Feature Selection Method  VarNum  Long F1 mean  \\\n",
       "0   CART                     CHI2       5      0.582865   \n",
       "1   CART                     CHI2      10      0.615319   \n",
       "2   CART                     CHI2      15      0.572240   \n",
       "3   CART                     CORR       5      0.546121   \n",
       "4   CART                     CORR      10      0.592467   \n",
       "5   CART                     CORR      15      0.604854   \n",
       "6   CART                       RF       5      0.575937   \n",
       "7   CART                       RF      10      0.567338   \n",
       "8   CART                       RF      15      0.620156   \n",
       "9   CART                      XGB       5      0.480400   \n",
       "10  CART                      XGB      10      0.544626   \n",
       "11  CART                      XGB      15      0.521908   \n",
       "12   LDA                     CHI2       5      0.669777   \n",
       "13   LDA                     CHI2      10      0.698646   \n",
       "14   LDA                     CHI2      15      0.663344   \n",
       "15   LDA                     CORR       5      0.609144   \n",
       "16   LDA                     CORR      10      0.659436   \n",
       "17   LDA                     CORR      15      0.625337   \n",
       "18   LDA                       RF       5      0.658328   \n",
       "19   LDA                       RF      10      0.643717   \n",
       "20   LDA                       RF      15      0.658430   \n",
       "21   LDA                      XGB       5      0.568515   \n",
       "22   LDA                      XGB      10      0.584599   \n",
       "23   LDA                      XGB      15      0.576131   \n",
       "24    RF                     CHI2       5      0.646643   \n",
       "25    RF                     CHI2      10      0.651633   \n",
       "26    RF                     CHI2      15      0.625509   \n",
       "27    RF                     CORR       5      0.613576   \n",
       "28    RF                     CORR      10      0.653107   \n",
       "29    RF                     CORR      15      0.673151   \n",
       "30    RF                       RF       5      0.636344   \n",
       "31    RF                       RF      10      0.632178   \n",
       "32    RF                       RF      15      0.593008   \n",
       "33    RF                      XGB       5      0.581433   \n",
       "34    RF                      XGB      10      0.605625   \n",
       "35    RF                      XGB      15      0.623116   \n",
       "36   XGB                     CHI2       5      0.633452   \n",
       "37   XGB                     CHI2      10      0.629836   \n",
       "38   XGB                     CHI2      15      0.654632   \n",
       "39   XGB                     CORR       5      0.610460   \n",
       "40   XGB                     CORR      10      0.642776   \n",
       "41   XGB                     CORR      15      0.645665   \n",
       "42   XGB                       RF       5      0.621143   \n",
       "43   XGB                       RF      10      0.598938   \n",
       "44   XGB                       RF      15      0.619767   \n",
       "45   XGB                      XGB       5      0.560921   \n",
       "46   XGB                      XGB      10      0.599630   \n",
       "47   XGB                      XGB      15      0.614535   \n",
       "\n",
       "    Monthly Min Return mean  Negative Month Number mean  \\\n",
       "0                  0.219092                    0.727273   \n",
       "1                  1.165910                    0.227273   \n",
       "2                  0.858182                    0.545455   \n",
       "3                 -0.067272                    0.590909   \n",
       "4                  0.692727                    0.545455   \n",
       "5                  0.671365                    0.500000   \n",
       "6                  0.383637                    0.590909   \n",
       "7                  0.037271                    0.727273   \n",
       "8                  0.627727                    0.545455   \n",
       "9                 -0.931364                    0.818182   \n",
       "10                -0.130910                    0.545455   \n",
       "11                -0.478636                    0.590909   \n",
       "12                 1.428636                    0.363636   \n",
       "13                 1.324545                    0.318182   \n",
       "14                 1.438636                    0.454545   \n",
       "15                 0.677272                    0.454545   \n",
       "16                 1.228181                    0.363636   \n",
       "17                 1.381818                    0.363636   \n",
       "18                 0.847273                    0.454545   \n",
       "19                 1.104091                    0.545455   \n",
       "20                 0.755908                    0.636364   \n",
       "21                 0.622273                    0.727273   \n",
       "22                 0.585454                    0.500000   \n",
       "23                 0.172272                    0.636364   \n",
       "24                 1.299091                    0.318182   \n",
       "25                 1.227273                    0.363636   \n",
       "26                 0.803182                    0.454545   \n",
       "27                 0.708183                    0.318182   \n",
       "28                 1.026363                    0.363636   \n",
       "29                 0.779091                    0.409091   \n",
       "30                 0.987726                    0.363636   \n",
       "31                 0.910454                    0.409091   \n",
       "32                 0.624546                    0.545455   \n",
       "33                -0.003637                    0.681818   \n",
       "34                 0.633636                    0.409091   \n",
       "35                 1.032272                    0.363636   \n",
       "36                 1.273637                    0.272727   \n",
       "37                 1.165909                    0.318182   \n",
       "38                 1.405909                    0.454545   \n",
       "39                 0.610001                    0.454545   \n",
       "40                 1.097272                    0.318182   \n",
       "41                 1.097727                    0.363636   \n",
       "42                 1.053636                    0.409091   \n",
       "43                 0.775909                    0.500000   \n",
       "44                 1.094999                    0.454545   \n",
       "45                -0.132275                    0.909091   \n",
       "46                 0.616363                    0.363636   \n",
       "47                 0.882273                    0.454545   \n",
       "\n",
       "    Negative Test Return sum  Negative Week Number mean  Short F1 mean  \\\n",
       "0                          4                   1.727273       0.404762   \n",
       "1                          1                   1.045455       0.369625   \n",
       "2                          4                   1.500000       0.407248   \n",
       "3                          6                   1.681818       0.442854   \n",
       "4                          4                   1.409091       0.388001   \n",
       "5                          3                   1.454545       0.394080   \n",
       "6                          4                   1.681818       0.414290   \n",
       "7                          7                   1.772727       0.409857   \n",
       "8                          5                   1.636364       0.362147   \n",
       "9                          7                   2.181818       0.460053   \n",
       "10                         6                   1.727273       0.397976   \n",
       "11                         8                   2.136364       0.416640   \n",
       "12                         0                   1.000000       0.292229   \n",
       "13                         1                   1.045455       0.241551   \n",
       "14                         3                   1.227273       0.288240   \n",
       "15                         1                   1.000000       0.333172   \n",
       "16                         1                   0.727273       0.287772   \n",
       "17                         1                   0.909091       0.328929   \n",
       "18                         1                   1.090909       0.336841   \n",
       "19                         1                   1.045455       0.355609   \n",
       "20                         3                   1.318182       0.314036   \n",
       "21                         6                   1.863636       0.298397   \n",
       "22                         3                   1.681818       0.304066   \n",
       "23                         5                   1.681818       0.313116   \n",
       "24                         0                   1.090909       0.318114   \n",
       "25                         0                   1.000000       0.315258   \n",
       "26                         2                   1.454545       0.359618   \n",
       "27                         1                   1.409091       0.345136   \n",
       "28                         1                   1.090909       0.323250   \n",
       "29                         1                   1.409091       0.280740   \n",
       "30                         1                   1.363636       0.335614   \n",
       "31                         1                   1.272727       0.352259   \n",
       "32                         1                   1.363636       0.384446   \n",
       "33                         6                   1.727273       0.373741   \n",
       "34                         1                   1.454545       0.348710   \n",
       "35                         0                   1.318182       0.339518   \n",
       "36                         1                   1.181818       0.349779   \n",
       "37                         3                   1.181818       0.336700   \n",
       "38                         2                   1.272727       0.320580   \n",
       "39                         4                   1.181818       0.373294   \n",
       "40                         2                   1.363636       0.340467   \n",
       "41                         2                   1.272727       0.339454   \n",
       "42                         3                   1.363636       0.359465   \n",
       "43                         4                   1.590909       0.393042   \n",
       "44                         2                   1.363636       0.372000   \n",
       "45                         8                   2.045455       0.371484   \n",
       "46                         2                   1.318182       0.334237   \n",
       "47                         3                   1.318182       0.333849   \n",
       "\n",
       "    Test Accuracy amin  Test Accuracy mean  Test Accuracy std  \\\n",
       "0             0.363636            0.575547           0.099888   \n",
       "1             0.368421            0.623306           0.083067   \n",
       "2             0.318182            0.591092           0.123076   \n",
       "3             0.318182            0.570561           0.094060   \n",
       "4             0.409091            0.605465           0.122911   \n",
       "5             0.454545            0.596827           0.091604   \n",
       "6             0.350000            0.562428           0.100915   \n",
       "7             0.285714            0.572598           0.140063   \n",
       "8             0.333333            0.590089           0.123785   \n",
       "9             0.300000            0.521328           0.108768   \n",
       "10            0.315789            0.553694           0.103986   \n",
       "11            0.315789            0.529707           0.115702   \n",
       "12            0.450000            0.672827           0.114776   \n",
       "13            0.454545            0.669926           0.125428   \n",
       "14            0.333333            0.648731           0.134404   \n",
       "15            0.450000            0.636455           0.088882   \n",
       "16            0.473684            0.667243           0.102069   \n",
       "17            0.421053            0.651714           0.106609   \n",
       "18            0.454545            0.653158           0.105587   \n",
       "19            0.363636            0.635726           0.134469   \n",
       "20            0.350000            0.631012           0.136385   \n",
       "21            0.380952            0.587689           0.117280   \n",
       "22            0.368421            0.604544           0.127395   \n",
       "23            0.409091            0.591479           0.118882   \n",
       "24            0.500000            0.654314           0.104799   \n",
       "25            0.409091            0.651853           0.102472   \n",
       "26            0.363636            0.609236           0.125419   \n",
       "27            0.363636            0.613680           0.095686   \n",
       "28            0.368421            0.653295           0.090096   \n",
       "29            0.421053            0.647619           0.091219   \n",
       "30            0.454545            0.623446           0.071185   \n",
       "31            0.473684            0.625608           0.085126   \n",
       "32            0.368421            0.601235           0.114693   \n",
       "33            0.380952            0.587524           0.117078   \n",
       "34            0.500000            0.609115           0.076309   \n",
       "35            0.454545            0.623991           0.102804   \n",
       "36            0.272727            0.634082           0.122114   \n",
       "37            0.409091            0.600890           0.093766   \n",
       "38            0.454545            0.634979           0.091412   \n",
       "39            0.363636            0.614143           0.113998   \n",
       "40            0.421053            0.643539           0.115794   \n",
       "41            0.421053            0.632719           0.095240   \n",
       "42            0.409091            0.611466           0.096500   \n",
       "43            0.409091            0.599285           0.103083   \n",
       "44            0.428571            0.613235           0.114442   \n",
       "45            0.428571            0.572388           0.099475   \n",
       "46            0.450000            0.613036           0.110285   \n",
       "47            0.368421            0.612670           0.113761   \n",
       "\n",
       "    Test Accuracy Trend mean  Test Return Mean mean  Test Return Mean std  \\\n",
       "0                  -0.006348               2.527727              2.510014   \n",
       "1                  -0.003039               3.916818              2.954734   \n",
       "2                  -0.014090               2.768639              3.219808   \n",
       "3                  -0.004886               2.151366              2.588190   \n",
       "4                  -0.003695               3.026820              2.835909   \n",
       "5                  -0.007736               2.556820              2.715199   \n",
       "6                  -0.003966               2.369546              2.616207   \n",
       "7                  -0.010372               1.998635              3.897937   \n",
       "8                  -0.010156               2.599545              3.873750   \n",
       "9                  -0.002030               0.474092              3.376659   \n",
       "10                  0.000571               1.997726              3.077036   \n",
       "11                  0.001688               1.255909              3.016739   \n",
       "12                 -0.011106               5.098635              2.388732   \n",
       "13                 -0.008010               4.825001              2.848791   \n",
       "14                 -0.010220               4.584092              3.444145   \n",
       "15                 -0.003708               3.980454              2.234978   \n",
       "16                 -0.009218               5.341363              2.888665   \n",
       "17                 -0.006378               5.089546              3.062947   \n",
       "18                 -0.006657               4.583181              2.728268   \n",
       "19                 -0.008222               4.608635              2.871597   \n",
       "20                 -0.012239               4.026817              3.787992   \n",
       "21                 -0.001961               2.327729              3.403955   \n",
       "22                 -0.007087               3.312273              3.198349   \n",
       "23                 -0.002574               2.798637              3.212251   \n",
       "24                 -0.005161               4.841365              2.171303   \n",
       "25                 -0.005192               4.665000              1.960421   \n",
       "26                 -0.010218               3.477728              3.273762   \n",
       "27                 -0.003540               3.598639              2.472801   \n",
       "28                 -0.005420               4.509546              2.680927   \n",
       "29                 -0.005579               3.987728              2.342737   \n",
       "30                 -0.005987               3.684999              1.960920   \n",
       "31                 -0.006907               3.818635              2.357471   \n",
       "32                 -0.009617               3.691364              2.613367   \n",
       "33                  0.002025               1.879546              3.027608   \n",
       "34                 -0.000590               3.288635              1.998248   \n",
       "35                 -0.002705               4.276818              2.501344   \n",
       "36                 -0.005990               4.374091              3.125094   \n",
       "37                 -0.006332               3.555909              2.826949   \n",
       "38                 -0.008714               3.894092              2.730311   \n",
       "39                 -0.002200               2.866821              3.484799   \n",
       "40                 -0.008648               4.045911              2.764069   \n",
       "41                 -0.006846               3.979546              2.408774   \n",
       "42                 -0.004748               3.387728              2.927613   \n",
       "43                 -0.007287               3.050455              2.722697   \n",
       "44                 -0.006755               3.966817              2.968113   \n",
       "45                 -0.002064               1.428636              2.861168   \n",
       "46                 -0.005459               3.344091              2.548680   \n",
       "47                 -0.001124               3.599546              2.909947   \n",
       "\n",
       "    Test Return Mean Trend mean  Weekly Minimum Return mean  Periods  \\\n",
       "0                     -0.091807                   -1.110000       22   \n",
       "1                     -0.080672                   -0.668183       22   \n",
       "2                     -0.384647                   -0.860454       22   \n",
       "3                     -0.099215                   -1.242727       22   \n",
       "4                     -0.019497                   -0.852727       22   \n",
       "5                     -0.284285                   -0.912727       22   \n",
       "6                     -0.142874                   -0.933636       22   \n",
       "7                     -0.253343                   -1.106819       22   \n",
       "8                     -0.303992                   -0.891818       22   \n",
       "9                     -0.046940                   -1.621363       22   \n",
       "10                     0.043269                   -1.079545       22   \n",
       "11                     0.116584                   -1.298637       22   \n",
       "12                    -0.153016                   -0.415909       22   \n",
       "13                    -0.155398                   -0.515455       22   \n",
       "14                    -0.204241                   -0.621363       22   \n",
       "15                     0.008182                   -0.690000       22   \n",
       "16                    -0.151649                   -0.461819       22   \n",
       "17                    -0.033331                   -0.440454       22   \n",
       "18                    -0.041474                   -0.499546       22   \n",
       "19                    -0.099565                   -0.445000       22   \n",
       "20                    -0.233049                   -0.877273       22   \n",
       "21                     0.000864                   -1.233636       22   \n",
       "22                    -0.063924                   -1.096818       22   \n",
       "23                     0.040954                   -1.183182       22   \n",
       "24                    -0.058933                   -0.389546       22   \n",
       "25                    -0.104557                   -0.405000       22   \n",
       "26                    -0.240864                   -0.583637       22   \n",
       "27                    -0.024658                   -0.834545       22   \n",
       "28                    -0.124082                   -0.614545       22   \n",
       "29                    -0.065110                   -0.823182       22   \n",
       "30                    -0.116844                   -0.570455       22   \n",
       "31                    -0.092078                   -0.656364       22   \n",
       "32                    -0.142671                   -0.726818       22   \n",
       "33                     0.067369                   -1.602727       22   \n",
       "34                     0.036336                   -0.757727       22   \n",
       "35                     0.030700                   -0.792728       22   \n",
       "36                    -0.182931                   -0.432274       22   \n",
       "37                    -0.140784                   -0.745456       22   \n",
       "38                    -0.233614                   -0.879546       22   \n",
       "39                    -0.013377                   -0.894091       22   \n",
       "40                    -0.196381                   -0.726819       22   \n",
       "41                    -0.184195                   -0.672273       22   \n",
       "42                    -0.175850                   -0.650001       22   \n",
       "43                    -0.164026                   -0.775909       22   \n",
       "44                    -0.065076                   -0.626819       22   \n",
       "45                    -0.052270                   -1.721363       22   \n",
       "46                    -0.075183                   -0.688181       22   \n",
       "47                    -0.017510                   -0.910455       22   \n",
       "\n",
       "   Top Feature 1 Top Feature 2 Top Feature 3 Top Feature 4 Top Feature 5  \n",
       "0        STOCH_K     WilliamsR        CCI_14        CCI_20  Return_^N225  \n",
       "1        STOCH_K        CCI_20     WilliamsR        CCI_14        RSI_14  \n",
       "2        STOCH_K        CCI_20     WilliamsR        CCI_14        RSI_14  \n",
       "3        STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "4        STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "5        STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "6        STOCH_K        CCI_20  Return_^AXJO        CCI_14  Return_^AORD  \n",
       "7        STOCH_K        CCI_20        CCI_14  Return_^TWII  Return_^AORD  \n",
       "8        STOCH_K        CCI_20        CCI_14  Return_^AORD  Return_^TWII  \n",
       "9         CCI_20     WilliamsR   Return_^DJI   Return_ADBE    Return_TXN  \n",
       "10        CCI_14       STOCH_K     WilliamsR    Return_TXN        CCI_20  \n",
       "11        CCI_14  Return_^AORD       STOCH_K  Return_^AXJO     WilliamsR  \n",
       "12       STOCH_K     WilliamsR        CCI_14        CCI_20  Return_^N225  \n",
       "13       STOCH_K        CCI_20     WilliamsR        CCI_14        RSI_14  \n",
       "14       STOCH_K        CCI_20     WilliamsR        CCI_14        RSI_14  \n",
       "15       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "16       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "17       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "18       STOCH_K        CCI_20  Return_^AXJO        CCI_14  Return_^AORD  \n",
       "19       STOCH_K        CCI_20        CCI_14  Return_^TWII  Return_^AORD  \n",
       "20       STOCH_K        CCI_20        CCI_14  Return_^AORD  Return_^TWII  \n",
       "21        CCI_20     WilliamsR   Return_^DJI   Return_ADBE    Return_TXN  \n",
       "22        CCI_14       STOCH_K     WilliamsR    Return_TXN        CCI_20  \n",
       "23        CCI_14  Return_^AORD       STOCH_K  Return_^AXJO     WilliamsR  \n",
       "24       STOCH_K     WilliamsR        CCI_14        CCI_20  Return_^N225  \n",
       "25       STOCH_K        CCI_20     WilliamsR        CCI_14        RSI_14  \n",
       "26       STOCH_K        CCI_20     WilliamsR        CCI_14        RSI_14  \n",
       "27       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "28       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "29       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "30       STOCH_K        CCI_20  Return_^AXJO        CCI_14  Return_^AORD  \n",
       "31       STOCH_K        CCI_20        CCI_14  Return_^TWII  Return_^AORD  \n",
       "32       STOCH_K        CCI_20        CCI_14  Return_^AORD  Return_^TWII  \n",
       "33        CCI_20     WilliamsR   Return_^DJI   Return_ADBE    Return_TXN  \n",
       "34        CCI_14       STOCH_K     WilliamsR    Return_TXN        CCI_20  \n",
       "35        CCI_14  Return_^AORD       STOCH_K  Return_^AXJO     WilliamsR  \n",
       "36       STOCH_K     WilliamsR        CCI_14        CCI_20  Return_^N225  \n",
       "37       STOCH_K        CCI_20     WilliamsR        CCI_14        RSI_14  \n",
       "38       STOCH_K        CCI_20     WilliamsR        CCI_14        RSI_14  \n",
       "39       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "40       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "41       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "42       STOCH_K        CCI_20  Return_^AXJO        CCI_14  Return_^AORD  \n",
       "43       STOCH_K        CCI_20        CCI_14  Return_^TWII  Return_^AORD  \n",
       "44       STOCH_K        CCI_20        CCI_14  Return_^AORD  Return_^TWII  \n",
       "45        CCI_20     WilliamsR   Return_^DJI   Return_ADBE    Return_TXN  \n",
       "46        CCI_14       STOCH_K     WilliamsR    Return_TXN        CCI_20  \n",
       "47        CCI_14  Return_^AORD       STOCH_K  Return_^AXJO     WilliamsR  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_table_roll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OtDiwjYRTOpg"
   },
   "source": [
    "# EXPANDIG WINDOW CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoH0siPMR4j9",
    "outputId": "90134876-7a21-476f-ea19-813ed71f87d0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model CART, Train Acc: 0.968, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model CART, Train Acc: 0.9726027397260274, Test Acc: 0.5217391304347826\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model CART, Train Acc: 0.9763313609467456, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model CART, Train Acc: 0.9787234042553191, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model CART, Train Acc: 0.981042654028436, Test Acc: 0.38095238095238093\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model CART, Train Acc: 0.9827586206896551, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model CART, Train Acc: 0.9840637450199203, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model CART, Train Acc: 0.9853479853479854, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model CART, Train Acc: 0.9863013698630136, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model CART, Train Acc: 0.987220447284345, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model CART, Train Acc: 0.9880239520958084, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model CART, Train Acc: 0.9887640449438202, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model CART, Train Acc: 0.9893333333333333, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model CART, Train Acc: 0.9899244332493703, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model CART, Train Acc: 0.9904534606205251, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model CART, Train Acc: 0.9908883826879271, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model CART, Train Acc: 0.9913419913419913, Test Acc: 0.55\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model CART, Train Acc: 0.991701244813278, Test Acc: 0.45\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model CART, Train Acc: 0.9920318725099602, Test Acc: 0.4090909090909091\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model CART, Train Acc: 0.9923664122137404, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model CART, Train Acc: 0.992633517495396, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model CART, Train Acc: 0.9929078014184397, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model RF, Train Acc: 0.968, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model RF, Train Acc: 0.9726027397260274, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model RF, Train Acc: 0.9763313609467456, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model RF, Train Acc: 0.9787234042553191, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model RF, Train Acc: 0.981042654028436, Test Acc: 0.5238095238095238\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model RF, Train Acc: 0.9827586206896551, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model RF, Train Acc: 0.9840637450199203, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model RF, Train Acc: 0.9853479853479854, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model RF, Train Acc: 0.9863013698630136, Test Acc: 0.8095238095238095\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model RF, Train Acc: 0.987220447284345, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model RF, Train Acc: 0.9880239520958084, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model RF, Train Acc: 0.9887640449438202, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 12, VarNum 5, Method CORR, Model RF, Train Acc: 0.9893333333333333, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model RF, Train Acc: 0.9899244332493703, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model RF, Train Acc: 0.9904534606205251, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model RF, Train Acc: 0.9908883826879271, Test Acc: 0.7391304347826086\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model RF, Train Acc: 0.9913419913419913, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model RF, Train Acc: 0.991701244813278, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model RF, Train Acc: 0.9920318725099602, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model RF, Train Acc: 0.9923664122137404, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model RF, Train Acc: 0.992633517495396, Test Acc: 0.8571428571428571\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model RF, Train Acc: 0.9929078014184397, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model XGB, Train Acc: 0.968, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9726027397260274, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9763313609467456, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9787234042553191, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model XGB, Train Acc: 0.981042654028436, Test Acc: 0.38095238095238093\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9827586206896551, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9800796812749004, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9853479853479854, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9863013698630136, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model XGB, Train Acc: 0.987220447284345, Test Acc: 0.5238095238095238\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9880239520958084, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9887640449438202, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9893333333333333, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9899244332493703, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9904534606205251, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9908883826879271, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9913419913419913, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model XGB, Train Acc: 0.991701244813278, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9920318725099602, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9923664122137404, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model XGB, Train Acc: 0.992633517495396, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model XGB, Train Acc: 0.9929078014184397, Test Acc: 0.4090909090909091\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 0, VarNum 5, Method CORR, Model LDA, Train Acc: 0.64, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 1, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6164383561643836, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 2, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6390532544378699, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 3, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6436170212765957, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 4, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6303317535545023, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 5, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6336206896551724, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 6, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6334661354581673, Test Acc: 0.6818181818181818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 7, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6373626373626373, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 8, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6506849315068494, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 9, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6549520766773163, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 10, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6526946107784432, Test Acc: 0.8181818181818182\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 11, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6601123595505618, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 12, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6586666666666666, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 13, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6523929471032746, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 14, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6563245823389021, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 15, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6492027334851936, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 16, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6493506493506493, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 17, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6556016597510373, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 18, VarNum 5, Method CORR, Model LDA, Train Acc: 0.651394422310757, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 19, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6507633587786259, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 20, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6519337016574586, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21]\n",
      "Fold 21, VarNum 5, Method CORR, Model LDA, Train Acc: 0.6595744680851063, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 1, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 1, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 30  33  35 121 127]\n",
      "Fold 2, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  33  35 121 127]\n",
      "Fold 2, VarNum 5, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9946808510638298, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model CART, Train Acc: 0.995260663507109, Test Acc: 0.5714285714285714\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9956896551724138, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9960159362549801, Test Acc: 0.8636363636363636\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9963369963369964, Test Acc: 0.47368421052631576\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 127]\n",
      "Fold 8, VarNum 5, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 127]\n",
      "Fold 8, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9965753424657534, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 9, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 9, VarNum 5, Method CHI2, Model CART, Train Acc: 0.987220447284345, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9880239520958084, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9887640449438202, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 12, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 12, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9893333333333333, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9899244332493703, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9904534606205251, Test Acc: 0.55\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9908883826879271, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9913419913419913, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 17, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 17, VarNum 5, Method CHI2, Model CART, Train Acc: 0.991701244813278, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 18, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 18, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9920318725099602, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 19, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 19, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9923664122137404, Test Acc: 0.5263157894736842\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model CART, Train Acc: 0.992633517495396, Test Acc: 0.5238095238095238\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 21, VarNum 5, Method CHI2, Model CART, Selected Features: [30 31 32 33 35]\n",
      "Fold 21, VarNum 5, Method CHI2, Model CART, Train Acc: 0.9929078014184397, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 121 127 129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 1, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 1, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (ChiSquareFS): [ 30  33  35 121 127]\n",
      "Fold 2, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  33  35 121 127]\n",
      "Fold 2, VarNum 5, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.9473684210526315\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9946808510638298, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model RF, Train Acc: 0.995260663507109, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9956896551724138, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9960159362549801, Test Acc: 0.8636363636363636\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9963369963369964, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 127]\n",
      "Fold 8, VarNum 5, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 127]\n",
      "Fold 8, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9965753424657534, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 9, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 9, VarNum 5, Method CHI2, Model RF, Train Acc: 0.987220447284345, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9880239520958084, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9887640449438202, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 12, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 12, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9893333333333333, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9899244332493703, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9904534606205251, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9908883826879271, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9913419913419913, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 17, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 17, VarNum 5, Method CHI2, Model RF, Train Acc: 0.991701244813278, Test Acc: 0.55\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 18, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 18, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9920318725099602, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 19, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 19, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9923664122137404, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model RF, Train Acc: 0.992633517495396, Test Acc: 0.8095238095238095\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 21, VarNum 5, Method CHI2, Model RF, Selected Features: [30 31 32 33 35]\n",
      "Fold 21, VarNum 5, Method CHI2, Model RF, Train Acc: 0.9929078014184397, Test Acc: 0.4090909090909091\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 1, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 1, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (ChiSquareFS): [ 30  33  35 121 127]\n",
      "Fold 2, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  33  35 121 127]\n",
      "Fold 2, VarNum 5, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9946808510638298, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.995260663507109, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9956896551724138, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9960159362549801, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9963369963369964, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 127]\n",
      "Fold 8, VarNum 5, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 127]\n",
      "Fold 8, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9965753424657534, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 9, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 9, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.987220447284345, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9880239520958084, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9887640449438202, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 12, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 12, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9893333333333333, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9899244332493703, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 14, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9904534606205251, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9908883826879271, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9913419913419913, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 17, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 17, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.991701244813278, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 18, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 18, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9920318725099602, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 19, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 19, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9923664122137404, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.992633517495396, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 21, VarNum 5, Method CHI2, Model XGB, Selected Features: [30 31 32 33 35]\n",
      "Fold 21, VarNum 5, Method CHI2, Model XGB, Train Acc: 0.9929078014184397, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 0, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.704, Test Acc: 0.8095238095238095\n",
      "Selected features (ChiSquareFS): [ 30  35 121 127 129]\n",
      "Fold 1, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 121 127 129]\n",
      "Fold 1, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7191780821917808, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [ 30  33  35 121 127]\n",
      "Fold 2, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  33  35 121 127]\n",
      "Fold 2, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6923076923076923, Test Acc: 0.8947368421052632\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 3, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6808510638297872, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 4, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7014218009478673, Test Acc: 0.8095238095238095\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 5, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7155172413793104, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  35 106 107 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  35 106 107 127]\n",
      "Fold 6, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7091633466135459, Test Acc: 0.8181818181818182\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 127]\n",
      "Fold 7, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.684981684981685, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 127]\n",
      "Fold 8, VarNum 5, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 127]\n",
      "Fold 8, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6883561643835616, Test Acc: 0.5714285714285714\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 9, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 9, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7060702875399361, Test Acc: 0.5238095238095238\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 10, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7005988023952096, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 11, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.699438202247191, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 12, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 12, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7146666666666667, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 13, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7027707808564232, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 14, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7016706443914081, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 15, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7038724373576309, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 16, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7012987012987013, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 17, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 17, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7033195020746889, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 18, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 18, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.7071713147410359, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 19, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 19, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6965648854961832, Test Acc: 0.5263157894736842\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 20, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6869244935543278, Test Acc: 0.5714285714285714\n",
      "Selected features (ChiSquareFS): [30 31 32 33 35]\n",
      "Fold 21, VarNum 5, Method CHI2, Model LDA, Selected Features: [30 31 32 33 35]\n",
      "Fold 21, VarNum 5, Method CHI2, Model LDA, Train Acc: 0.6826241134751773, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [129 134 106  30 107]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387]\n",
      "Fold 0, VarNum 5, Method RF, Model CART, Selected Features: [129 134 106  30 107]\n",
      "Fold 0, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [129 134 106  30 127]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167]\n",
      "Fold 1, VarNum 5, Method RF, Model CART, Selected Features: [129 134 106  30 127]\n",
      "Fold 1, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [134 129 107  30 106]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617]\n",
      "Fold 2, VarNum 5, Method RF, Model CART, Selected Features: [134 129 107  30 106]\n",
      "Fold 2, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [129 106 107  30 134]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656]\n",
      "Fold 3, VarNum 5, Method RF, Model CART, Selected Features: [129 106 107  30 134]\n",
      "Fold 3, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "Selected features (RF): [106 107 129 134  30]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219 ]\n",
      "Fold 4, VarNum 5, Method RF, Model CART, Selected Features: [106 107 129 134  30]\n",
      "Fold 4, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [107  30 127 106 129]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077]\n",
      "Fold 5, VarNum 5, Method RF, Model CART, Selected Features: [107  30 127 106 129]\n",
      "Fold 5, VarNum 5, Method RF, Model CART, Train Acc: 0.9956896551724138, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [134 107 106 127  30]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231]\n",
      "Fold 6, VarNum 5, Method RF, Model CART, Selected Features: [134 107 106 127  30]\n",
      "Fold 6, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (RF): [107  30 134 127 106]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009]\n",
      "Fold 7, VarNum 5, Method RF, Model CART, Selected Features: [107  30 134 127 106]\n",
      "Fold 7, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463]\n",
      "Fold 8, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 8, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201]\n",
      "Fold 9, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 9, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542]\n",
      "Fold 10, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 10, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 134  33 106]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717]\n",
      "Fold 11, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 134  33 106]\n",
      "Fold 11, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106  33 134]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571]\n",
      "Fold 12, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 106  33 134]\n",
      "Fold 12, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 30 107 106  33 134]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771]\n",
      "Fold 13, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 106  33 134]\n",
      "Fold 13, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 106  33 127]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319]\n",
      "Fold 14, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 106  33 127]\n",
      "Fold 14, VarNum 5, Method RF, Model CART, Train Acc: 0.9976133651551312, Test Acc: 0.45\n",
      "Selected features (RF): [ 30 106 107  33  32]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972]\n",
      "Fold 15, VarNum 5, Method RF, Model CART, Selected Features: [ 30 106 107  33  32]\n",
      "Fold 15, VarNum 5, Method RF, Model CART, Train Acc: 0.9977220956719818, Test Acc: 0.4782608695652174\n",
      "Selected features (RF): [ 30 106  32 107  33]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613]\n",
      "Fold 16, VarNum 5, Method RF, Model CART, Selected Features: [ 30 106  32 107  33]\n",
      "Fold 16, VarNum 5, Method RF, Model CART, Train Acc: 0.9978354978354979, Test Acc: 0.45\n",
      "Selected features (RF): [ 30 107 106  33  32]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276]\n",
      "Fold 17, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 106  33  32]\n",
      "Fold 17, VarNum 5, Method RF, Model CART, Train Acc: 0.9979253112033195, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 107 134 106  32]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419]\n",
      "Fold 18, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 134 106  32]\n",
      "Fold 18, VarNum 5, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [ 30 107 106  33  32]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018]\n",
      "Fold 19, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107 106  33  32]\n",
      "Fold 19, VarNum 5, Method RF, Model CART, Train Acc: 0.9980916030534351, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [ 30 107  33  35  32]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176]\n",
      "Fold 20, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107  33  35  32]\n",
      "Fold 20, VarNum 5, Method RF, Model CART, Train Acc: 0.998158379373849, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107  32  33  35]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627]\n",
      "Fold 21, VarNum 5, Method RF, Model CART, Selected Features: [ 30 107  32  33  35]\n",
      "Fold 21, VarNum 5, Method RF, Model CART, Train Acc: 0.99822695035461, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [129 134 106  30 107]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387]\n",
      "Fold 0, VarNum 5, Method RF, Model RF, Selected Features: [129 134 106  30 107]\n",
      "Fold 0, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [129 134 106  30 127]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167]\n",
      "Fold 1, VarNum 5, Method RF, Model RF, Selected Features: [129 134 106  30 127]\n",
      "Fold 1, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [134 129 107  30 106]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617]\n",
      "Fold 2, VarNum 5, Method RF, Model RF, Selected Features: [134 129 107  30 106]\n",
      "Fold 2, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [129 106 107  30 134]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656]\n",
      "Fold 3, VarNum 5, Method RF, Model RF, Selected Features: [129 106 107  30 134]\n",
      "Fold 3, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [106 107 129 134  30]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219 ]\n",
      "Fold 4, VarNum 5, Method RF, Model RF, Selected Features: [106 107 129 134  30]\n",
      "Fold 4, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [107  30 127 106 129]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077]\n",
      "Fold 5, VarNum 5, Method RF, Model RF, Selected Features: [107  30 127 106 129]\n",
      "Fold 5, VarNum 5, Method RF, Model RF, Train Acc: 0.9956896551724138, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [134 107 106 127  30]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231]\n",
      "Fold 6, VarNum 5, Method RF, Model RF, Selected Features: [134 107 106 127  30]\n",
      "Fold 6, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.9090909090909091\n",
      "Selected features (RF): [107  30 134 127 106]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009]\n",
      "Fold 7, VarNum 5, Method RF, Model RF, Selected Features: [107  30 134 127 106]\n",
      "Fold 7, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463]\n",
      "Fold 8, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 8, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201]\n",
      "Fold 9, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 9, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542]\n",
      "Fold 10, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 10, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [ 30 107 134  33 106]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717]\n",
      "Fold 11, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 134  33 106]\n",
      "Fold 11, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [ 30 107 106  33 134]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571]\n",
      "Fold 12, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 106  33 134]\n",
      "Fold 12, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 106  33 134]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771]\n",
      "Fold 13, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 106  33 134]\n",
      "Fold 13, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [ 30 107 106  33 127]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319]\n",
      "Fold 14, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 106  33 127]\n",
      "Fold 14, VarNum 5, Method RF, Model RF, Train Acc: 0.9976133651551312, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 106 107  33  32]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972]\n",
      "Fold 15, VarNum 5, Method RF, Model RF, Selected Features: [ 30 106 107  33  32]\n",
      "Fold 15, VarNum 5, Method RF, Model RF, Train Acc: 0.9977220956719818, Test Acc: 0.5652173913043478\n",
      "Selected features (RF): [ 30 106  32 107  33]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613]\n",
      "Fold 16, VarNum 5, Method RF, Model RF, Selected Features: [ 30 106  32 107  33]\n",
      "Fold 16, VarNum 5, Method RF, Model RF, Train Acc: 0.9978354978354979, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 107 106  33  32]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276]\n",
      "Fold 17, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 106  33  32]\n",
      "Fold 17, VarNum 5, Method RF, Model RF, Train Acc: 0.9979253112033195, Test Acc: 0.7\n",
      "Selected features (RF): [ 30 107 134 106  32]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419]\n",
      "Fold 18, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 134 106  32]\n",
      "Fold 18, VarNum 5, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 30 107 106  33  32]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018]\n",
      "Fold 19, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107 106  33  32]\n",
      "Fold 19, VarNum 5, Method RF, Model RF, Train Acc: 0.9980916030534351, Test Acc: 0.3684210526315789\n",
      "Selected features (RF): [ 30 107  33  35  32]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176]\n",
      "Fold 20, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107  33  35  32]\n",
      "Fold 20, VarNum 5, Method RF, Model RF, Train Acc: 0.998158379373849, Test Acc: 0.7619047619047619\n",
      "Selected features (RF): [ 30 107  32  33  35]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627]\n",
      "Fold 21, VarNum 5, Method RF, Model RF, Selected Features: [ 30 107  32  33  35]\n",
      "Fold 21, VarNum 5, Method RF, Model RF, Train Acc: 0.99822695035461, Test Acc: 0.4090909090909091\n",
      "Selected features (RF): [129 134 106  30 107]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387]\n",
      "Fold 0, VarNum 5, Method RF, Model XGB, Selected Features: [129 134 106  30 107]\n",
      "Fold 0, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [129 134 106  30 127]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167]\n",
      "Fold 1, VarNum 5, Method RF, Model XGB, Selected Features: [129 134 106  30 127]\n",
      "Fold 1, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [134 129 107  30 106]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617]\n",
      "Fold 2, VarNum 5, Method RF, Model XGB, Selected Features: [134 129 107  30 106]\n",
      "Fold 2, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [129 106 107  30 134]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656]\n",
      "Fold 3, VarNum 5, Method RF, Model XGB, Selected Features: [129 106 107  30 134]\n",
      "Fold 3, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [106 107 129 134  30]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219 ]\n",
      "Fold 4, VarNum 5, Method RF, Model XGB, Selected Features: [106 107 129 134  30]\n",
      "Fold 4, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [107  30 127 106 129]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077]\n",
      "Fold 5, VarNum 5, Method RF, Model XGB, Selected Features: [107  30 127 106 129]\n",
      "Fold 5, VarNum 5, Method RF, Model XGB, Train Acc: 0.9956896551724138, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [134 107 106 127  30]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231]\n",
      "Fold 6, VarNum 5, Method RF, Model XGB, Selected Features: [134 107 106 127  30]\n",
      "Fold 6, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [107  30 134 127 106]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009]\n",
      "Fold 7, VarNum 5, Method RF, Model XGB, Selected Features: [107  30 134 127 106]\n",
      "Fold 7, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463]\n",
      "Fold 8, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 8, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201]\n",
      "Fold 9, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 9, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542]\n",
      "Fold 10, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 10, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30 107 134  33 106]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717]\n",
      "Fold 11, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 134  33 106]\n",
      "Fold 11, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106  33 134]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571]\n",
      "Fold 12, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 106  33 134]\n",
      "Fold 12, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 106  33 134]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771]\n",
      "Fold 13, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 106  33 134]\n",
      "Fold 13, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30 107 106  33 127]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319]\n",
      "Fold 14, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 106  33 127]\n",
      "Fold 14, VarNum 5, Method RF, Model XGB, Train Acc: 0.9976133651551312, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 106 107  33  32]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972]\n",
      "Fold 15, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 106 107  33  32]\n",
      "Fold 15, VarNum 5, Method RF, Model XGB, Train Acc: 0.9977220956719818, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [ 30 106  32 107  33]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613]\n",
      "Fold 16, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 106  32 107  33]\n",
      "Fold 16, VarNum 5, Method RF, Model XGB, Train Acc: 0.9978354978354979, Test Acc: 0.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [ 30 107 106  33  32]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276]\n",
      "Fold 17, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 106  33  32]\n",
      "Fold 17, VarNum 5, Method RF, Model XGB, Train Acc: 0.9979253112033195, Test Acc: 0.7\n",
      "Selected features (RF): [ 30 107 134 106  32]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419]\n",
      "Fold 18, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 134 106  32]\n",
      "Fold 18, VarNum 5, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30 107 106  33  32]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018]\n",
      "Fold 19, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107 106  33  32]\n",
      "Fold 19, VarNum 5, Method RF, Model XGB, Train Acc: 0.9980916030534351, Test Acc: 0.47368421052631576\n",
      "Selected features (RF): [ 30 107  33  35  32]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176]\n",
      "Fold 20, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107  33  35  32]\n",
      "Fold 20, VarNum 5, Method RF, Model XGB, Train Acc: 0.998158379373849, Test Acc: 0.8095238095238095\n",
      "Selected features (RF): [ 30 107  32  33  35]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627]\n",
      "Fold 21, VarNum 5, Method RF, Model XGB, Selected Features: [ 30 107  32  33  35]\n",
      "Fold 21, VarNum 5, Method RF, Model XGB, Train Acc: 0.99822695035461, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [129 134 106  30 107]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387]\n",
      "Fold 0, VarNum 5, Method RF, Model LDA, Selected Features: [129 134 106  30 107]\n",
      "Fold 0, VarNum 5, Method RF, Model LDA, Train Acc: 0.648, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [129 134 106  30 127]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167]\n",
      "Fold 1, VarNum 5, Method RF, Model LDA, Selected Features: [129 134 106  30 127]\n",
      "Fold 1, VarNum 5, Method RF, Model LDA, Train Acc: 0.684931506849315, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [134 129 107  30 106]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617]\n",
      "Fold 2, VarNum 5, Method RF, Model LDA, Selected Features: [134 129 107  30 106]\n",
      "Fold 2, VarNum 5, Method RF, Model LDA, Train Acc: 0.6627218934911243, Test Acc: 0.8947368421052632\n",
      "Selected features (RF): [129 106 107  30 134]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656]\n",
      "Fold 3, VarNum 5, Method RF, Model LDA, Selected Features: [129 106 107  30 134]\n",
      "Fold 3, VarNum 5, Method RF, Model LDA, Train Acc: 0.6914893617021277, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [106 107 129 134  30]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219 ]\n",
      "Fold 4, VarNum 5, Method RF, Model LDA, Selected Features: [106 107 129 134  30]\n",
      "Fold 4, VarNum 5, Method RF, Model LDA, Train Acc: 0.7061611374407583, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [107  30 127 106 129]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077]\n",
      "Fold 5, VarNum 5, Method RF, Model LDA, Selected Features: [107  30 127 106 129]\n",
      "Fold 5, VarNum 5, Method RF, Model LDA, Train Acc: 0.6767241379310345, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [134 107 106 127  30]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231]\n",
      "Fold 6, VarNum 5, Method RF, Model LDA, Selected Features: [134 107 106 127  30]\n",
      "Fold 6, VarNum 5, Method RF, Model LDA, Train Acc: 0.6892430278884463, Test Acc: 0.8181818181818182\n",
      "Selected features (RF): [107  30 134 127 106]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009]\n",
      "Fold 7, VarNum 5, Method RF, Model LDA, Selected Features: [107  30 134 127 106]\n",
      "Fold 7, VarNum 5, Method RF, Model LDA, Train Acc: 0.6923076923076923, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463]\n",
      "Fold 8, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 8, VarNum 5, Method RF, Model LDA, Train Acc: 0.7123287671232876, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201]\n",
      "Fold 9, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 9, VarNum 5, Method RF, Model LDA, Train Acc: 0.6932907348242812, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 106 134 127]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542]\n",
      "Fold 10, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127]\n",
      "Fold 10, VarNum 5, Method RF, Model LDA, Train Acc: 0.6856287425149701, Test Acc: 0.8181818181818182\n",
      "Selected features (RF): [ 30 107 134  33 106]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717]\n",
      "Fold 11, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 134  33 106]\n",
      "Fold 11, VarNum 5, Method RF, Model LDA, Train Acc: 0.6882022471910112, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [ 30 107 106  33 134]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571]\n",
      "Fold 12, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 106  33 134]\n",
      "Fold 12, VarNum 5, Method RF, Model LDA, Train Acc: 0.6826666666666666, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 106  33 134]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771]\n",
      "Fold 13, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 106  33 134]\n",
      "Fold 13, VarNum 5, Method RF, Model LDA, Train Acc: 0.6801007556675063, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30 107 106  33 127]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319]\n",
      "Fold 14, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 106  33 127]\n",
      "Fold 14, VarNum 5, Method RF, Model LDA, Train Acc: 0.6706443914081146, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 106 107  33  32]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972]\n",
      "Fold 15, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 106 107  33  32]\n",
      "Fold 15, VarNum 5, Method RF, Model LDA, Train Acc: 0.6742596810933941, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [ 30 106  32 107  33]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613]\n",
      "Fold 16, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 106  32 107  33]\n",
      "Fold 16, VarNum 5, Method RF, Model LDA, Train Acc: 0.6731601731601732, Test Acc: 0.65\n",
      "Selected features (RF): [ 30 107 106  33  32]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276]\n",
      "Fold 17, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 106  33  32]\n",
      "Fold 17, VarNum 5, Method RF, Model LDA, Train Acc: 0.6721991701244814, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 107 134 106  32]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419]\n",
      "Fold 18, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 134 106  32]\n",
      "Fold 18, VarNum 5, Method RF, Model LDA, Train Acc: 0.6832669322709163, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 30 107 106  33  32]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018]\n",
      "Fold 19, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107 106  33  32]\n",
      "Fold 19, VarNum 5, Method RF, Model LDA, Train Acc: 0.666030534351145, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107  33  35  32]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176]\n",
      "Fold 20, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107  33  35  32]\n",
      "Fold 20, VarNum 5, Method RF, Model LDA, Train Acc: 0.6721915285451197, Test Acc: 0.8095238095238095\n",
      "Selected features (RF): [ 30 107  32  33  35]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627]\n",
      "Fold 21, VarNum 5, Method RF, Model LDA, Selected Features: [ 30 107  32  33  35]\n",
      "Fold 21, VarNum 5, Method RF, Model LDA, Train Acc: 0.6737588652482269, Test Acc: 0.7272727272727273\n",
      "[15:24:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202]\n",
      "Fold 0, VarNum 5, Method XGB, Model CART, Selected Features: [ 54  92  67 105  46]\n",
      "Fold 0, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.38095238095238093\n",
      "[15:24:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [  1 114  92  79 100]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001]\n",
      "Fold 1, VarNum 5, Method XGB, Model CART, Selected Features: [  1 114  92  79 100]\n",
      "Fold 1, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:24:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364 ]\n",
      "Fold 2, VarNum 5, Method XGB, Model CART, Selected Features: [ 26  72  77  46 119]\n",
      "Fold 2, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:24:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [59 25 46  0 61]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437]\n",
      "Fold 3, VarNum 5, Method XGB, Model CART, Selected Features: [59 25 46  0 61]\n",
      "Fold 3, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "[15:24:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888]\n",
      "Fold 4, VarNum 5, Method XGB, Model CART, Selected Features: [ 91 106  11 134  25]\n",
      "Fold 4, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:24:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451]\n",
      "Fold 5, VarNum 5, Method XGB, Model CART, Selected Features: [ 74  99 134 119  45]\n",
      "Fold 5, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "[15:24:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231]\n",
      "Fold 6, VarNum 5, Method XGB, Model CART, Selected Features: [107  99  17  21  45]\n",
      "Fold 6, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.2727272727272727\n",
      "[15:24:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901]\n",
      "Fold 7, VarNum 5, Method XGB, Model CART, Selected Features: [107  99  66 130  40]\n",
      "Fold 7, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "[15:24:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053 ]\n",
      "Fold 8, VarNum 5, Method XGB, Model CART, Selected Features: [107  22  65  60  30]\n",
      "Fold 8, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:24:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297 ]\n",
      "Fold 9, VarNum 5, Method XGB, Model CART, Selected Features: [107  99   0  21  54]\n",
      "Fold 9, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:24:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609]\n",
      "Fold 10, VarNum 5, Method XGB, Model CART, Selected Features: [ 70 107  85  87 110]\n",
      "Fold 10, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:24:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574 ]\n",
      "Fold 11, VarNum 5, Method XGB, Model CART, Selected Features: [ 99  12 106  93  85]\n",
      "Fold 11, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:24:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579]\n",
      "Fold 12, VarNum 5, Method XGB, Model CART, Selected Features: [ 27 107  74  30  20]\n",
      "Fold 12, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:24:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133]\n",
      "Fold 13, VarNum 5, Method XGB, Model CART, Selected Features: [107  78  84  45  30]\n",
      "Fold 13, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:24:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958]\n",
      "Fold 14, VarNum 5, Method XGB, Model CART, Selected Features: [107 106 112  51  94]\n",
      "Fold 14, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4\n",
      "[15:24:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043]\n",
      "Fold 15, VarNum 5, Method XGB, Model CART, Selected Features: [106  93 119  30  45]\n",
      "Fold 15, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "[15:24:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031]\n",
      "Fold 16, VarNum 5, Method XGB, Model CART, Selected Features: [ 28  30 106  85  22]\n",
      "Fold 16, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.45\n",
      "[15:24:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613]\n",
      "Fold 17, VarNum 5, Method XGB, Model CART, Selected Features: [106  45  40  18  93]\n",
      "Fold 17, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.45\n",
      "[15:24:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 18  73 106 107  30]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267]\n",
      "Fold 18, VarNum 5, Method XGB, Model CART, Selected Features: [ 18  73 106 107  30]\n",
      "Fold 18, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:24:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243]\n",
      "Fold 19, VarNum 5, Method XGB, Model CART, Selected Features: [ 40  76  17  85 114]\n",
      "Fold 19, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:24:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375]\n",
      "Fold 20, VarNum 5, Method XGB, Model CART, Selected Features: [ 46  18 114  30 106]\n",
      "Fold 20, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:24:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [76 40 64 30 36]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063]\n",
      "Fold 21, VarNum 5, Method XGB, Model CART, Selected Features: [76 40 64 30 36]\n",
      "Fold 21, VarNum 5, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "[15:24:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202]\n",
      "Fold 0, VarNum 5, Method XGB, Model RF, Selected Features: [ 54  92  67 105  46]\n",
      "Fold 0, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.38095238095238093\n",
      "[15:24:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001]\n",
      "Fold 1, VarNum 5, Method XGB, Model RF, Selected Features: [  1 114  92  79 100]\n",
      "Fold 1, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "[15:24:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364 ]\n",
      "Fold 2, VarNum 5, Method XGB, Model RF, Selected Features: [ 26  72  77  46 119]\n",
      "Fold 2, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:24:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [59 25 46  0 61]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437]\n",
      "Fold 3, VarNum 5, Method XGB, Model RF, Selected Features: [59 25 46  0 61]\n",
      "Fold 3, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "[15:24:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888]\n",
      "Fold 4, VarNum 5, Method XGB, Model RF, Selected Features: [ 91 106  11 134  25]\n",
      "Fold 4, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "[15:24:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451]\n",
      "Fold 5, VarNum 5, Method XGB, Model RF, Selected Features: [ 74  99 134 119  45]\n",
      "Fold 5, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:24:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231]\n",
      "Fold 6, VarNum 5, Method XGB, Model RF, Selected Features: [107  99  17  21  45]\n",
      "Fold 6, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:24:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901]\n",
      "Fold 7, VarNum 5, Method XGB, Model RF, Selected Features: [107  99  66 130  40]\n",
      "Fold 7, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:24:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053 ]\n",
      "Fold 8, VarNum 5, Method XGB, Model RF, Selected Features: [107  22  65  60  30]\n",
      "Fold 8, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "[15:24:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297 ]\n",
      "Fold 9, VarNum 5, Method XGB, Model RF, Selected Features: [107  99   0  21  54]\n",
      "Fold 9, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "[15:24:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609]\n",
      "Fold 10, VarNum 5, Method XGB, Model RF, Selected Features: [ 70 107  85  87 110]\n",
      "Fold 10, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:24:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574 ]\n",
      "Fold 11, VarNum 5, Method XGB, Model RF, Selected Features: [ 99  12 106  93  85]\n",
      "Fold 11, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "[15:24:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579]\n",
      "Fold 12, VarNum 5, Method XGB, Model RF, Selected Features: [ 27 107  74  30  20]\n",
      "Fold 12, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:24:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [107  78  84  45  30]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133]\n",
      "Fold 13, VarNum 5, Method XGB, Model RF, Selected Features: [107  78  84  45  30]\n",
      "Fold 13, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:24:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958]\n",
      "Fold 14, VarNum 5, Method XGB, Model RF, Selected Features: [107 106 112  51  94]\n",
      "Fold 14, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.4\n",
      "[15:24:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043]\n",
      "Fold 15, VarNum 5, Method XGB, Model RF, Selected Features: [106  93 119  30  45]\n",
      "Fold 15, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "[15:24:55] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031]\n",
      "Fold 16, VarNum 5, Method XGB, Model RF, Selected Features: [ 28  30 106  85  22]\n",
      "Fold 16, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:24:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613]\n",
      "Fold 17, VarNum 5, Method XGB, Model RF, Selected Features: [106  45  40  18  93]\n",
      "Fold 17, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:24:56] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267]\n",
      "Fold 18, VarNum 5, Method XGB, Model RF, Selected Features: [ 18  73 106 107  30]\n",
      "Fold 18, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:24:57] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243]\n",
      "Fold 19, VarNum 5, Method XGB, Model RF, Selected Features: [ 40  76  17  85 114]\n",
      "Fold 19, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:24:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375]\n",
      "Fold 20, VarNum 5, Method XGB, Model RF, Selected Features: [ 46  18 114  30 106]\n",
      "Fold 20, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:24:58] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [76 40 64 30 36]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063]\n",
      "Fold 21, VarNum 5, Method XGB, Model RF, Selected Features: [76 40 64 30 36]\n",
      "Fold 21, VarNum 5, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:24:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202]\n",
      "Fold 0, VarNum 5, Method XGB, Model XGB, Selected Features: [ 54  92  67 105  46]\n",
      "Fold 0, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "[15:24:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001]\n",
      "Fold 1, VarNum 5, Method XGB, Model XGB, Selected Features: [  1 114  92  79 100]\n",
      "Fold 1, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.391304347826087\n",
      "[15:24:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364 ]\n",
      "Fold 2, VarNum 5, Method XGB, Model XGB, Selected Features: [ 26  72  77  46 119]\n",
      "Fold 2, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.3684210526315789\n",
      "[15:24:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [59 25 46  0 61]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437]\n",
      "Fold 3, VarNum 5, Method XGB, Model XGB, Selected Features: [59 25 46  0 61]\n",
      "Fold 3, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.43478260869565216\n",
      "[15:24:59] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888]\n",
      "Fold 4, VarNum 5, Method XGB, Model XGB, Selected Features: [ 91 106  11 134  25]\n",
      "Fold 4, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "[15:25:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451]\n",
      "Fold 5, VarNum 5, Method XGB, Model XGB, Selected Features: [ 74  99 134 119  45]\n",
      "Fold 5, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:25:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231]\n",
      "Fold 6, VarNum 5, Method XGB, Model XGB, Selected Features: [107  99  17  21  45]\n",
      "Fold 6, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:25:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901]\n",
      "Fold 7, VarNum 5, Method XGB, Model XGB, Selected Features: [107  99  66 130  40]\n",
      "Fold 7, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:25:00] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [107  22  65  60  30]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053 ]\n",
      "Fold 8, VarNum 5, Method XGB, Model XGB, Selected Features: [107  22  65  60  30]\n",
      "Fold 8, VarNum 5, Method XGB, Model XGB, Train Acc: 0.9965753424657534, Test Acc: 0.7142857142857143\n",
      "[15:25:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297 ]\n",
      "Fold 9, VarNum 5, Method XGB, Model XGB, Selected Features: [107  99   0  21  54]\n",
      "Fold 9, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.47619047619047616\n",
      "[15:25:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609]\n",
      "Fold 10, VarNum 5, Method XGB, Model XGB, Selected Features: [ 70 107  85  87 110]\n",
      "Fold 10, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:25:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574 ]\n",
      "Fold 11, VarNum 5, Method XGB, Model XGB, Selected Features: [ 99  12 106  93  85]\n",
      "Fold 11, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:25:01] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579]\n",
      "Fold 12, VarNum 5, Method XGB, Model XGB, Selected Features: [ 27 107  74  30  20]\n",
      "Fold 12, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:25:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133]\n",
      "Fold 13, VarNum 5, Method XGB, Model XGB, Selected Features: [107  78  84  45  30]\n",
      "Fold 13, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:25:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958]\n",
      "Fold 14, VarNum 5, Method XGB, Model XGB, Selected Features: [107 106 112  51  94]\n",
      "Fold 14, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.4\n",
      "[15:25:02] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043]\n",
      "Fold 15, VarNum 5, Method XGB, Model XGB, Selected Features: [106  93 119  30  45]\n",
      "Fold 15, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:25:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031]\n",
      "Fold 16, VarNum 5, Method XGB, Model XGB, Selected Features: [ 28  30 106  85  22]\n",
      "Fold 16, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:25:03] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613]\n",
      "Fold 17, VarNum 5, Method XGB, Model XGB, Selected Features: [106  45  40  18  93]\n",
      "Fold 17, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "[15:25:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267]\n",
      "Fold 18, VarNum 5, Method XGB, Model XGB, Selected Features: [ 18  73 106 107  30]\n",
      "Fold 18, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:25:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243]\n",
      "Fold 19, VarNum 5, Method XGB, Model XGB, Selected Features: [ 40  76  17  85 114]\n",
      "Fold 19, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "[15:25:04] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375]\n",
      "Fold 20, VarNum 5, Method XGB, Model XGB, Selected Features: [ 46  18 114  30 106]\n",
      "Fold 20, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:25:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [76 40 64 30 36]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063]\n",
      "Fold 21, VarNum 5, Method XGB, Model XGB, Selected Features: [76 40 64 30 36]\n",
      "Fold 21, VarNum 5, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:25:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202]\n",
      "Fold 0, VarNum 5, Method XGB, Model LDA, Selected Features: [ 54  92  67 105  46]\n",
      "Fold 0, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6, Test Acc: 0.38095238095238093\n",
      "[15:25:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001]\n",
      "Fold 1, VarNum 5, Method XGB, Model LDA, Selected Features: [  1 114  92  79 100]\n",
      "Fold 1, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6232876712328768, Test Acc: 0.5217391304347826\n",
      "[15:25:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364 ]\n",
      "Fold 2, VarNum 5, Method XGB, Model LDA, Selected Features: [ 26  72  77  46 119]\n",
      "Fold 2, VarNum 5, Method XGB, Model LDA, Train Acc: 0.5562130177514792, Test Acc: 0.631578947368421\n",
      "[15:25:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [59 25 46  0 61]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437]\n",
      "Fold 3, VarNum 5, Method XGB, Model LDA, Selected Features: [59 25 46  0 61]\n",
      "Fold 3, VarNum 5, Method XGB, Model LDA, Train Acc: 0.5585106382978723, Test Acc: 0.5652173913043478\n",
      "[15:25:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 91 106  11 134  25]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888]\n",
      "Fold 4, VarNum 5, Method XGB, Model LDA, Selected Features: [ 91 106  11 134  25]\n",
      "Fold 4, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6635071090047393, Test Acc: 0.6190476190476191\n",
      "[15:25:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451]\n",
      "Fold 5, VarNum 5, Method XGB, Model LDA, Selected Features: [ 74  99 134 119  45]\n",
      "Fold 5, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6853448275862069, Test Acc: 0.6842105263157895\n",
      "[15:25:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231]\n",
      "Fold 6, VarNum 5, Method XGB, Model LDA, Selected Features: [107  99  17  21  45]\n",
      "Fold 6, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6852589641434262, Test Acc: 0.36363636363636365\n",
      "[15:25:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901]\n",
      "Fold 7, VarNum 5, Method XGB, Model LDA, Selected Features: [107  99  66 130  40]\n",
      "Fold 7, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6703296703296703, Test Acc: 0.5789473684210527\n",
      "[15:25:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053 ]\n",
      "Fold 8, VarNum 5, Method XGB, Model LDA, Selected Features: [107  22  65  60  30]\n",
      "Fold 8, VarNum 5, Method XGB, Model LDA, Train Acc: 0.7054794520547946, Test Acc: 0.7142857142857143\n",
      "[15:25:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297 ]\n",
      "Fold 9, VarNum 5, Method XGB, Model LDA, Selected Features: [107  99   0  21  54]\n",
      "Fold 9, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6869009584664537, Test Acc: 0.6666666666666666\n",
      "[15:25:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609]\n",
      "Fold 10, VarNum 5, Method XGB, Model LDA, Selected Features: [ 70 107  85  87 110]\n",
      "Fold 10, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6616766467065869, Test Acc: 0.5\n",
      "[15:25:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574 ]\n",
      "Fold 11, VarNum 5, Method XGB, Model LDA, Selected Features: [ 99  12 106  93  85]\n",
      "Fold 11, VarNum 5, Method XGB, Model LDA, Train Acc: 0.648876404494382, Test Acc: 0.5789473684210527\n",
      "[15:25:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579]\n",
      "Fold 12, VarNum 5, Method XGB, Model LDA, Selected Features: [ 27 107  74  30  20]\n",
      "Fold 12, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6986666666666667, Test Acc: 0.5909090909090909\n",
      "[15:25:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133]\n",
      "Fold 13, VarNum 5, Method XGB, Model LDA, Selected Features: [107  78  84  45  30]\n",
      "Fold 13, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6926952141057935, Test Acc: 0.6818181818181818\n",
      "[15:25:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958]\n",
      "Fold 14, VarNum 5, Method XGB, Model LDA, Selected Features: [107 106 112  51  94]\n",
      "Fold 14, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6348448687350835, Test Acc: 0.45\n",
      "[15:25:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043]\n",
      "Fold 15, VarNum 5, Method XGB, Model LDA, Selected Features: [106  93 119  30  45]\n",
      "Fold 15, VarNum 5, Method XGB, Model LDA, Train Acc: 0.7061503416856492, Test Acc: 0.7391304347826086\n",
      "[15:25:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031]\n",
      "Fold 16, VarNum 5, Method XGB, Model LDA, Selected Features: [ 28  30 106  85  22]\n",
      "Fold 16, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6948051948051948, Test Acc: 0.7\n",
      "[15:25:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613]\n",
      "Fold 17, VarNum 5, Method XGB, Model LDA, Selected Features: [106  45  40  18  93]\n",
      "Fold 17, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6493775933609959, Test Acc: 0.6\n",
      "[15:25:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267]\n",
      "Fold 18, VarNum 5, Method XGB, Model LDA, Selected Features: [ 18  73 106 107  30]\n",
      "Fold 18, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6872509960159362, Test Acc: 0.5909090909090909\n",
      "[15:25:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243]\n",
      "Fold 19, VarNum 5, Method XGB, Model LDA, Selected Features: [ 40  76  17  85 114]\n",
      "Fold 19, VarNum 5, Method XGB, Model LDA, Train Acc: 0.583969465648855, Test Acc: 0.42105263157894735\n",
      "[15:25:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375]\n",
      "Fold 20, VarNum 5, Method XGB, Model LDA, Selected Features: [ 46  18 114  30 106]\n",
      "Fold 20, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6795580110497238, Test Acc: 0.7619047619047619\n",
      "[15:25:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [76 40 64 30 36]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063]\n",
      "Fold 21, VarNum 5, Method XGB, Model LDA, Selected Features: [76 40 64 30 36]\n",
      "Fold 21, VarNum 5, Method XGB, Model LDA, Train Acc: 0.6595744680851063, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model CART, Train Acc: 0.992, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model CART, Train Acc: 0.9931506849315068, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model CART, Train Acc: 0.9940828402366864, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model CART, Train Acc: 0.9946808510638298, Test Acc: 0.5217391304347826\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model CART, Train Acc: 0.995260663507109, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model CART, Train Acc: 0.9956896551724138, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model CART, Train Acc: 0.9960159362549801, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model CART, Train Acc: 0.9963369963369964, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model CART, Train Acc: 0.9965753424657534, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model CART, Train Acc: 0.9968051118210862, Test Acc: 0.5238095238095238\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model CART, Train Acc: 0.9970059880239521, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model CART, Train Acc: 0.9971910112359551, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model CART, Train Acc: 0.9973333333333333, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model CART, Train Acc: 0.9974811083123426, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model CART, Train Acc: 0.9976133651551312, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model CART, Train Acc: 0.9977220956719818, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model CART, Train Acc: 0.9978354978354979, Test Acc: 0.4\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model CART, Train Acc: 0.9979253112033195, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model CART, Train Acc: 0.99800796812749, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model CART, Train Acc: 0.9980916030534351, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model CART, Train Acc: 0.998158379373849, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model CART, Train Acc: 0.99822695035461, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model RF, Train Acc: 0.992, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model RF, Train Acc: 0.9931506849315068, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model RF, Train Acc: 0.9940828402366864, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model RF, Train Acc: 0.9946808510638298, Test Acc: 0.782608695652174\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model RF, Train Acc: 0.995260663507109, Test Acc: 0.8095238095238095\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5, VarNum 10, Method CORR, Model RF, Train Acc: 0.9956896551724138, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model RF, Train Acc: 0.9960159362549801, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model RF, Train Acc: 0.9963369963369964, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model RF, Train Acc: 0.9965753424657534, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model RF, Train Acc: 0.9968051118210862, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model RF, Train Acc: 0.9970059880239521, Test Acc: 0.8181818181818182\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model RF, Train Acc: 0.9971910112359551, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model RF, Train Acc: 0.9973333333333333, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model RF, Train Acc: 0.9974811083123426, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model RF, Train Acc: 0.9976133651551312, Test Acc: 0.55\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model RF, Train Acc: 0.9977220956719818, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model RF, Train Acc: 0.9978354978354979, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model RF, Train Acc: 0.9979253112033195, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model RF, Train Acc: 0.99800796812749, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model RF, Train Acc: 0.9980916030534351, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model RF, Train Acc: 0.998158379373849, Test Acc: 0.8095238095238095\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model RF, Train Acc: 0.99822695035461, Test Acc: 0.4090909090909091\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model XGB, Train Acc: 0.992, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9931506849315068, Test Acc: 0.5652173913043478\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9940828402366864, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9946808510638298, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model XGB, Train Acc: 0.995260663507109, Test Acc: 0.8095238095238095\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9956896551724138, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9960159362549801, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9963369963369964, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9965753424657534, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9968051118210862, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9970059880239521, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9971910112359551, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9973333333333333, Test Acc: 0.6363636363636364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9974811083123426, Test Acc: 0.4090909090909091\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9976133651551312, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9977220956719818, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9978354978354979, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9979253112033195, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model XGB, Train Acc: 0.99800796812749, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model XGB, Train Acc: 0.9980916030534351, Test Acc: 0.3684210526315789\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model XGB, Train Acc: 0.998158379373849, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model XGB, Train Acc: 0.99822695035461, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 0, VarNum 10, Method CORR, Model LDA, Train Acc: 0.736, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 1, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7397260273972602, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 2, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7100591715976331, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 3, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7180851063829787, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 4, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7251184834123223, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 5, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7198275862068966, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 6, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7131474103585658, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 7, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7326007326007326, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 8, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7363013698630136, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 9, VarNum 10, Method CORR, Model LDA, Train Acc: 0.731629392971246, Test Acc: 0.5238095238095238\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 10, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7155688622754491, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 11, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7162921348314607, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 12, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7093333333333334, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 13, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7052896725440806, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 14, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7040572792362768, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 15, VarNum 10, Method CORR, Model LDA, Train Acc: 0.6993166287015945, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 16, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7056277056277056, Test Acc: 0.75\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 17, VarNum 10, Method CORR, Model LDA, Train Acc: 0.7033195020746889, Test Acc: 0.75\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 18, VarNum 10, Method CORR, Model LDA, Train Acc: 0.6992031872509961, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 19, VarNum 10, Method CORR, Model LDA, Train Acc: 0.6927480916030534, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 20, VarNum 10, Method CORR, Model LDA, Train Acc: 0.6869244935543278, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127]\n",
      "Fold 21, VarNum 10, Method CORR, Model LDA, Train Acc: 0.6932624113475178, Test Acc: 0.6818181818181818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (ChiSquareFS): [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 2, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 2, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  31  33  35 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  31  33  35 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 4, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 4, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 5, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 5, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 123 127 129 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model CART, Selected Features: [ 30  32  33  35 106 107 123 127 129 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 106 107 127 134]\n",
      "Fold 7, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35 106 107 127 134]\n",
      "Fold 7, VarNum 10, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  39 106 107 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  39 106 107 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9965753424657534, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 9, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 9, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9968051118210862, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 10, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 10, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9970059880239521, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 11, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 11, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9971910112359551, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [21 25 30 31 32 33 35 37 38 39]\n",
      "Fold 12, VarNum 10, Method CHI2, Model CART, Selected Features: [21 25 30 31 32 33 35 37 38 39]\n",
      "Fold 12, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9893333333333333, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 13, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 13, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9974811083123426, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 14, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 14, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9976133651551312, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 15, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 15, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9908883826879271, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 16, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 16, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9978354978354979, Test Acc: 0.6\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 17, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 17, VarNum 10, Method CHI2, Model CART, Train Acc: 0.991701244813278, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 18, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 18, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9920318725099602, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9980916030534351, Test Acc: 0.47368421052631576\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 20, VarNum 10, Method CHI2, Model CART, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 20, VarNum 10, Method CHI2, Model CART, Train Acc: 0.998158379373849, Test Acc: 0.5714285714285714\n",
      "Selected features (ChiSquareFS): [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 21, VarNum 10, Method CHI2, Model CART, Selected Features: [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 21, VarNum 10, Method CHI2, Model CART, Train Acc: 0.9929078014184397, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 2, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 2, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [ 30  31  33  35 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  31  33  35 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 4, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 4, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 5, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 5, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 123 127 129 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model RF, Selected Features: [ 30  32  33  35 106 107 123 127 129 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.8181818181818182\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 106 107 127 134]\n",
      "Fold 7, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35 106 107 127 134]\n",
      "Fold 7, VarNum 10, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  39 106 107 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  39 106 107 127]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 8, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9965753424657534, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 9, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 9, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9968051118210862, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 10, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 10, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9970059880239521, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 11, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 11, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9971910112359551, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [21 25 30 31 32 33 35 37 38 39]\n",
      "Fold 12, VarNum 10, Method CHI2, Model RF, Selected Features: [21 25 30 31 32 33 35 37 38 39]\n",
      "Fold 12, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9893333333333333, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 13, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 13, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9974811083123426, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 14, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 14, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9976133651551312, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 15, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 15, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9908883826879271, Test Acc: 0.782608695652174\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 16, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 16, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9978354978354979, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 17, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 17, VarNum 10, Method CHI2, Model RF, Train Acc: 0.991701244813278, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 18, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 18, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9920318725099602, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9980916030534351, Test Acc: 0.42105263157894735\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 20, VarNum 10, Method CHI2, Model RF, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 20, VarNum 10, Method CHI2, Model RF, Train Acc: 0.998158379373849, Test Acc: 0.8571428571428571\n",
      "Selected features (ChiSquareFS): [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 21, VarNum 10, Method CHI2, Model RF, Selected Features: [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 21, VarNum 10, Method CHI2, Model RF, Train Acc: 0.9929078014184397, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 2, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 2, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.8947368421052632\n",
      "Selected features (ChiSquareFS): [ 30  31  33  35 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  31  33  35 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 4, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 4, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 5, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 5, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 123 127 129 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 30  32  33  35 106 107 123 127 129 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 106 107 127 134]\n",
      "Fold 7, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35 106 107 127 134]\n",
      "Fold 7, VarNum 10, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  39 106 107 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  39 106 107 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9965753424657534, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 9, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 9, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9968051118210862, Test Acc: 0.5238095238095238\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 10, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 10, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9970059880239521, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 11, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 11, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9971910112359551, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [21 25 30 31 32 33 35 37 38 39]\n",
      "Fold 12, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 25 30 31 32 33 35 37 38 39]\n",
      "Fold 12, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9893333333333333, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 13, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 13, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9974811083123426, Test Acc: 0.45454545454545453\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 14, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 14, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9976133651551312, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 15, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 15, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9908883826879271, Test Acc: 0.5217391304347826\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 16, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 16, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9978354978354979, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 17, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 25 30 31 32 33 35 38 39]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 17, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.991701244813278, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 18, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 18, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9920318725099602, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9980916030534351, Test Acc: 0.42105263157894735\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 20, VarNum 10, Method CHI2, Model XGB, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 20, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.998158379373849, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 21, VarNum 10, Method CHI2, Model XGB, Selected Features: [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 21, VarNum 10, Method CHI2, Model XGB, Train Acc: 0.9929078014184397, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  33  35  55 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.696, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 1, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.726027397260274, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 2, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 2, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7159763313609467, Test Acc: 0.9473684210526315\n",
      "Selected features (ChiSquareFS): [ 30  31  33  35 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  31  33  35 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7287234042553191, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 4, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 4, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7251184834123223, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 5, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 106 107 121 127 129 134]\n",
      "Fold 5, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7155172413793104, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 30  32  33  35 106 107 123 127 129 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 30  32  33  35 106 107 123 127 129 134]\n",
      "Fold 6, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7091633466135459, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35 106 107 127 134]\n",
      "Fold 7, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35 106 107 127 134]\n",
      "Fold 7, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7289377289377289, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  39 106 107 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  39 106 107 127]\n",
      "Fold 8, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7431506849315068, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 9, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 9, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7252396166134185, Test Acc: 0.47619047619047616\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 10, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 10, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7095808383233533, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 11, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  25  30  31  32  33  35  37  39 127]\n",
      "Fold 11, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7162921348314607, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [21 25 30 31 32 33 35 37 38 39]\n",
      "Fold 12, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 25 30 31 32 33 35 37 38 39]\n",
      "Fold 12, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.72, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 13, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 13, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7178841309823678, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 14, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  22  30  31  32  33  35  37  39 127]\n",
      "Fold 14, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7231503579952268, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 15, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 15, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7175398633257403, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 16, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 16, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.696969696969697, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 17, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 17, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7074688796680498, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 18, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 25 30 31 32 33 35 38 39]\n",
      "Fold 18, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.7071713147410359, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 19, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.6965648854961832, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 20, VarNum 10, Method CHI2, Model LDA, Selected Features: [ 21  22  30  31  32  33  35  38  39 127]\n",
      "Fold 20, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.6961325966850829, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 21, VarNum 10, Method CHI2, Model LDA, Selected Features: [21 22 30 31 32 33 35 37 38 39]\n",
      "Fold 21, VarNum 10, Method CHI2, Model LDA, Train Acc: 0.700354609929078, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252]\n",
      "Fold 0, VarNum 10, Method RF, Model CART, Selected Features: [129 134 106  30 107 105  43 123 100  96]\n",
      "Fold 0, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.38095238095238093\n",
      "Selected features (RF): [129 134 106  30 127 107 131  97  87 123]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167 0.02039644\n",
      " 0.01602532 0.01587018 0.01557886 0.01551213]\n",
      "Fold 1, VarNum 10, Method RF, Model CART, Selected Features: [129 134 106  30 127 107 131  97  87 123]\n",
      "Fold 1, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [134 129 107  30 106 127 123 100 102 105]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617 0.022985\n",
      " 0.01814312 0.01668106 0.01635147 0.0157809 ]\n",
      "Fold 2, VarNum 10, Method RF, Model CART, Selected Features: [134 129 107  30 106 127 123 100 102 105]\n",
      "Fold 2, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [129 106 107  30 134 127 123 109 105  32]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656 0.02107612\n",
      " 0.02026495 0.01998218 0.01919863 0.01749485]\n",
      "Fold 3, VarNum 10, Method RF, Model CART, Selected Features: [129 106 107  30 134 127 123 109 105  32]\n",
      "Fold 3, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [106 107 129 134  30 127  32 122 123 131]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219  0.02424004\n",
      " 0.01996565 0.01907085 0.01639677 0.01511131]\n",
      "Fold 4, VarNum 10, Method RF, Model CART, Selected Features: [106 107 129 134  30 127  32 122 123 131]\n",
      "Fold 4, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [107  30 127 106 129 134 123 109 120 122]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077 0.02806966\n",
      " 0.02059141 0.01501137 0.01421506 0.01419714]\n",
      "Fold 5, VarNum 10, Method RF, Model CART, Selected Features: [107  30 127 106 129 134 123 109 120 122]\n",
      "Fold 5, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "Selected features (RF): [134 107 106 127  30 123 129 109 118 122]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231 0.02580135\n",
      " 0.02334401 0.01569307 0.01390511 0.01283023]\n",
      "Fold 6, VarNum 10, Method RF, Model CART, Selected Features: [134 107 106 127  30 123 129 109 118 122]\n",
      "Fold 6, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [107  30 134 127 106 109 123 129 131 133]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009 0.01869848\n",
      " 0.01837955 0.0174225  0.01470666 0.01373099]\n",
      "Fold 7, VarNum 10, Method RF, Model CART, Selected Features: [107  30 134 127 106 109 123 129 131 133]\n",
      "Fold 7, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106 134 127 129  32 109  33 123]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463 0.01950434\n",
      " 0.01929563 0.01788344 0.01579003 0.01488815]\n",
      "Fold 8, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 106 134 127 129  32 109  33 123]\n",
      "Fold 8, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107 106 134 127  33 129  32  35 123]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201 0.01766004\n",
      " 0.0159082  0.01583366 0.01568882 0.01516106]\n",
      "Fold 9, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 106 134 127  33 129  32  35 123]\n",
      "Fold 9, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [ 30 107 106 134 127  33  35  32 123 129]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542 0.01950735\n",
      " 0.01673918 0.01632669 0.01592091 0.01496493]\n",
      "Fold 10, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 106 134 127  33  35  32 123 129]\n",
      "Fold 10, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30 107 134  33 106 127  35  32 129  31]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717 0.01998843\n",
      " 0.01947537 0.01675318 0.01650051 0.01521515]\n",
      "Fold 11, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 134  33 106 127  35  32 129  31]\n",
      "Fold 11, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35 123  31]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571 0.01720357\n",
      " 0.01623954 0.01578945 0.01415767 0.01344477]\n",
      "Fold 12, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 106  33 134 127  32  35 123  31]\n",
      "Fold 12, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35  21 123]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771 0.01754468\n",
      " 0.01747435 0.01446083 0.01321363 0.01310117]\n",
      "Fold 13, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 106  33 134 127  32  35  21 123]\n",
      "Fold 13, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [ 30 107 106  33 127  32 134  35  22 123]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319 0.01842883\n",
      " 0.01823597 0.01525073 0.01457077 0.01385602]\n",
      "Fold 14, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 106  33 127  32 134  35  22 123]\n",
      "Fold 14, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [ 30 106 107  33  32 134  35 105 129 127]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972 0.01836004\n",
      " 0.01680213 0.01447181 0.01430088 0.0139538 ]\n",
      "Fold 15, VarNum 10, Method RF, Model CART, Selected Features: [ 30 106 107  33  32 134  35 105 129 127]\n",
      "Fold 15, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [ 30 106  32 107  33  35 127 134 105 122]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613 0.01873923\n",
      " 0.01618912 0.01571743 0.01457547 0.01430517]\n",
      "Fold 16, VarNum 10, Method RF, Model CART, Selected Features: [ 30 106  32 107  33  35 127 134 105 122]\n",
      "Fold 16, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [ 30 107 106  33  32 134  35 109 127  21]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276 0.01956944\n",
      " 0.01569444 0.01428796 0.01372584 0.01366784]\n",
      "Fold 17, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 106  33  32 134  35 109 127  21]\n",
      "Fold 17, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "Selected features (RF): [ 30 107 134 106  32  33  21  35 127 123]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419 0.02063558\n",
      " 0.01696546 0.01649607 0.01265368 0.01217766]\n",
      "Fold 18, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 134 106  32  33  21  35 127 123]\n",
      "Fold 18, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 107 106  33  32  35  21 127 134 109]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018 0.01912931\n",
      " 0.01576826 0.01519539 0.01477722 0.0138591 ]\n",
      "Fold 19, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107 106  33  32  35  21 127 134 109]\n",
      "Fold 19, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.2631578947368421\n",
      "Selected features (RF): [ 30 107  33  35  32 106  21 127 123 134]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176 0.01980564\n",
      " 0.01624184 0.01587182 0.0137815  0.01343143]\n",
      "Fold 20, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107  33  35  32 106  21 127 123 134]\n",
      "Fold 20, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30 107  32  33  35 106  21 109 127 134]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627 0.01809367\n",
      " 0.01624739 0.01549509 0.0149973  0.01417063]\n",
      "Fold 21, VarNum 10, Method RF, Model CART, Selected Features: [ 30 107  32  33  35 106  21 109 127 134]\n",
      "Fold 21, VarNum 10, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252]\n",
      "Fold 0, VarNum 10, Method RF, Model RF, Selected Features: [129 134 106  30 107 105  43 123 100  96]\n",
      "Fold 0, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [129 134 106  30 127 107 131  97  87 123]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167 0.02039644\n",
      " 0.01602532 0.01587018 0.01557886 0.01551213]\n",
      "Fold 1, VarNum 10, Method RF, Model RF, Selected Features: [129 134 106  30 127 107 131  97  87 123]\n",
      "Fold 1, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [134 129 107  30 106 127 123 100 102 105]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617 0.022985\n",
      " 0.01814312 0.01668106 0.01635147 0.0157809 ]\n",
      "Fold 2, VarNum 10, Method RF, Model RF, Selected Features: [134 129 107  30 106 127 123 100 102 105]\n",
      "Fold 2, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.8421052631578947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [129 106 107  30 134 127 123 109 105  32]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656 0.02107612\n",
      " 0.02026495 0.01998218 0.01919863 0.01749485]\n",
      "Fold 3, VarNum 10, Method RF, Model RF, Selected Features: [129 106 107  30 134 127 123 109 105  32]\n",
      "Fold 3, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [106 107 129 134  30 127  32 122 123 131]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219  0.02424004\n",
      " 0.01996565 0.01907085 0.01639677 0.01511131]\n",
      "Fold 4, VarNum 10, Method RF, Model RF, Selected Features: [106 107 129 134  30 127  32 122 123 131]\n",
      "Fold 4, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [107  30 127 106 129 134 123 109 120 122]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077 0.02806966\n",
      " 0.02059141 0.01501137 0.01421506 0.01419714]\n",
      "Fold 5, VarNum 10, Method RF, Model RF, Selected Features: [107  30 127 106 129 134 123 109 120 122]\n",
      "Fold 5, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [134 107 106 127  30 123 129 109 118 122]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231 0.02580135\n",
      " 0.02334401 0.01569307 0.01390511 0.01283023]\n",
      "Fold 6, VarNum 10, Method RF, Model RF, Selected Features: [134 107 106 127  30 123 129 109 118 122]\n",
      "Fold 6, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [107  30 134 127 106 109 123 129 131 133]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009 0.01869848\n",
      " 0.01837955 0.0174225  0.01470666 0.01373099]\n",
      "Fold 7, VarNum 10, Method RF, Model RF, Selected Features: [107  30 134 127 106 109 123 129 131 133]\n",
      "Fold 7, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106 134 127 129  32 109  33 123]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463 0.01950434\n",
      " 0.01929563 0.01788344 0.01579003 0.01488815]\n",
      "Fold 8, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 106 134 127 129  32 109  33 123]\n",
      "Fold 8, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [ 30 107 106 134 127  33 129  32  35 123]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201 0.01766004\n",
      " 0.0159082  0.01583366 0.01568882 0.01516106]\n",
      "Fold 9, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 106 134 127  33 129  32  35 123]\n",
      "Fold 9, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 106 134 127  33  35  32 123 129]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542 0.01950735\n",
      " 0.01673918 0.01632669 0.01592091 0.01496493]\n",
      "Fold 10, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 106 134 127  33  35  32 123 129]\n",
      "Fold 10, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 134  33 106 127  35  32 129  31]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717 0.01998843\n",
      " 0.01947537 0.01675318 0.01650051 0.01521515]\n",
      "Fold 11, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 134  33 106 127  35  32 129  31]\n",
      "Fold 11, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35 123  31]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571 0.01720357\n",
      " 0.01623954 0.01578945 0.01415767 0.01344477]\n",
      "Fold 12, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 106  33 134 127  32  35 123  31]\n",
      "Fold 12, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35  21 123]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771 0.01754468\n",
      " 0.01747435 0.01446083 0.01321363 0.01310117]\n",
      "Fold 13, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 106  33 134 127  32  35  21 123]\n",
      "Fold 13, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 30 107 106  33 127  32 134  35  22 123]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319 0.01842883\n",
      " 0.01823597 0.01525073 0.01457077 0.01385602]\n",
      "Fold 14, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 106  33 127  32 134  35  22 123]\n",
      "Fold 14, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (RF): [ 30 106 107  33  32 134  35 105 129 127]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972 0.01836004\n",
      " 0.01680213 0.01447181 0.01430088 0.0139538 ]\n",
      "Fold 15, VarNum 10, Method RF, Model RF, Selected Features: [ 30 106 107  33  32 134  35 105 129 127]\n",
      "Fold 15, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [ 30 106  32 107  33  35 127 134 105 122]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613 0.01873923\n",
      " 0.01618912 0.01571743 0.01457547 0.01430517]\n",
      "Fold 16, VarNum 10, Method RF, Model RF, Selected Features: [ 30 106  32 107  33  35 127 134 105 122]\n",
      "Fold 16, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (RF): [ 30 107 106  33  32 134  35 109 127  21]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276 0.01956944\n",
      " 0.01569444 0.01428796 0.01372584 0.01366784]\n",
      "Fold 17, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 106  33  32 134  35 109 127  21]\n",
      "Fold 17, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 107 134 106  32  33  21  35 127 123]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419 0.02063558\n",
      " 0.01696546 0.01649607 0.01265368 0.01217766]\n",
      "Fold 18, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 134 106  32  33  21  35 127 123]\n",
      "Fold 18, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 107 106  33  32  35  21 127 134 109]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018 0.01912931\n",
      " 0.01576826 0.01519539 0.01477722 0.0138591 ]\n",
      "Fold 19, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107 106  33  32  35  21 127 134 109]\n",
      "Fold 19, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.3684210526315789\n",
      "Selected features (RF): [ 30 107  33  35  32 106  21 127 123 134]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176 0.01980564\n",
      " 0.01624184 0.01587182 0.0137815  0.01343143]\n",
      "Fold 20, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107  33  35  32 106  21 127 123 134]\n",
      "Fold 20, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107  32  33  35 106  21 109 127 134]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627 0.01809367\n",
      " 0.01624739 0.01549509 0.0149973  0.01417063]\n",
      "Fold 21, VarNum 10, Method RF, Model RF, Selected Features: [ 30 107  32  33  35 106  21 109 127 134]\n",
      "Fold 21, VarNum 10, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252]\n",
      "Fold 0, VarNum 10, Method RF, Model XGB, Selected Features: [129 134 106  30 107 105  43 123 100  96]\n",
      "Fold 0, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [129 134 106  30 127 107 131  97  87 123]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167 0.02039644\n",
      " 0.01602532 0.01587018 0.01557886 0.01551213]\n",
      "Fold 1, VarNum 10, Method RF, Model XGB, Selected Features: [129 134 106  30 127 107 131  97  87 123]\n",
      "Fold 1, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [134 129 107  30 106 127 123 100 102 105]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617 0.022985\n",
      " 0.01814312 0.01668106 0.01635147 0.0157809 ]\n",
      "Fold 2, VarNum 10, Method RF, Model XGB, Selected Features: [134 129 107  30 106 127 123 100 102 105]\n",
      "Fold 2, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.8421052631578947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [129 106 107  30 134 127 123 109 105  32]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656 0.02107612\n",
      " 0.02026495 0.01998218 0.01919863 0.01749485]\n",
      "Fold 3, VarNum 10, Method RF, Model XGB, Selected Features: [129 106 107  30 134 127 123 109 105  32]\n",
      "Fold 3, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [106 107 129 134  30 127  32 122 123 131]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219  0.02424004\n",
      " 0.01996565 0.01907085 0.01639677 0.01511131]\n",
      "Fold 4, VarNum 10, Method RF, Model XGB, Selected Features: [106 107 129 134  30 127  32 122 123 131]\n",
      "Fold 4, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (RF): [107  30 127 106 129 134 123 109 120 122]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077 0.02806966\n",
      " 0.02059141 0.01501137 0.01421506 0.01419714]\n",
      "Fold 5, VarNum 10, Method RF, Model XGB, Selected Features: [107  30 127 106 129 134 123 109 120 122]\n",
      "Fold 5, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [134 107 106 127  30 123 129 109 118 122]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231 0.02580135\n",
      " 0.02334401 0.01569307 0.01390511 0.01283023]\n",
      "Fold 6, VarNum 10, Method RF, Model XGB, Selected Features: [134 107 106 127  30 123 129 109 118 122]\n",
      "Fold 6, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [107  30 134 127 106 109 123 129 131 133]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009 0.01869848\n",
      " 0.01837955 0.0174225  0.01470666 0.01373099]\n",
      "Fold 7, VarNum 10, Method RF, Model XGB, Selected Features: [107  30 134 127 106 109 123 129 131 133]\n",
      "Fold 7, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [ 30 107 106 134 127 129  32 109  33 123]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463 0.01950434\n",
      " 0.01929563 0.01788344 0.01579003 0.01488815]\n",
      "Fold 8, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127 129  32 109  33 123]\n",
      "Fold 8, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107 106 134 127  33 129  32  35 123]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201 0.01766004\n",
      " 0.0159082  0.01583366 0.01568882 0.01516106]\n",
      "Fold 9, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127  33 129  32  35 123]\n",
      "Fold 9, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [ 30 107 106 134 127  33  35  32 123 129]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542 0.01950735\n",
      " 0.01673918 0.01632669 0.01592091 0.01496493]\n",
      "Fold 10, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127  33  35  32 123 129]\n",
      "Fold 10, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 134  33 106 127  35  32 129  31]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717 0.01998843\n",
      " 0.01947537 0.01675318 0.01650051 0.01521515]\n",
      "Fold 11, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 134  33 106 127  35  32 129  31]\n",
      "Fold 11, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35 123  31]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571 0.01720357\n",
      " 0.01623954 0.01578945 0.01415767 0.01344477]\n",
      "Fold 12, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 106  33 134 127  32  35 123  31]\n",
      "Fold 12, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35  21 123]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771 0.01754468\n",
      " 0.01747435 0.01446083 0.01321363 0.01310117]\n",
      "Fold 13, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 106  33 134 127  32  35  21 123]\n",
      "Fold 13, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [ 30 107 106  33 127  32 134  35  22 123]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319 0.01842883\n",
      " 0.01823597 0.01525073 0.01457077 0.01385602]\n",
      "Fold 14, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 106  33 127  32 134  35  22 123]\n",
      "Fold 14, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.45\n",
      "Selected features (RF): [ 30 106 107  33  32 134  35 105 129 127]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972 0.01836004\n",
      " 0.01680213 0.01447181 0.01430088 0.0139538 ]\n",
      "Fold 15, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 106 107  33  32 134  35 105 129 127]\n",
      "Fold 15, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [ 30 106  32 107  33  35 127 134 105 122]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613 0.01873923\n",
      " 0.01618912 0.01571743 0.01457547 0.01430517]\n",
      "Fold 16, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 106  32 107  33  35 127 134 105 122]\n",
      "Fold 16, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (RF): [ 30 107 106  33  32 134  35 109 127  21]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276 0.01956944\n",
      " 0.01569444 0.01428796 0.01372584 0.01366784]\n",
      "Fold 17, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 106  33  32 134  35 109 127  21]\n",
      "Fold 17, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (RF): [ 30 107 134 106  32  33  21  35 127 123]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419 0.02063558\n",
      " 0.01696546 0.01649607 0.01265368 0.01217766]\n",
      "Fold 18, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 134 106  32  33  21  35 127 123]\n",
      "Fold 18, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 107 106  33  32  35  21 127 134 109]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018 0.01912931\n",
      " 0.01576826 0.01519539 0.01477722 0.0138591 ]\n",
      "Fold 19, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107 106  33  32  35  21 127 134 109]\n",
      "Fold 19, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "Selected features (RF): [ 30 107  33  35  32 106  21 127 123 134]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176 0.01980564\n",
      " 0.01624184 0.01587182 0.0137815  0.01343143]\n",
      "Fold 20, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107  33  35  32 106  21 127 123 134]\n",
      "Fold 20, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107  32  33  35 106  21 109 127 134]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627 0.01809367\n",
      " 0.01624739 0.01549509 0.0149973  0.01417063]\n",
      "Fold 21, VarNum 10, Method RF, Model XGB, Selected Features: [ 30 107  32  33  35 106  21 109 127 134]\n",
      "Fold 21, VarNum 10, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252]\n",
      "Fold 0, VarNum 10, Method RF, Model LDA, Selected Features: [129 134 106  30 107 105  43 123 100  96]\n",
      "Fold 0, VarNum 10, Method RF, Model LDA, Train Acc: 0.72, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [129 134 106  30 127 107 131  97  87 123]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167 0.02039644\n",
      " 0.01602532 0.01587018 0.01557886 0.01551213]\n",
      "Fold 1, VarNum 10, Method RF, Model LDA, Selected Features: [129 134 106  30 127 107 131  97  87 123]\n",
      "Fold 1, VarNum 10, Method RF, Model LDA, Train Acc: 0.7397260273972602, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [134 129 107  30 106 127 123 100 102 105]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617 0.022985\n",
      " 0.01814312 0.01668106 0.01635147 0.0157809 ]\n",
      "Fold 2, VarNum 10, Method RF, Model LDA, Selected Features: [134 129 107  30 106 127 123 100 102 105]\n",
      "Fold 2, VarNum 10, Method RF, Model LDA, Train Acc: 0.7159763313609467, Test Acc: 0.8421052631578947\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [129 106 107  30 134 127 123 109 105  32]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656 0.02107612\n",
      " 0.02026495 0.01998218 0.01919863 0.01749485]\n",
      "Fold 3, VarNum 10, Method RF, Model LDA, Selected Features: [129 106 107  30 134 127 123 109 105  32]\n",
      "Fold 3, VarNum 10, Method RF, Model LDA, Train Acc: 0.7393617021276596, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [106 107 129 134  30 127  32 122 123 131]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219  0.02424004\n",
      " 0.01996565 0.01907085 0.01639677 0.01511131]\n",
      "Fold 4, VarNum 10, Method RF, Model LDA, Selected Features: [106 107 129 134  30 127  32 122 123 131]\n",
      "Fold 4, VarNum 10, Method RF, Model LDA, Train Acc: 0.7109004739336493, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [107  30 127 106 129 134 123 109 120 122]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077 0.02806966\n",
      " 0.02059141 0.01501137 0.01421506 0.01419714]\n",
      "Fold 5, VarNum 10, Method RF, Model LDA, Selected Features: [107  30 127 106 129 134 123 109 120 122]\n",
      "Fold 5, VarNum 10, Method RF, Model LDA, Train Acc: 0.7370689655172413, Test Acc: 0.5263157894736842\n",
      "Selected features (RF): [134 107 106 127  30 123 129 109 118 122]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231 0.02580135\n",
      " 0.02334401 0.01569307 0.01390511 0.01283023]\n",
      "Fold 6, VarNum 10, Method RF, Model LDA, Selected Features: [134 107 106 127  30 123 129 109 118 122]\n",
      "Fold 6, VarNum 10, Method RF, Model LDA, Train Acc: 0.7211155378486056, Test Acc: 0.8181818181818182\n",
      "Selected features (RF): [107  30 134 127 106 109 123 129 131 133]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009 0.01869848\n",
      " 0.01837955 0.0174225  0.01470666 0.01373099]\n",
      "Fold 7, VarNum 10, Method RF, Model LDA, Selected Features: [107  30 134 127 106 109 123 129 131 133]\n",
      "Fold 7, VarNum 10, Method RF, Model LDA, Train Acc: 0.7326007326007326, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [ 30 107 106 134 127 129  32 109  33 123]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463 0.01950434\n",
      " 0.01929563 0.01788344 0.01579003 0.01488815]\n",
      "Fold 8, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127 129  32 109  33 123]\n",
      "Fold 8, VarNum 10, Method RF, Model LDA, Train Acc: 0.726027397260274, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107 106 134 127  33 129  32  35 123]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201 0.01766004\n",
      " 0.0159082  0.01583366 0.01568882 0.01516106]\n",
      "Fold 9, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127  33 129  32  35 123]\n",
      "Fold 9, VarNum 10, Method RF, Model LDA, Train Acc: 0.7188498402555911, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107 106 134 127  33  35  32 123 129]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542 0.01950735\n",
      " 0.01673918 0.01632669 0.01592091 0.01496493]\n",
      "Fold 10, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127  33  35  32 123 129]\n",
      "Fold 10, VarNum 10, Method RF, Model LDA, Train Acc: 0.7125748502994012, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [ 30 107 134  33 106 127  35  32 129  31]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717 0.01998843\n",
      " 0.01947537 0.01675318 0.01650051 0.01521515]\n",
      "Fold 11, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 134  33 106 127  35  32 129  31]\n",
      "Fold 11, VarNum 10, Method RF, Model LDA, Train Acc: 0.7162921348314607, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35 123  31]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571 0.01720357\n",
      " 0.01623954 0.01578945 0.01415767 0.01344477]\n",
      "Fold 12, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 106  33 134 127  32  35 123  31]\n",
      "Fold 12, VarNum 10, Method RF, Model LDA, Train Acc: 0.728, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35  21 123]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771 0.01754468\n",
      " 0.01747435 0.01446083 0.01321363 0.01310117]\n",
      "Fold 13, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 106  33 134 127  32  35  21 123]\n",
      "Fold 13, VarNum 10, Method RF, Model LDA, Train Acc: 0.6977329974811083, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 106  33 127  32 134  35  22 123]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319 0.01842883\n",
      " 0.01823597 0.01525073 0.01457077 0.01385602]\n",
      "Fold 14, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 106  33 127  32 134  35  22 123]\n",
      "Fold 14, VarNum 10, Method RF, Model LDA, Train Acc: 0.6945107398568019, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 106 107  33  32 134  35 105 129 127]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972 0.01836004\n",
      " 0.01680213 0.01447181 0.01430088 0.0139538 ]\n",
      "Fold 15, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 106 107  33  32 134  35 105 129 127]\n",
      "Fold 15, VarNum 10, Method RF, Model LDA, Train Acc: 0.715261958997722, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [ 30 106  32 107  33  35 127 134 105 122]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613 0.01873923\n",
      " 0.01618912 0.01571743 0.01457547 0.01430517]\n",
      "Fold 16, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 106  32 107  33  35 127 134 105 122]\n",
      "Fold 16, VarNum 10, Method RF, Model LDA, Train Acc: 0.7121212121212122, Test Acc: 0.75\n",
      "Selected features (RF): [ 30 107 106  33  32 134  35 109 127  21]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276 0.01956944\n",
      " 0.01569444 0.01428796 0.01372584 0.01366784]\n",
      "Fold 17, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 106  33  32 134  35 109 127  21]\n",
      "Fold 17, VarNum 10, Method RF, Model LDA, Train Acc: 0.6721991701244814, Test Acc: 0.65\n",
      "Selected features (RF): [ 30 107 134 106  32  33  21  35 127 123]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419 0.02063558\n",
      " 0.01696546 0.01649607 0.01265368 0.01217766]\n",
      "Fold 18, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 134 106  32  33  21  35 127 123]\n",
      "Fold 18, VarNum 10, Method RF, Model LDA, Train Acc: 0.6812749003984063, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 30 107 106  33  32  35  21 127 134 109]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018 0.01912931\n",
      " 0.01576826 0.01519539 0.01477722 0.0138591 ]\n",
      "Fold 19, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107 106  33  32  35  21 127 134 109]\n",
      "Fold 19, VarNum 10, Method RF, Model LDA, Train Acc: 0.6736641221374046, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [ 30 107  33  35  32 106  21 127 123 134]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176 0.01980564\n",
      " 0.01624184 0.01587182 0.0137815  0.01343143]\n",
      "Fold 20, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107  33  35  32 106  21 127 123 134]\n",
      "Fold 20, VarNum 10, Method RF, Model LDA, Train Acc: 0.6685082872928176, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107  32  33  35 106  21 109 127 134]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627 0.01809367\n",
      " 0.01624739 0.01549509 0.0149973  0.01417063]\n",
      "Fold 21, VarNum 10, Method RF, Model LDA, Selected Features: [ 30 107  32  33  35 106  21 109 127 134]\n",
      "Fold 21, VarNum 10, Method RF, Model LDA, Train Acc: 0.6737588652482269, Test Acc: 0.6818181818181818\n",
      "[15:26:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594]\n",
      "Fold 0, VarNum 10, Method XGB, Model CART, Selected Features: [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Fold 0, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:26:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [  1 114  92  79 100  99 133 134  15 135]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001 0.03824509\n",
      " 0.03144442 0.03067115 0.02586752 0.02579053]\n",
      "Fold 1, VarNum 10, Method XGB, Model CART, Selected Features: [  1 114  92  79 100  99 133 134  15 135]\n",
      "Fold 1, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "[15:26:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119 118  83  97 134  89]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364  0.03260981\n",
      " 0.02879154 0.02842961 0.02837311 0.02583595]\n",
      "Fold 2, VarNum 10, Method XGB, Model CART, Selected Features: [ 26  72  77  46 119 118  83  97 134  89]\n",
      "Fold 2, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:26:05] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 59  25  46   0  61 106  99 134  30  97]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437 0.02968394\n",
      " 0.02835124 0.02730615 0.02658134 0.02494756]\n",
      "Fold 3, VarNum 10, Method XGB, Model CART, Selected Features: [ 59  25  46   0  61 106  99 134  30  97]\n",
      "Fold 3, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "[15:26:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25  32 107 114  64  35]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888 0.02571377\n",
      " 0.02553768 0.02345947 0.02330734 0.02305957]\n",
      "Fold 4, VarNum 10, Method XGB, Model CART, Selected Features: [ 91 106  11 134  25  32 107 114  64  35]\n",
      "Fold 4, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:26:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45  47  25 106  32 135]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451 0.03064233\n",
      " 0.02763383 0.02525128 0.02185246 0.02143421]\n",
      "Fold 5, VarNum 10, Method XGB, Model CART, Selected Features: [ 74  99 134 119  45  47  25 106  32 135]\n",
      "Fold 5, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:26:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45  64  95 134 106  92]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231 0.02633443\n",
      " 0.02538574 0.02404596 0.02323602 0.0198393 ]\n",
      "Fold 6, VarNum 10, Method XGB, Model CART, Selected Features: [107  99  17  21  45  64  95 134 106  92]\n",
      "Fold 6, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "[15:26:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40 134 127  45  25   1]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901 0.0272389\n",
      " 0.02550623 0.02285499 0.02245414 0.02209218]\n",
      "Fold 7, VarNum 10, Method XGB, Model CART, Selected Features: [107  99  66 130  40 134 127  45  25   1]\n",
      "Fold 7, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "[15:26:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30  18 112  85 134 135]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053  0.0235923\n",
      " 0.02342531 0.02282099 0.02216369 0.02056672]\n",
      "Fold 8, VarNum 10, Method XGB, Model CART, Selected Features: [107  22  65  60  30  18 112  85 134 135]\n",
      "Fold 8, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:26:06] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54  12  18  30  68  43]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297  0.02258516\n",
      " 0.02081037 0.02071864 0.01912442 0.01802449]\n",
      "Fold 9, VarNum 10, Method XGB, Model CART, Selected Features: [107  99   0  21  54  12  18  30  68  43]\n",
      "Fold 9, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "[15:26:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110  30  63  45 117 134]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609 0.02446463\n",
      " 0.02375421 0.02277838 0.01852702 0.01821284]\n",
      "Fold 10, VarNum 10, Method XGB, Model CART, Selected Features: [ 70 107  85  87 110  30  63  45 117 134]\n",
      "Fold 10, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:26:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85  39  49  30  15  59]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574  0.02708106\n",
      " 0.02700011 0.0267045  0.02186688 0.01906396]\n",
      "Fold 11, VarNum 10, Method XGB, Model CART, Selected Features: [ 99  12 106  93  85  39  49  30  15  59]\n",
      "Fold 11, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "[15:26:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20  38  99 124  93  44]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579 0.02196426\n",
      " 0.02097165 0.02021421 0.02020423 0.01939055]\n",
      "Fold 12, VarNum 10, Method XGB, Model CART, Selected Features: [ 27 107  74  30  20  38  99 124  93  44]\n",
      "Fold 12, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:26:07] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30  21  50  63  62 131]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133 0.02203631\n",
      " 0.02164648 0.01945004 0.01928781 0.01914054]\n",
      "Fold 13, VarNum 10, Method XGB, Model CART, Selected Features: [107  78  84  45  30  21  50  63  62 131]\n",
      "Fold 13, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:26:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94  99  30  71 132  24]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958 0.02117782\n",
      " 0.02106225 0.0206327  0.01638661 0.01580617]\n",
      "Fold 14, VarNum 10, Method XGB, Model CART, Selected Features: [107 106 112  51  94  99  30  71 132  24]\n",
      "Fold 14, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:26:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [106  93 119  30  45  29  85  33   3 105]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043 0.02049493\n",
      " 0.02032613 0.01902149 0.01853412 0.01766204]\n",
      "Fold 15, VarNum 10, Method XGB, Model CART, Selected Features: [106  93 119  30  45  29  85  33   3 105]\n",
      "Fold 15, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:26:08] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22  93  40 114  63 107]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031 0.01834612\n",
      " 0.01832638 0.01660874 0.0163494  0.01607329]\n",
      "Fold 16, VarNum 10, Method XGB, Model CART, Selected Features: [ 28  30 106  85  22  93  40 114  63 107]\n",
      "Fold 16, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.55\n",
      "[15:26:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93  84  30  26  67  49]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613 0.02621297\n",
      " 0.02436594 0.02368471 0.02286324 0.01924192]\n",
      "Fold 17, VarNum 10, Method XGB, Model CART, Selected Features: [106  45  40  18  93  84  30  26  67  49]\n",
      "Fold 17, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:26:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30  94  93   1  77  85]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267 0.02270854\n",
      " 0.02157388 0.02096329 0.0197053  0.01822887]\n",
      "Fold 18, VarNum 10, Method XGB, Model CART, Selected Features: [ 18  73 106 107  30  94  93   1  77  85]\n",
      "Fold 18, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "[15:26:09] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114  13  30  65  71  26]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243 0.02239063\n",
      " 0.02224347 0.02035736 0.02010566 0.01846889]\n",
      "Fold 19, VarNum 10, Method XGB, Model CART, Selected Features: [ 40  76  17  85 114  13  30  65  71  26]\n",
      "Fold 19, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:26:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106  40  34  71 121  87]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375 0.01896119\n",
      " 0.01791704 0.01761022 0.01748885 0.01717754]\n",
      "Fold 20, VarNum 10, Method XGB, Model CART, Selected Features: [ 46  18 114  30 106  40  34  71 121  87]\n",
      "Fold 20, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:26:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 76  40  64  30  36 114  85 106  32   7]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063 0.02041966\n",
      " 0.01861721 0.01584065 0.01534018 0.01502906]\n",
      "Fold 21, VarNum 10, Method XGB, Model CART, Selected Features: [ 76  40  64  30  36 114  85 106  32   7]\n",
      "Fold 21, VarNum 10, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:26:10] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594]\n",
      "Fold 0, VarNum 10, Method XGB, Model RF, Selected Features: [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Fold 0, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:26:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100  99 133 134  15 135]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001 0.03824509\n",
      " 0.03144442 0.03067115 0.02586752 0.02579053]\n",
      "Fold 1, VarNum 10, Method XGB, Model RF, Selected Features: [  1 114  92  79 100  99 133 134  15 135]\n",
      "Fold 1, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "[15:26:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119 118  83  97 134  89]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364  0.03260981\n",
      " 0.02879154 0.02842961 0.02837311 0.02583595]\n",
      "Fold 2, VarNum 10, Method XGB, Model RF, Selected Features: [ 26  72  77  46 119 118  83  97 134  89]\n",
      "Fold 2, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:26:11] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 59  25  46   0  61 106  99 134  30  97]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437 0.02968394\n",
      " 0.02835124 0.02730615 0.02658134 0.02494756]\n",
      "Fold 3, VarNum 10, Method XGB, Model RF, Selected Features: [ 59  25  46   0  61 106  99 134  30  97]\n",
      "Fold 3, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.782608695652174\n",
      "[15:26:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25  32 107 114  64  35]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888 0.02571377\n",
      " 0.02553768 0.02345947 0.02330734 0.02305957]\n",
      "Fold 4, VarNum 10, Method XGB, Model RF, Selected Features: [ 91 106  11 134  25  32 107 114  64  35]\n",
      "Fold 4, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:26:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45  47  25 106  32 135]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451 0.03064233\n",
      " 0.02763383 0.02525128 0.02185246 0.02143421]\n",
      "Fold 5, VarNum 10, Method XGB, Model RF, Selected Features: [ 74  99 134 119  45  47  25 106  32 135]\n",
      "Fold 5, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:26:12] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45  64  95 134 106  92]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231 0.02633443\n",
      " 0.02538574 0.02404596 0.02323602 0.0198393 ]\n",
      "Fold 6, VarNum 10, Method XGB, Model RF, Selected Features: [107  99  17  21  45  64  95 134 106  92]\n",
      "Fold 6, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:26:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40 134 127  45  25   1]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901 0.0272389\n",
      " 0.02550623 0.02285499 0.02245414 0.02209218]\n",
      "Fold 7, VarNum 10, Method XGB, Model RF, Selected Features: [107  99  66 130  40 134 127  45  25   1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:26:13] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30  18 112  85 134 135]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053  0.0235923\n",
      " 0.02342531 0.02282099 0.02216369 0.02056672]\n",
      "Fold 8, VarNum 10, Method XGB, Model RF, Selected Features: [107  22  65  60  30  18 112  85 134 135]\n",
      "Fold 8, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:26:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54  12  18  30  68  43]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297  0.02258516\n",
      " 0.02081037 0.02071864 0.01912442 0.01802449]\n",
      "Fold 9, VarNum 10, Method XGB, Model RF, Selected Features: [107  99   0  21  54  12  18  30  68  43]\n",
      "Fold 9, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:26:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110  30  63  45 117 134]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609 0.02446463\n",
      " 0.02375421 0.02277838 0.01852702 0.01821284]\n",
      "Fold 10, VarNum 10, Method XGB, Model RF, Selected Features: [ 70 107  85  87 110  30  63  45 117 134]\n",
      "Fold 10, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "[15:26:14] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85  39  49  30  15  59]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574  0.02708106\n",
      " 0.02700011 0.0267045  0.02186688 0.01906396]\n",
      "Fold 11, VarNum 10, Method XGB, Model RF, Selected Features: [ 99  12 106  93  85  39  49  30  15  59]\n",
      "Fold 11, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:26:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20  38  99 124  93  44]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579 0.02196426\n",
      " 0.02097165 0.02021421 0.02020423 0.01939055]\n",
      "Fold 12, VarNum 10, Method XGB, Model RF, Selected Features: [ 27 107  74  30  20  38  99 124  93  44]\n",
      "Fold 12, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:26:15] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30  21  50  63  62 131]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133 0.02203631\n",
      " 0.02164648 0.01945004 0.01928781 0.01914054]\n",
      "Fold 13, VarNum 10, Method XGB, Model RF, Selected Features: [107  78  84  45  30  21  50  63  62 131]\n",
      "Fold 13, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:26:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94  99  30  71 132  24]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958 0.02117782\n",
      " 0.02106225 0.0206327  0.01638661 0.01580617]\n",
      "Fold 14, VarNum 10, Method XGB, Model RF, Selected Features: [107 106 112  51  94  99  30  71 132  24]\n",
      "Fold 14, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.45\n",
      "[15:26:16] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45  29  85  33   3 105]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043 0.02049493\n",
      " 0.02032613 0.01902149 0.01853412 0.01766204]\n",
      "Fold 15, VarNum 10, Method XGB, Model RF, Selected Features: [106  93 119  30  45  29  85  33   3 105]\n",
      "Fold 15, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:26:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22  93  40 114  63 107]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031 0.01834612\n",
      " 0.01832638 0.01660874 0.0163494  0.01607329]\n",
      "Fold 16, VarNum 10, Method XGB, Model RF, Selected Features: [ 28  30 106  85  22  93  40 114  63 107]\n",
      "Fold 16, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:26:17] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93  84  30  26  67  49]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613 0.02621297\n",
      " 0.02436594 0.02368471 0.02286324 0.01924192]\n",
      "Fold 17, VarNum 10, Method XGB, Model RF, Selected Features: [106  45  40  18  93  84  30  26  67  49]\n",
      "Fold 17, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:26:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30  94  93   1  77  85]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267 0.02270854\n",
      " 0.02157388 0.02096329 0.0197053  0.01822887]\n",
      "Fold 18, VarNum 10, Method XGB, Model RF, Selected Features: [ 18  73 106 107  30  94  93   1  77  85]\n",
      "Fold 18, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:26:18] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114  13  30  65  71  26]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243 0.02239063\n",
      " 0.02224347 0.02035736 0.02010566 0.01846889]\n",
      "Fold 19, VarNum 10, Method XGB, Model RF, Selected Features: [ 40  76  17  85 114  13  30  65  71  26]\n",
      "Fold 19, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:26:19] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106  40  34  71 121  87]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375 0.01896119\n",
      " 0.01791704 0.01761022 0.01748885 0.01717754]\n",
      "Fold 20, VarNum 10, Method XGB, Model RF, Selected Features: [ 46  18 114  30 106  40  34  71 121  87]\n",
      "Fold 20, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:26:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 76  40  64  30  36 114  85 106  32   7]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063 0.02041966\n",
      " 0.01861721 0.01584065 0.01534018 0.01502906]\n",
      "Fold 21, VarNum 10, Method XGB, Model RF, Selected Features: [ 76  40  64  30  36 114  85 106  32   7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 21, VarNum 10, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "[15:26:20] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594]\n",
      "Fold 0, VarNum 10, Method XGB, Model XGB, Selected Features: [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Fold 0, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:26:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100  99 133 134  15 135]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001 0.03824509\n",
      " 0.03144442 0.03067115 0.02586752 0.02579053]\n",
      "Fold 1, VarNum 10, Method XGB, Model XGB, Selected Features: [  1 114  92  79 100  99 133 134  15 135]\n",
      "Fold 1, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.2608695652173913\n",
      "[15:26:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119 118  83  97 134  89]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364  0.03260981\n",
      " 0.02879154 0.02842961 0.02837311 0.02583595]\n",
      "Fold 2, VarNum 10, Method XGB, Model XGB, Selected Features: [ 26  72  77  46 119 118  83  97 134  89]\n",
      "Fold 2, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:26:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 59  25  46   0  61 106  99 134  30  97]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437 0.02968394\n",
      " 0.02835124 0.02730615 0.02658134 0.02494756]\n",
      "Fold 3, VarNum 10, Method XGB, Model XGB, Selected Features: [ 59  25  46   0  61 106  99 134  30  97]\n",
      "Fold 3, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "[15:26:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25  32 107 114  64  35]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888 0.02571377\n",
      " 0.02553768 0.02345947 0.02330734 0.02305957]\n",
      "Fold 4, VarNum 10, Method XGB, Model XGB, Selected Features: [ 91 106  11 134  25  32 107 114  64  35]\n",
      "Fold 4, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "[15:26:21] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45  47  25 106  32 135]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451 0.03064233\n",
      " 0.02763383 0.02525128 0.02185246 0.02143421]\n",
      "Fold 5, VarNum 10, Method XGB, Model XGB, Selected Features: [ 74  99 134 119  45  47  25 106  32 135]\n",
      "Fold 5, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:26:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45  64  95 134 106  92]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231 0.02633443\n",
      " 0.02538574 0.02404596 0.02323602 0.0198393 ]\n",
      "Fold 6, VarNum 10, Method XGB, Model XGB, Selected Features: [107  99  17  21  45  64  95 134 106  92]\n",
      "Fold 6, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:26:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40 134 127  45  25   1]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901 0.0272389\n",
      " 0.02550623 0.02285499 0.02245414 0.02209218]\n",
      "Fold 7, VarNum 10, Method XGB, Model XGB, Selected Features: [107  99  66 130  40 134 127  45  25   1]\n",
      "Fold 7, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:26:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30  18 112  85 134 135]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053  0.0235923\n",
      " 0.02342531 0.02282099 0.02216369 0.02056672]\n",
      "Fold 8, VarNum 10, Method XGB, Model XGB, Selected Features: [107  22  65  60  30  18 112  85 134 135]\n",
      "Fold 8, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "[15:26:22] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54  12  18  30  68  43]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297  0.02258516\n",
      " 0.02081037 0.02071864 0.01912442 0.01802449]\n",
      "Fold 9, VarNum 10, Method XGB, Model XGB, Selected Features: [107  99   0  21  54  12  18  30  68  43]\n",
      "Fold 9, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:26:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110  30  63  45 117 134]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609 0.02446463\n",
      " 0.02375421 0.02277838 0.01852702 0.01821284]\n",
      "Fold 10, VarNum 10, Method XGB, Model XGB, Selected Features: [ 70 107  85  87 110  30  63  45 117 134]\n",
      "Fold 10, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "[15:26:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85  39  49  30  15  59]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574  0.02708106\n",
      " 0.02700011 0.0267045  0.02186688 0.01906396]\n",
      "Fold 11, VarNum 10, Method XGB, Model XGB, Selected Features: [ 99  12 106  93  85  39  49  30  15  59]\n",
      "Fold 11, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:26:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20  38  99 124  93  44]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579 0.02196426\n",
      " 0.02097165 0.02021421 0.02020423 0.01939055]\n",
      "Fold 12, VarNum 10, Method XGB, Model XGB, Selected Features: [ 27 107  74  30  20  38  99 124  93  44]\n",
      "Fold 12, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:26:23] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30  21  50  63  62 131]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133 0.02203631\n",
      " 0.02164648 0.01945004 0.01928781 0.01914054]\n",
      "Fold 13, VarNum 10, Method XGB, Model XGB, Selected Features: [107  78  84  45  30  21  50  63  62 131]\n",
      "Fold 13, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:26:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [107 106 112  51  94  99  30  71 132  24]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958 0.02117782\n",
      " 0.02106225 0.0206327  0.01638661 0.01580617]\n",
      "Fold 14, VarNum 10, Method XGB, Model XGB, Selected Features: [107 106 112  51  94  99  30  71 132  24]\n",
      "Fold 14, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:26:24] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45  29  85  33   3 105]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043 0.02049493\n",
      " 0.02032613 0.01902149 0.01853412 0.01766204]\n",
      "Fold 15, VarNum 10, Method XGB, Model XGB, Selected Features: [106  93 119  30  45  29  85  33   3 105]\n",
      "Fold 15, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:26:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22  93  40 114  63 107]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031 0.01834612\n",
      " 0.01832638 0.01660874 0.0163494  0.01607329]\n",
      "Fold 16, VarNum 10, Method XGB, Model XGB, Selected Features: [ 28  30 106  85  22  93  40 114  63 107]\n",
      "Fold 16, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:26:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93  84  30  26  67  49]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613 0.02621297\n",
      " 0.02436594 0.02368471 0.02286324 0.01924192]\n",
      "Fold 17, VarNum 10, Method XGB, Model XGB, Selected Features: [106  45  40  18  93  84  30  26  67  49]\n",
      "Fold 17, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.55\n",
      "[15:26:25] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30  94  93   1  77  85]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267 0.02270854\n",
      " 0.02157388 0.02096329 0.0197053  0.01822887]\n",
      "Fold 18, VarNum 10, Method XGB, Model XGB, Selected Features: [ 18  73 106 107  30  94  93   1  77  85]\n",
      "Fold 18, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:26:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114  13  30  65  71  26]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243 0.02239063\n",
      " 0.02224347 0.02035736 0.02010566 0.01846889]\n",
      "Fold 19, VarNum 10, Method XGB, Model XGB, Selected Features: [ 40  76  17  85 114  13  30  65  71  26]\n",
      "Fold 19, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:26:26] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106  40  34  71 121  87]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375 0.01896119\n",
      " 0.01791704 0.01761022 0.01748885 0.01717754]\n",
      "Fold 20, VarNum 10, Method XGB, Model XGB, Selected Features: [ 46  18 114  30 106  40  34  71 121  87]\n",
      "Fold 20, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:26:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 76  40  64  30  36 114  85 106  32   7]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063 0.02041966\n",
      " 0.01861721 0.01584065 0.01534018 0.01502906]\n",
      "Fold 21, VarNum 10, Method XGB, Model XGB, Selected Features: [ 76  40  64  30  36 114  85 106  32   7]\n",
      "Fold 21, VarNum 10, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:26:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594]\n",
      "Fold 0, VarNum 10, Method XGB, Model LDA, Selected Features: [ 54  92  67 105  46  76 134 111  66  57]\n",
      "Fold 0, VarNum 10, Method XGB, Model LDA, Train Acc: 0.696, Test Acc: 0.5714285714285714\n",
      "[15:26:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100  99 133 134  15 135]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001 0.03824509\n",
      " 0.03144442 0.03067115 0.02586752 0.02579053]\n",
      "Fold 1, VarNum 10, Method XGB, Model LDA, Selected Features: [  1 114  92  79 100  99 133 134  15 135]\n",
      "Fold 1, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6232876712328768, Test Acc: 0.5652173913043478\n",
      "[15:26:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119 118  83  97 134  89]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364  0.03260981\n",
      " 0.02879154 0.02842961 0.02837311 0.02583595]\n",
      "Fold 2, VarNum 10, Method XGB, Model LDA, Selected Features: [ 26  72  77  46 119 118  83  97 134  89]\n",
      "Fold 2, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6923076923076923, Test Acc: 0.6842105263157895\n",
      "[15:26:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 59  25  46   0  61 106  99 134  30  97]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437 0.02968394\n",
      " 0.02835124 0.02730615 0.02658134 0.02494756]\n",
      "Fold 3, VarNum 10, Method XGB, Model LDA, Selected Features: [ 59  25  46   0  61 106  99 134  30  97]\n",
      "Fold 3, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7127659574468085, Test Acc: 0.7391304347826086\n",
      "[15:26:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25  32 107 114  64  35]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888 0.02571377\n",
      " 0.02553768 0.02345947 0.02330734 0.02305957]\n",
      "Fold 4, VarNum 10, Method XGB, Model LDA, Selected Features: [ 91 106  11 134  25  32 107 114  64  35]\n",
      "Fold 4, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6777251184834123, Test Acc: 0.47619047619047616\n",
      "[15:26:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45  47  25 106  32 135]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451 0.03064233\n",
      " 0.02763383 0.02525128 0.02185246 0.02143421]\n",
      "Fold 5, VarNum 10, Method XGB, Model LDA, Selected Features: [ 74  99 134 119  45  47  25 106  32 135]\n",
      "Fold 5, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7112068965517241, Test Acc: 0.5789473684210527\n",
      "[15:26:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selected features (XGB): [107  99  17  21  45  64  95 134 106  92]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231 0.02633443\n",
      " 0.02538574 0.02404596 0.02323602 0.0198393 ]\n",
      "Fold 6, VarNum 10, Method XGB, Model LDA, Selected Features: [107  99  17  21  45  64  95 134 106  92]\n",
      "Fold 6, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6892430278884463, Test Acc: 0.45454545454545453\n",
      "[15:26:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40 134 127  45  25   1]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901 0.0272389\n",
      " 0.02550623 0.02285499 0.02245414 0.02209218]\n",
      "Fold 7, VarNum 10, Method XGB, Model LDA, Selected Features: [107  99  66 130  40 134 127  45  25   1]\n",
      "Fold 7, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6996336996336996, Test Acc: 0.5263157894736842\n",
      "[15:26:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30  18 112  85 134 135]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053  0.0235923\n",
      " 0.02342531 0.02282099 0.02216369 0.02056672]\n",
      "Fold 8, VarNum 10, Method XGB, Model LDA, Selected Features: [107  22  65  60  30  18 112  85 134 135]\n",
      "Fold 8, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7294520547945206, Test Acc: 0.6666666666666666\n",
      "[15:26:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54  12  18  30  68  43]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297  0.02258516\n",
      " 0.02081037 0.02071864 0.01912442 0.01802449]\n",
      "Fold 9, VarNum 10, Method XGB, Model LDA, Selected Features: [107  99   0  21  54  12  18  30  68  43]\n",
      "Fold 9, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7220447284345048, Test Acc: 0.5714285714285714\n",
      "[15:26:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110  30  63  45 117 134]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609 0.02446463\n",
      " 0.02375421 0.02277838 0.01852702 0.01821284]\n",
      "Fold 10, VarNum 10, Method XGB, Model LDA, Selected Features: [ 70 107  85  87 110  30  63  45 117 134]\n",
      "Fold 10, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6976047904191617, Test Acc: 0.6363636363636364\n",
      "[15:26:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85  39  49  30  15  59]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574  0.02708106\n",
      " 0.02700011 0.0267045  0.02186688 0.01906396]\n",
      "Fold 11, VarNum 10, Method XGB, Model LDA, Selected Features: [ 99  12 106  93  85  39  49  30  15  59]\n",
      "Fold 11, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7331460674157303, Test Acc: 0.5789473684210527\n",
      "[15:26:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20  38  99 124  93  44]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579 0.02196426\n",
      " 0.02097165 0.02021421 0.02020423 0.01939055]\n",
      "Fold 12, VarNum 10, Method XGB, Model LDA, Selected Features: [ 27 107  74  30  20  38  99 124  93  44]\n",
      "Fold 12, VarNum 10, Method XGB, Model LDA, Train Acc: 0.72, Test Acc: 0.6363636363636364\n",
      "[15:26:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30  21  50  63  62 131]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133 0.02203631\n",
      " 0.02164648 0.01945004 0.01928781 0.01914054]\n",
      "Fold 13, VarNum 10, Method XGB, Model LDA, Selected Features: [107  78  84  45  30  21  50  63  62 131]\n",
      "Fold 13, VarNum 10, Method XGB, Model LDA, Train Acc: 0.690176322418136, Test Acc: 0.6818181818181818\n",
      "[15:26:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94  99  30  71 132  24]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958 0.02117782\n",
      " 0.02106225 0.0206327  0.01638661 0.01580617]\n",
      "Fold 14, VarNum 10, Method XGB, Model LDA, Selected Features: [107 106 112  51  94  99  30  71 132  24]\n",
      "Fold 14, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6873508353221957, Test Acc: 0.65\n",
      "[15:26:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45  29  85  33   3 105]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043 0.02049493\n",
      " 0.02032613 0.01902149 0.01853412 0.01766204]\n",
      "Fold 15, VarNum 10, Method XGB, Model LDA, Selected Features: [106  93 119  30  45  29  85  33   3 105]\n",
      "Fold 15, VarNum 10, Method XGB, Model LDA, Train Acc: 0.7061503416856492, Test Acc: 0.7391304347826086\n",
      "[15:26:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22  93  40 114  63 107]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031 0.01834612\n",
      " 0.01832638 0.01660874 0.0163494  0.01607329]\n",
      "Fold 16, VarNum 10, Method XGB, Model LDA, Selected Features: [ 28  30 106  85  22  93  40 114  63 107]\n",
      "Fold 16, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6861471861471862, Test Acc: 0.75\n",
      "[15:26:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93  84  30  26  67  49]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613 0.02621297\n",
      " 0.02436594 0.02368471 0.02286324 0.01924192]\n",
      "Fold 17, VarNum 10, Method XGB, Model LDA, Selected Features: [106  45  40  18  93  84  30  26  67  49]\n",
      "Fold 17, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6763485477178424, Test Acc: 0.65\n",
      "[15:26:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30  94  93   1  77  85]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267 0.02270854\n",
      " 0.02157388 0.02096329 0.0197053  0.01822887]\n",
      "Fold 18, VarNum 10, Method XGB, Model LDA, Selected Features: [ 18  73 106 107  30  94  93   1  77  85]\n",
      "Fold 18, VarNum 10, Method XGB, Model LDA, Train Acc: 0.701195219123506, Test Acc: 0.6363636363636364\n",
      "[15:26:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114  13  30  65  71  26]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243 0.02239063\n",
      " 0.02224347 0.02035736 0.02010566 0.01846889]\n",
      "Fold 19, VarNum 10, Method XGB, Model LDA, Selected Features: [ 40  76  17  85 114  13  30  65  71  26]\n",
      "Fold 19, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6583969465648855, Test Acc: 0.6842105263157895\n",
      "[15:26:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 46  18 114  30 106  40  34  71 121  87]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375 0.01896119\n",
      " 0.01791704 0.01761022 0.01748885 0.01717754]\n",
      "Fold 20, VarNum 10, Method XGB, Model LDA, Selected Features: [ 46  18 114  30 106  40  34  71 121  87]\n",
      "Fold 20, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6942909760589319, Test Acc: 0.7142857142857143\n",
      "[15:26:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 76  40  64  30  36 114  85 106  32   7]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063 0.02041966\n",
      " 0.01861721 0.01584065 0.01534018 0.01502906]\n",
      "Fold 21, VarNum 10, Method XGB, Model LDA, Selected Features: [ 76  40  64  30  36 114  85 106  32   7]\n",
      "Fold 21, VarNum 10, Method XGB, Model LDA, Train Acc: 0.6897163120567376, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.47619047619047616\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.4\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.4\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.8\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.2631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model CART, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model CART, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.8947368421052632\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model RF, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.8636363636363636\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.2631578947368421\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model XGB, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 0, VarNum 15, Method CORR, Model LDA, Train Acc: 0.776, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 1, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7671232876712328, Test Acc: 0.6956521739130435\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 2, VarNum 15, Method CORR, Model LDA, Train Acc: 0.757396449704142, Test Acc: 0.7894736842105263\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 3, VarNum 15, Method CORR, Model LDA, Train Acc: 0.75, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 4, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7393364928909952, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 5, VarNum 15, Method CORR, Model LDA, Train Acc: 0.728448275862069, Test Acc: 0.6842105263157895\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 6, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7370517928286853, Test Acc: 0.5909090909090909\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 7, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7435897435897436, Test Acc: 0.8421052631578947\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 8, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7397260273972602, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 9, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7348242811501597, Test Acc: 0.5238095238095238\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 10, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7305389221556886, Test Acc: 0.6818181818181818\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 11, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7191011235955056, Test Acc: 0.631578947368421\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 12, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7306666666666667, Test Acc: 0.6363636363636364\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 13, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7204030226700252, Test Acc: 0.7727272727272727\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 14, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7183770883054893, Test Acc: 0.65\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 15, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7175398633257403, Test Acc: 0.6521739130434783\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 16, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7164502164502164, Test Acc: 0.75\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 17, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7136929460580913, Test Acc: 0.7\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 18, VarNum 15, Method CORR, Model LDA, Train Acc: 0.7151394422310757, Test Acc: 0.5\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 19, VarNum 15, Method CORR, Model LDA, Train Acc: 0.700381679389313, Test Acc: 0.5263157894736842\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 20, VarNum 15, Method CORR, Model LDA, Train Acc: 0.6979742173112339, Test Acc: 0.7142857142857143\n",
      "Selected features (CorrelationFS): [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model LDA, Selected Features: [30, 32, 33, 35, 21, 106, 107, 31, 22, 127, 38, 39, 134, 37, 25]\n",
      "Fold 21, VarNum 15, Method CORR, Model LDA, Train Acc: 0.6932624113475178, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model CART, Selected Features: [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  87 106 107 121 122 127 129 131 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  87 106 107 121 122 127 129 131 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  87 100 106 107 121 122 127 129 134]\n",
      "Fold 2, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  87 100 106 107 121 122 127 129 134]\n",
      "Fold 2, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37  39  87 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  37  39  87 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35  37  39 106 107 121 122 123 127 129 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model CART, Selected Features: [ 30  31  32  33  35  37  39 106 107 121 122 123 127 129 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 25  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model CART, Selected Features: [ 25  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39  87 106 107 121 127 134]\n",
      "Fold 7, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  30  31  32  33  35  37  39  87 106 107 121 127 134]\n",
      "Fold 7, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  39 106 107 123 127 134]\n",
      "Fold 8, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  25  30  31  32  33  35  37  39 106 107 123 127 134]\n",
      "Fold 8, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 9, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 9, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 10, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 10, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 11, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 11, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 19, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 20, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 20, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 21, VarNum 15, Method CHI2, Model CART, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 21, VarNum 15, Method CHI2, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model RF, Selected Features: [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  87 106 107 121 122 127 129 131 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  87 106 107 121 122 127 129 131 134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  87 100 106 107 121 122 127 129 134]\n",
      "Fold 2, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  87 100 106 107 121 122 127 129 134]\n",
      "Fold 2, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37  39  87 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  37  39  87 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35  37  39 106 107 121 122 123 127 129 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model RF, Selected Features: [ 30  31  32  33  35  37  39 106 107 121 122 123 127 129 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (ChiSquareFS): [ 25  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model RF, Selected Features: [ 25  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39  87 106 107 121 127 134]\n",
      "Fold 7, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  30  31  32  33  35  37  39  87 106 107 121 127 134]\n",
      "Fold 7, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  39 106 107 123 127 134]\n",
      "Fold 8, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  39 106 107 123 127 134]\n",
      "Fold 8, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 9, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 9, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 10, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 10, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 11, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 11, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 12, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 12, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 13, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 13, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 14, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 14, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 15, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 15, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 16, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 16, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 17, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 17, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 18, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 18, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.3157894736842105\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 20, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 20, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 21, VarNum 15, Method CHI2, Model RF, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 21, VarNum 15, Method CHI2, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  87 106 107 121 122 127 129 131 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  87 106 107 121 122 127 129 131 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  87 100 106 107 121 122 127 129 134]\n",
      "Fold 2, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  87 100 106 107 121 122 127 129 134]\n",
      "Fold 2, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.8947368421052632\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37  39  87 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  37  39  87 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35  37  39 106 107 121 122 123 127 129 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 30  31  32  33  35  37  39 106 107 121 122 123 127 129 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7619047619047619\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (ChiSquareFS): [ 25  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 25  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39  87 106 107 121 127 134]\n",
      "Fold 7, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  30  31  32  33  35  37  39  87 106 107 121 127 134]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 7, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  39 106 107 123 127 134]\n",
      "Fold 8, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  39 106 107 123 127 134]\n",
      "Fold 8, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 9, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 9, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 10, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 10, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.8181818181818182\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 11, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 11, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 12, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 12, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 13, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 13, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 14, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 14, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 15, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 15, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 16, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 16, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.8\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 17, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 17, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 18, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 18, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.2631578947368421\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 20, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 20, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 21, VarNum 15, Method CHI2, Model XGB, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 21, VarNum 15, Method CHI2, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (ChiSquareFS): [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 30  33  35  44  49  51  55  87 102 106 107 121 127 129 134]\n",
      "Fold 0, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.736, Test Acc: 0.6190476190476191\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  87 106 107 121 122 127 129 131 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  87 106 107 121 122 127 129 131 134]\n",
      "Fold 1, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7671232876712328, Test Acc: 0.5217391304347826\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  87 100 106 107 121 122 127 129 134]\n",
      "Fold 2, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  87 100 106 107 121 122 127 129 134]\n",
      "Fold 2, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7692307692307693, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37  39  87 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  37  39  87 106 107 121 127 129 134]\n",
      "Fold 3, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7393617021276596, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 30  31  32  33  35  37  39 106 107 121 122 123 127 129 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 30  31  32  33  35  37  39 106 107 121 122 123 127 129 134]\n",
      "Fold 4, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7393364928909952, Test Acc: 0.5714285714285714\n",
      "Selected features (ChiSquareFS): [ 21  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 5, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7327586206896551, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 25  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 25  30  31  32  33  35  37 106 107 121 122 123 127 129 134]\n",
      "Fold 6, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7131474103585658, Test Acc: 0.7272727272727273\n",
      "Selected features (ChiSquareFS): [ 21  22  30  31  32  33  35  37  39  87 106 107 121 127 134]\n",
      "Fold 7, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  30  31  32  33  35  37  39  87 106 107 121 127 134]\n",
      "Fold 7, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.73992673992674, Test Acc: 0.8421052631578947\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  39 106 107 123 127 134]\n",
      "Fold 8, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  39 106 107 123 127 134]\n",
      "Fold 8, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7602739726027398, Test Acc: 0.8095238095238095\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 9, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 9, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7348242811501597, Test Acc: 0.5238095238095238\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 10, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 10, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7305389221556886, Test Acc: 0.6818181818181818\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 11, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 11, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7191011235955056, Test Acc: 0.631578947368421\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 12, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 12, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7306666666666667, Test Acc: 0.6363636363636364\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 13, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39 106 107 127 134]\n",
      "Fold 13, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7204030226700252, Test Acc: 0.7727272727272727\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 14, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 14, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.711217183770883, Test Acc: 0.65\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 15, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 15, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7107061503416856, Test Acc: 0.6521739130434783\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 16, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  87 106 107 127]\n",
      "Fold 16, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7164502164502164, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 17, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 17, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7157676348547718, Test Acc: 0.75\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 18, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  37  38  39  51  76  87 127]\n",
      "Fold 18, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7131474103585658, Test Acc: 0.5909090909090909\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 19, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7156488549618321, Test Acc: 0.42105263157894735\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 20, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 20, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.7053406998158379, Test Acc: 0.6666666666666666\n",
      "Selected features (ChiSquareFS): [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 21, VarNum 15, Method CHI2, Model LDA, Selected Features: [ 21  22  25  30  31  32  33  35  36  37  38  39  87 107 127]\n",
      "Fold 21, VarNum 15, Method CHI2, Model LDA, Train Acc: 0.6985815602836879, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252 0.01557481 0.01491991\n",
      " 0.01489731 0.01448407 0.01424924]\n",
      "Fold 0, VarNum 15, Method RF, Model CART, Selected Features: [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Fold 0, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [129 134 106  30 127 107 131  97  87 123  76 124 102 121 105]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167 0.02039644\n",
      " 0.01602532 0.01587018 0.01557886 0.01551213 0.01518244 0.01426517\n",
      " 0.01395143 0.01385261 0.01367092]\n",
      "Fold 1, VarNum 15, Method RF, Model CART, Selected Features: [129 134 106  30 127 107 131  97  87 123  76 124 102 121 105]\n",
      "Fold 1, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (RF): [134 129 107  30 106 127 123 100 102 105 131  87 108 104  89]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617 0.022985\n",
      " 0.01814312 0.01668106 0.01635147 0.0157809  0.01432202 0.01393263\n",
      " 0.01270142 0.01249753 0.01204802]\n",
      "Fold 2, VarNum 15, Method RF, Model CART, Selected Features: [134 129 107  30 106 127 123 100 102 105 131  87 108 104  89]\n",
      "Fold 2, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "Selected features (RF): [129 106 107  30 134 127 123 109 105  32  89  87 131 100  93]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656 0.02107612\n",
      " 0.02026495 0.01998218 0.01919863 0.01749485 0.0166377  0.01642018\n",
      " 0.0161489  0.01350251 0.01300882]\n",
      "Fold 3, VarNum 15, Method RF, Model CART, Selected Features: [129 106 107  30 134 127 123 109 105  32  89  87 131 100  93]\n",
      "Fold 3, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (RF): [106 107 129 134  30 127  32 122 123 131 120 109  89 105  94]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219  0.02424004\n",
      " 0.01996565 0.01907085 0.01639677 0.01511131 0.0150213  0.01447112\n",
      " 0.01347839 0.01309091 0.01281931]\n",
      "Fold 4, VarNum 15, Method RF, Model CART, Selected Features: [106 107 129 134  30 127  32 122 123 131 120 109  89 105  94]\n",
      "Fold 4, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.8095238095238095\n",
      "Selected features (RF): [107  30 127 106 129 134 123 109 120 122  32 131  89 118  33]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077 0.02806966\n",
      " 0.02059141 0.01501137 0.01421506 0.01419714 0.0139807  0.01389486\n",
      " 0.01336675 0.0132965  0.01235081]\n",
      "Fold 5, VarNum 15, Method RF, Model CART, Selected Features: [107  30 127 106 129 134 123 109 120 122  32 131  89 118  33]\n",
      "Fold 5, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [134 107 106 127  30 123 129 109 118 122  89  32  33 120  96]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231 0.02580135\n",
      " 0.02334401 0.01569307 0.01390511 0.01283023 0.01279845 0.01251089\n",
      " 0.01217836 0.01197494 0.011698  ]\n",
      "Fold 6, VarNum 15, Method RF, Model CART, Selected Features: [134 107 106 127  30 123 129 109 118 122  89  32  33 120  96]\n",
      "Fold 6, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.8181818181818182\n",
      "Selected features (RF): [107  30 134 127 106 109 123 129 131 133  32  33 100  96 108]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009 0.01869848\n",
      " 0.01837955 0.0174225  0.01470666 0.01373099 0.01369416 0.01369062\n",
      " 0.01305791 0.0128005  0.01204342]\n",
      "Fold 7, VarNum 15, Method RF, Model CART, Selected Features: [107  30 134 127 106 109 123 129 131 133  32  33 100  96 108]\n",
      "Fold 7, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [ 30 107 106 134 127 129  32 109  33 123 122 131 108  35  89]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463 0.01950434\n",
      " 0.01929563 0.01788344 0.01579003 0.01488815 0.01444204 0.01390951\n",
      " 0.01369903 0.0124751  0.01192624]\n",
      "Fold 8, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 106 134 127 129  32 109  33 123 122 131 108  35  89]\n",
      "Fold 8, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107 106 134 127  33 129  32  35 123 108 109  22 131  98]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201 0.01766004\n",
      " 0.0159082  0.01583366 0.01568882 0.01516106 0.01415489 0.01263284\n",
      " 0.01213981 0.01178665 0.01175603]\n",
      "Fold 9, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 106 134 127  33 129  32  35 123 108 109  22 131  98]\n",
      "Fold 9, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.47619047619047616\n",
      "Selected features (RF): [ 30 107 106 134 127  33  35  32 123 129 109  90  96  21 108]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542 0.01950735\n",
      " 0.01673918 0.01632669 0.01592091 0.01496493 0.01466358 0.01431061\n",
      " 0.01368107 0.01287874 0.01240728]\n",
      "Fold 10, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 106 134 127  33  35  32 123 129 109  90  96  21 108]\n",
      "Fold 10, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [ 30 107 134  33 106 127  35  32 129  31 123 108  57  96 109]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717 0.01998843\n",
      " 0.01947537 0.01675318 0.01650051 0.01521515 0.01391107 0.01209299\n",
      " 0.01157108 0.01148276 0.01021131]\n",
      "Fold 11, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 134  33 106 127  35  32 129  31 123 108  57  96 109]\n",
      "Fold 11, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35 123  31  21  96  57 132  89]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571 0.01720357\n",
      " 0.01623954 0.01578945 0.01415767 0.01344477 0.01321165 0.01152826\n",
      " 0.01145633 0.01114404 0.01105053]\n",
      "Fold 12, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 106  33 134 127  32  35 123  31  21  96  57 132  89]\n",
      "Fold 12, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35  21 123  57 129 109  94  22]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771 0.01754468\n",
      " 0.01747435 0.01446083 0.01321363 0.01310117 0.01289599 0.01222572\n",
      " 0.01219213 0.01106477 0.01089312]\n",
      "Fold 13, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 106  33 134 127  32  35  21 123  57 129 109  94  22]\n",
      "Fold 13, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [ 30 107 106  33 127  32 134  35  22 123 100 108  21 109  96]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319 0.01842883\n",
      " 0.01823597 0.01525073 0.01457077 0.01385602 0.01377492 0.01294186\n",
      " 0.01275241 0.01209829 0.01207726]\n",
      "Fold 14, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 106  33 127  32 134  35  22 123 100 108  21 109  96]\n",
      "Fold 14, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 106 107  33  32 134  35 105 129 127 123 122  90  96  31]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972 0.01836004\n",
      " 0.01680213 0.01447181 0.01430088 0.0139538  0.01331619 0.01156336\n",
      " 0.01147893 0.01138128 0.01122854]\n",
      "Fold 15, VarNum 15, Method RF, Model CART, Selected Features: [ 30 106 107  33  32 134  35 105 129 127 123 122  90  96  31]\n",
      "Fold 15, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5217391304347826\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [ 30 106  32 107  33  35 127 134 105 122  21 123  22 129  31]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613 0.01873923\n",
      " 0.01618912 0.01571743 0.01457547 0.01430517 0.01297532 0.01275019\n",
      " 0.01272992 0.01222445 0.01137464]\n",
      "Fold 16, VarNum 15, Method RF, Model CART, Selected Features: [ 30 106  32 107  33  35 127 134 105 122  21 123  22 129  31]\n",
      "Fold 16, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.35\n",
      "Selected features (RF): [ 30 107 106  33  32 134  35 109 127  21 100 105 122  22 123]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276 0.01956944\n",
      " 0.01569444 0.01428796 0.01372584 0.01366784 0.01229067 0.01214229\n",
      " 0.01193667 0.011559   0.01086449]\n",
      "Fold 17, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 106  33  32 134  35 109 127  21 100 105 122  22 123]\n",
      "Fold 17, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.7\n",
      "Selected features (RF): [ 30 107 134 106  32  33  21  35 127 123 109 105 100 122  25]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419 0.02063558\n",
      " 0.01696546 0.01649607 0.01265368 0.01217766 0.01185532 0.01174153\n",
      " 0.01132177 0.01129399 0.01109355]\n",
      "Fold 18, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 134 106  32  33  21  35 127 123 109 105 100 122  25]\n",
      "Fold 18, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 106  33  32  35  21 127 134 109 100  31 123 131  36]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018 0.01912931\n",
      " 0.01576826 0.01519539 0.01477722 0.0138591  0.01338353 0.01234779\n",
      " 0.01193372 0.01166622 0.01078146]\n",
      "Fold 19, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107 106  33  32  35  21 127 134 109 100  31 123 131  36]\n",
      "Fold 19, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "Selected features (RF): [ 30 107  33  35  32 106  21 127 123 134 109 100 129 122 131]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176 0.01980564\n",
      " 0.01624184 0.01587182 0.0137815  0.01343143 0.01305402 0.01254294\n",
      " 0.01184195 0.01181257 0.01122953]\n",
      "Fold 20, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107  33  35  32 106  21 127 123 134 109 100 129 122 131]\n",
      "Fold 20, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107  32  33  35 106  21 109 127 134 123 105 122 110  90]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627 0.01809367\n",
      " 0.01624739 0.01549509 0.0149973  0.01417063 0.01319223 0.01289244\n",
      " 0.01200101 0.01124886 0.0108701 ]\n",
      "Fold 21, VarNum 15, Method RF, Model CART, Selected Features: [ 30 107  32  33  35 106  21 109 127 134 123 105 122 110  90]\n",
      "Fold 21, VarNum 15, Method RF, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252 0.01557481 0.01491991\n",
      " 0.01489731 0.01448407 0.01424924]\n",
      "Fold 0, VarNum 15, Method RF, Model RF, Selected Features: [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Fold 0, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [129 134 106  30 127 107 131  97  87 123  76 124 102 121 105]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167 0.02039644\n",
      " 0.01602532 0.01587018 0.01557886 0.01551213 0.01518244 0.01426517\n",
      " 0.01395143 0.01385261 0.01367092]\n",
      "Fold 1, VarNum 15, Method RF, Model RF, Selected Features: [129 134 106  30 127 107 131  97  87 123  76 124 102 121 105]\n",
      "Fold 1, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [134 129 107  30 106 127 123 100 102 105 131  87 108 104  89]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617 0.022985\n",
      " 0.01814312 0.01668106 0.01635147 0.0157809  0.01432202 0.01393263\n",
      " 0.01270142 0.01249753 0.01204802]\n",
      "Fold 2, VarNum 15, Method RF, Model RF, Selected Features: [134 129 107  30 106 127 123 100 102 105 131  87 108 104  89]\n",
      "Fold 2, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.8947368421052632\n",
      "Selected features (RF): [129 106 107  30 134 127 123 109 105  32  89  87 131 100  93]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656 0.02107612\n",
      " 0.02026495 0.01998218 0.01919863 0.01749485 0.0166377  0.01642018\n",
      " 0.0161489  0.01350251 0.01300882]\n",
      "Fold 3, VarNum 15, Method RF, Model RF, Selected Features: [129 106 107  30 134 127 123 109 105  32  89  87 131 100  93]\n",
      "Fold 3, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [106 107 129 134  30 127  32 122 123 131 120 109  89 105  94]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219  0.02424004\n",
      " 0.01996565 0.01907085 0.01639677 0.01511131 0.0150213  0.01447112\n",
      " 0.01347839 0.01309091 0.01281931]\n",
      "Fold 4, VarNum 15, Method RF, Model RF, Selected Features: [106 107 129 134  30 127  32 122 123 131 120 109  89 105  94]\n",
      "Fold 4, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [107  30 127 106 129 134 123 109 120 122  32 131  89 118  33]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077 0.02806966\n",
      " 0.02059141 0.01501137 0.01421506 0.01419714 0.0139807  0.01389486\n",
      " 0.01336675 0.0132965  0.01235081]\n",
      "Fold 5, VarNum 15, Method RF, Model RF, Selected Features: [107  30 127 106 129 134 123 109 120 122  32 131  89 118  33]\n",
      "Fold 5, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [134 107 106 127  30 123 129 109 118 122  89  32  33 120  96]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231 0.02580135\n",
      " 0.02334401 0.01569307 0.01390511 0.01283023 0.01279845 0.01251089\n",
      " 0.01217836 0.01197494 0.011698  ]\n",
      "Fold 6, VarNum 15, Method RF, Model RF, Selected Features: [134 107 106 127  30 123 129 109 118 122  89  32  33 120  96]\n",
      "Fold 6, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [107  30 134 127 106 109 123 129 131 133  32  33 100  96 108]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009 0.01869848\n",
      " 0.01837955 0.0174225  0.01470666 0.01373099 0.01369416 0.01369062\n",
      " 0.01305791 0.0128005  0.01204342]\n",
      "Fold 7, VarNum 15, Method RF, Model RF, Selected Features: [107  30 134 127 106 109 123 129 131 133  32  33 100  96 108]\n",
      "Fold 7, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7368421052631579\n",
      "Selected features (RF): [ 30 107 106 134 127 129  32 109  33 123 122 131 108  35  89]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463 0.01950434\n",
      " 0.01929563 0.01788344 0.01579003 0.01488815 0.01444204 0.01390951\n",
      " 0.01369903 0.0124751  0.01192624]\n",
      "Fold 8, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 106 134 127 129  32 109  33 123 122 131 108  35  89]\n",
      "Fold 8, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 106 134 127  33 129  32  35 123 108 109  22 131  98]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201 0.01766004\n",
      " 0.0159082  0.01583366 0.01568882 0.01516106 0.01415489 0.01263284\n",
      " 0.01213981 0.01178665 0.01175603]\n",
      "Fold 9, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 106 134 127  33 129  32  35 123 108 109  22 131  98]\n",
      "Fold 9, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [ 30 107 106 134 127  33  35  32 123 129 109  90  96  21 108]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542 0.01950735\n",
      " 0.01673918 0.01632669 0.01592091 0.01496493 0.01466358 0.01431061\n",
      " 0.01368107 0.01287874 0.01240728]\n",
      "Fold 10, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 106 134 127  33  35  32 123 129 109  90  96  21 108]\n",
      "Fold 10, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 134  33 106 127  35  32 129  31 123 108  57  96 109]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717 0.01998843\n",
      " 0.01947537 0.01675318 0.01650051 0.01521515 0.01391107 0.01209299\n",
      " 0.01157108 0.01148276 0.01021131]\n",
      "Fold 11, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 134  33 106 127  35  32 129  31 123 108  57  96 109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 11, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35 123  31  21  96  57 132  89]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571 0.01720357\n",
      " 0.01623954 0.01578945 0.01415767 0.01344477 0.01321165 0.01152826\n",
      " 0.01145633 0.01114404 0.01105053]\n",
      "Fold 12, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 106  33 134 127  32  35 123  31  21  96  57 132  89]\n",
      "Fold 12, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35  21 123  57 129 109  94  22]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771 0.01754468\n",
      " 0.01747435 0.01446083 0.01321363 0.01310117 0.01289599 0.01222572\n",
      " 0.01219213 0.01106477 0.01089312]\n",
      "Fold 13, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 106  33 134 127  32  35  21 123  57 129 109  94  22]\n",
      "Fold 13, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 106  33 127  32 134  35  22 123 100 108  21 109  96]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319 0.01842883\n",
      " 0.01823597 0.01525073 0.01457077 0.01385602 0.01377492 0.01294186\n",
      " 0.01275241 0.01209829 0.01207726]\n",
      "Fold 14, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 106  33 127  32 134  35  22 123 100 108  21 109  96]\n",
      "Fold 14, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (RF): [ 30 106 107  33  32 134  35 105 129 127 123 122  90  96  31]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972 0.01836004\n",
      " 0.01680213 0.01447181 0.01430088 0.0139538  0.01331619 0.01156336\n",
      " 0.01147893 0.01138128 0.01122854]\n",
      "Fold 15, VarNum 15, Method RF, Model RF, Selected Features: [ 30 106 107  33  32 134  35 105 129 127 123 122  90  96  31]\n",
      "Fold 15, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [ 30 106  32 107  33  35 127 134 105 122  21 123  22 129  31]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613 0.01873923\n",
      " 0.01618912 0.01571743 0.01457547 0.01430517 0.01297532 0.01275019\n",
      " 0.01272992 0.01222445 0.01137464]\n",
      "Fold 16, VarNum 15, Method RF, Model RF, Selected Features: [ 30 106  32 107  33  35 127 134 105 122  21 123  22 129  31]\n",
      "Fold 16, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.75\n",
      "Selected features (RF): [ 30 107 106  33  32 134  35 109 127  21 100 105 122  22 123]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276 0.01956944\n",
      " 0.01569444 0.01428796 0.01372584 0.01366784 0.01229067 0.01214229\n",
      " 0.01193667 0.011559   0.01086449]\n",
      "Fold 17, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 106  33  32 134  35 109 127  21 100 105 122  22 123]\n",
      "Fold 17, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.8\n",
      "Selected features (RF): [ 30 107 134 106  32  33  21  35 127 123 109 105 100 122  25]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419 0.02063558\n",
      " 0.01696546 0.01649607 0.01265368 0.01217766 0.01185532 0.01174153\n",
      " 0.01132177 0.01129399 0.01109355]\n",
      "Fold 18, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 134 106  32  33  21  35 127 123 109 105 100 122  25]\n",
      "Fold 18, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 106  33  32  35  21 127 134 109 100  31 123 131  36]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018 0.01912931\n",
      " 0.01576826 0.01519539 0.01477722 0.0138591  0.01338353 0.01234779\n",
      " 0.01193372 0.01166622 0.01078146]\n",
      "Fold 19, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107 106  33  32  35  21 127 134 109 100  31 123 131  36]\n",
      "Fold 19, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "Selected features (RF): [ 30 107  33  35  32 106  21 127 123 134 109 100 129 122 131]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176 0.01980564\n",
      " 0.01624184 0.01587182 0.0137815  0.01343143 0.01305402 0.01254294\n",
      " 0.01184195 0.01181257 0.01122953]\n",
      "Fold 20, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107  33  35  32 106  21 127 123 134 109 100 129 122 131]\n",
      "Fold 20, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107  32  33  35 106  21 109 127 134 123 105 122 110  90]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627 0.01809367\n",
      " 0.01624739 0.01549509 0.0149973  0.01417063 0.01319223 0.01289244\n",
      " 0.01200101 0.01124886 0.0108701 ]\n",
      "Fold 21, VarNum 15, Method RF, Model RF, Selected Features: [ 30 107  32  33  35 106  21 109 127 134 123 105 122 110  90]\n",
      "Fold 21, VarNum 15, Method RF, Model RF, Train Acc: 1.0, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252 0.01557481 0.01491991\n",
      " 0.01489731 0.01448407 0.01424924]\n",
      "Fold 0, VarNum 15, Method RF, Model XGB, Selected Features: [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Fold 0, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [129 134 106  30 127 107 131  97  87 123  76 124 102 121 105]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167 0.02039644\n",
      " 0.01602532 0.01587018 0.01557886 0.01551213 0.01518244 0.01426517\n",
      " 0.01395143 0.01385261 0.01367092]\n",
      "Fold 1, VarNum 15, Method RF, Model XGB, Selected Features: [129 134 106  30 127 107 131  97  87 123  76 124 102 121 105]\n",
      "Fold 1, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5652173913043478\n",
      "Selected features (RF): [134 129 107  30 106 127 123 100 102 105 131  87 108 104  89]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617 0.022985\n",
      " 0.01814312 0.01668106 0.01635147 0.0157809  0.01432202 0.01393263\n",
      " 0.01270142 0.01249753 0.01204802]\n",
      "Fold 2, VarNum 15, Method RF, Model XGB, Selected Features: [134 129 107  30 106 127 123 100 102 105 131  87 108 104  89]\n",
      "Fold 2, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [129 106 107  30 134 127 123 109 105  32  89  87 131 100  93]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656 0.02107612\n",
      " 0.02026495 0.01998218 0.01919863 0.01749485 0.0166377  0.01642018\n",
      " 0.0161489  0.01350251 0.01300882]\n",
      "Fold 3, VarNum 15, Method RF, Model XGB, Selected Features: [129 106 107  30 134 127 123 109 105  32  89  87 131 100  93]\n",
      "Fold 3, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "Selected features (RF): [106 107 129 134  30 127  32 122 123 131 120 109  89 105  94]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219  0.02424004\n",
      " 0.01996565 0.01907085 0.01639677 0.01511131 0.0150213  0.01447112\n",
      " 0.01347839 0.01309091 0.01281931]\n",
      "Fold 4, VarNum 15, Method RF, Model XGB, Selected Features: [106 107 129 134  30 127  32 122 123 131 120 109  89 105  94]\n",
      "Fold 4, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [107  30 127 106 129 134 123 109 120 122  32 131  89 118  33]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077 0.02806966\n",
      " 0.02059141 0.01501137 0.01421506 0.01419714 0.0139807  0.01389486\n",
      " 0.01336675 0.0132965  0.01235081]\n",
      "Fold 5, VarNum 15, Method RF, Model XGB, Selected Features: [107  30 127 106 129 134 123 109 120 122  32 131  89 118  33]\n",
      "Fold 5, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [134 107 106 127  30 123 129 109 118 122  89  32  33 120  96]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231 0.02580135\n",
      " 0.02334401 0.01569307 0.01390511 0.01283023 0.01279845 0.01251089\n",
      " 0.01217836 0.01197494 0.011698  ]\n",
      "Fold 6, VarNum 15, Method RF, Model XGB, Selected Features: [134 107 106 127  30 123 129 109 118 122  89  32  33 120  96]\n",
      "Fold 6, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.7727272727272727\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [107  30 134 127 106 109 123 129 131 133  32  33 100  96 108]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009 0.01869848\n",
      " 0.01837955 0.0174225  0.01470666 0.01373099 0.01369416 0.01369062\n",
      " 0.01305791 0.0128005  0.01204342]\n",
      "Fold 7, VarNum 15, Method RF, Model XGB, Selected Features: [107  30 134 127 106 109 123 129 131 133  32  33 100  96 108]\n",
      "Fold 7, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106 134 127 129  32 109  33 123 122 131 108  35  89]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463 0.01950434\n",
      " 0.01929563 0.01788344 0.01579003 0.01488815 0.01444204 0.01390951\n",
      " 0.01369903 0.0124751  0.01192624]\n",
      "Fold 8, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127 129  32 109  33 123 122 131 108  35  89]\n",
      "Fold 8, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [ 30 107 106 134 127  33 129  32  35 123 108 109  22 131  98]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201 0.01766004\n",
      " 0.0159082  0.01583366 0.01568882 0.01516106 0.01415489 0.01263284\n",
      " 0.01213981 0.01178665 0.01175603]\n",
      "Fold 9, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127  33 129  32  35 123 108 109  22 131  98]\n",
      "Fold 9, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "Selected features (RF): [ 30 107 106 134 127  33  35  32 123 129 109  90  96  21 108]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542 0.01950735\n",
      " 0.01673918 0.01632669 0.01592091 0.01496493 0.01466358 0.01431061\n",
      " 0.01368107 0.01287874 0.01240728]\n",
      "Fold 10, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 106 134 127  33  35  32 123 129 109  90  96  21 108]\n",
      "Fold 10, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "Selected features (RF): [ 30 107 134  33 106 127  35  32 129  31 123 108  57  96 109]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717 0.01998843\n",
      " 0.01947537 0.01675318 0.01650051 0.01521515 0.01391107 0.01209299\n",
      " 0.01157108 0.01148276 0.01021131]\n",
      "Fold 11, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 134  33 106 127  35  32 129  31 123 108  57  96 109]\n",
      "Fold 11, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35 123  31  21  96  57 132  89]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571 0.01720357\n",
      " 0.01623954 0.01578945 0.01415767 0.01344477 0.01321165 0.01152826\n",
      " 0.01145633 0.01114404 0.01105053]\n",
      "Fold 12, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 106  33 134 127  32  35 123  31  21  96  57 132  89]\n",
      "Fold 12, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35  21 123  57 129 109  94  22]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771 0.01754468\n",
      " 0.01747435 0.01446083 0.01321363 0.01310117 0.01289599 0.01222572\n",
      " 0.01219213 0.01106477 0.01089312]\n",
      "Fold 13, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 106  33 134 127  32  35  21 123  57 129 109  94  22]\n",
      "Fold 13, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "Selected features (RF): [ 30 107 106  33 127  32 134  35  22 123 100 108  21 109  96]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319 0.01842883\n",
      " 0.01823597 0.01525073 0.01457077 0.01385602 0.01377492 0.01294186\n",
      " 0.01275241 0.01209829 0.01207726]\n",
      "Fold 14, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 106  33 127  32 134  35  22 123 100 108  21 109  96]\n",
      "Fold 14, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 106 107  33  32 134  35 105 129 127 123 122  90  96  31]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972 0.01836004\n",
      " 0.01680213 0.01447181 0.01430088 0.0139538  0.01331619 0.01156336\n",
      " 0.01147893 0.01138128 0.01122854]\n",
      "Fold 15, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 106 107  33  32 134  35 105 129 127 123 122  90  96  31]\n",
      "Fold 15, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "Selected features (RF): [ 30 106  32 107  33  35 127 134 105 122  21 123  22 129  31]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613 0.01873923\n",
      " 0.01618912 0.01571743 0.01457547 0.01430517 0.01297532 0.01275019\n",
      " 0.01272992 0.01222445 0.01137464]\n",
      "Fold 16, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 106  32 107  33  35 127 134 105 122  21 123  22 129  31]\n",
      "Fold 16, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (RF): [ 30 107 106  33  32 134  35 109 127  21 100 105 122  22 123]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276 0.01956944\n",
      " 0.01569444 0.01428796 0.01372584 0.01366784 0.01229067 0.01214229\n",
      " 0.01193667 0.011559   0.01086449]\n",
      "Fold 17, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 106  33  32 134  35 109 127  21 100 105 122  22 123]\n",
      "Fold 17, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "Selected features (RF): [ 30 107 134 106  32  33  21  35 127 123 109 105 100 122  25]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419 0.02063558\n",
      " 0.01696546 0.01649607 0.01265368 0.01217766 0.01185532 0.01174153\n",
      " 0.01132177 0.01129399 0.01109355]\n",
      "Fold 18, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 134 106  32  33  21  35 127 123 109 105 100 122  25]\n",
      "Fold 18, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30 107 106  33  32  35  21 127 134 109 100  31 123 131  36]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018 0.01912931\n",
      " 0.01576826 0.01519539 0.01477722 0.0138591  0.01338353 0.01234779\n",
      " 0.01193372 0.01166622 0.01078146]\n",
      "Fold 19, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107 106  33  32  35  21 127 134 109 100  31 123 131  36]\n",
      "Fold 19, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107  33  35  32 106  21 127 123 134 109 100 129 122 131]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176 0.01980564\n",
      " 0.01624184 0.01587182 0.0137815  0.01343143 0.01305402 0.01254294\n",
      " 0.01184195 0.01181257 0.01122953]\n",
      "Fold 20, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107  33  35  32 106  21 127 123 134 109 100 129 122 131]\n",
      "Fold 20, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107  32  33  35 106  21 109 127 134 123 105 122 110  90]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627 0.01809367\n",
      " 0.01624739 0.01549509 0.0149973  0.01417063 0.01319223 0.01289244\n",
      " 0.01200101 0.01124886 0.0108701 ]\n",
      "Fold 21, VarNum 15, Method RF, Model XGB, Selected Features: [ 30 107  32  33  35 106  21 109 127 134 123 105 122 110  90]\n",
      "Fold 21, VarNum 15, Method RF, Model XGB, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "Selected features (RF): [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Feature importances: [0.02979938 0.02638811 0.02477297 0.02445759 0.02369387 0.02131219\n",
      " 0.01807298 0.01765663 0.01580206 0.01562252 0.01557481 0.01491991\n",
      " 0.01489731 0.01448407 0.01424924]\n",
      "Fold 0, VarNum 15, Method RF, Model LDA, Selected Features: [129 134 106  30 107 105  43 123 100  96  94 102 127 108 124]\n",
      "Fold 0, VarNum 15, Method RF, Model LDA, Train Acc: 0.728, Test Acc: 0.6666666666666666\n",
      "Selected features (RF): [129 134 106  30 127 107 131  97  87 123  76 124 102 121 105]\n",
      "Feature importances: [0.03060411 0.02873798 0.02685792 0.02369497 0.02278167 0.02039644\n",
      " 0.01602532 0.01587018 0.01557886 0.01551213 0.01518244 0.01426517\n",
      " 0.01395143 0.01385261 0.01367092]\n",
      "Fold 1, VarNum 15, Method RF, Model LDA, Selected Features: [129 134 106  30 127 107 131  97  87 123  76 124 102 121 105]\n",
      "Fold 1, VarNum 15, Method RF, Model LDA, Train Acc: 0.7397260273972602, Test Acc: 0.7391304347826086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [134 129 107  30 106 127 123 100 102 105 131  87 108 104  89]\n",
      "Feature importances: [0.0314324  0.02947542 0.02747998 0.02633576 0.02577617 0.022985\n",
      " 0.01814312 0.01668106 0.01635147 0.0157809  0.01432202 0.01393263\n",
      " 0.01270142 0.01249753 0.01204802]\n",
      "Fold 2, VarNum 15, Method RF, Model LDA, Selected Features: [134 129 107  30 106 127 123 100 102 105 131  87 108 104  89]\n",
      "Fold 2, VarNum 15, Method RF, Model LDA, Train Acc: 0.727810650887574, Test Acc: 0.8421052631578947\n",
      "Selected features (RF): [129 106 107  30 134 127 123 109 105  32  89  87 131 100  93]\n",
      "Feature importances: [0.03617946 0.03200665 0.02957268 0.02700925 0.02588656 0.02107612\n",
      " 0.02026495 0.01998218 0.01919863 0.01749485 0.0166377  0.01642018\n",
      " 0.0161489  0.01350251 0.01300882]\n",
      "Fold 3, VarNum 15, Method RF, Model LDA, Selected Features: [129 106 107  30 134 127 123 109 105  32  89  87 131 100  93]\n",
      "Fold 3, VarNum 15, Method RF, Model LDA, Train Acc: 0.75, Test Acc: 0.6956521739130435\n",
      "Selected features (RF): [106 107 129 134  30 127  32 122 123 131 120 109  89 105  94]\n",
      "Feature importances: [0.03187074 0.03179959 0.03067957 0.02750244 0.0274219  0.02424004\n",
      " 0.01996565 0.01907085 0.01639677 0.01511131 0.0150213  0.01447112\n",
      " 0.01347839 0.01309091 0.01281931]\n",
      "Fold 4, VarNum 15, Method RF, Model LDA, Selected Features: [106 107 129 134  30 127  32 122 123 131 120 109  89 105  94]\n",
      "Fold 4, VarNum 15, Method RF, Model LDA, Train Acc: 0.7298578199052133, Test Acc: 0.5238095238095238\n",
      "Selected features (RF): [107  30 127 106 129 134 123 109 120 122  32 131  89 118  33]\n",
      "Feature importances: [0.03507982 0.0334233  0.03234858 0.03198341 0.02911077 0.02806966\n",
      " 0.02059141 0.01501137 0.01421506 0.01419714 0.0139807  0.01389486\n",
      " 0.01336675 0.0132965  0.01235081]\n",
      "Fold 5, VarNum 15, Method RF, Model LDA, Selected Features: [107  30 127 106 129 134 123 109 120 122  32 131  89 118  33]\n",
      "Fold 5, VarNum 15, Method RF, Model LDA, Train Acc: 0.728448275862069, Test Acc: 0.5789473684210527\n",
      "Selected features (RF): [134 107 106 127  30 123 129 109 118 122  89  32  33 120  96]\n",
      "Feature importances: [0.03800635 0.0371001  0.03219007 0.03011992 0.02874231 0.02580135\n",
      " 0.02334401 0.01569307 0.01390511 0.01283023 0.01279845 0.01251089\n",
      " 0.01217836 0.01197494 0.011698  ]\n",
      "Fold 6, VarNum 15, Method RF, Model LDA, Selected Features: [134 107 106 127  30 123 129 109 118 122  89  32  33 120  96]\n",
      "Fold 6, VarNum 15, Method RF, Model LDA, Train Acc: 0.701195219123506, Test Acc: 0.7272727272727273\n",
      "Selected features (RF): [107  30 134 127 106 109 123 129 131 133  32  33 100  96 108]\n",
      "Feature importances: [0.03741965 0.03320219 0.02976549 0.02690499 0.02542009 0.01869848\n",
      " 0.01837955 0.0174225  0.01470666 0.01373099 0.01369416 0.01369062\n",
      " 0.01305791 0.0128005  0.01204342]\n",
      "Fold 7, VarNum 15, Method RF, Model LDA, Selected Features: [107  30 134 127 106 109 123 129 131 133  32  33 100  96 108]\n",
      "Fold 7, VarNum 15, Method RF, Model LDA, Train Acc: 0.7106227106227107, Test Acc: 0.6842105263157895\n",
      "Selected features (RF): [ 30 107 106 134 127 129  32 109  33 123 122 131 108  35  89]\n",
      "Feature importances: [0.04425214 0.03810914 0.03055344 0.02858422 0.02541463 0.01950434\n",
      " 0.01929563 0.01788344 0.01579003 0.01488815 0.01444204 0.01390951\n",
      " 0.01369903 0.0124751  0.01192624]\n",
      "Fold 8, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127 129  32 109  33 123 122 131 108  35  89]\n",
      "Fold 8, VarNum 15, Method RF, Model LDA, Train Acc: 0.7328767123287672, Test Acc: 0.6190476190476191\n",
      "Selected features (RF): [ 30 107 106 134 127  33 129  32  35 123 108 109  22 131  98]\n",
      "Feature importances: [0.044955   0.03518582 0.02833876 0.02647672 0.02248201 0.01766004\n",
      " 0.0159082  0.01583366 0.01568882 0.01516106 0.01415489 0.01263284\n",
      " 0.01213981 0.01178665 0.01175603]\n",
      "Fold 9, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127  33 129  32  35 123 108 109  22 131  98]\n",
      "Fold 9, VarNum 15, Method RF, Model LDA, Train Acc: 0.7188498402555911, Test Acc: 0.5714285714285714\n",
      "Selected features (RF): [ 30 107 106 134 127  33  35  32 123 129 109  90  96  21 108]\n",
      "Feature importances: [0.04318349 0.03125439 0.02841659 0.02673746 0.02172542 0.01950735\n",
      " 0.01673918 0.01632669 0.01592091 0.01496493 0.01466358 0.01431061\n",
      " 0.01368107 0.01287874 0.01240728]\n",
      "Fold 10, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 106 134 127  33  35  32 123 129 109  90  96  21 108]\n",
      "Fold 10, VarNum 15, Method RF, Model LDA, Train Acc: 0.7095808383233533, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30 107 134  33 106 127  35  32 129  31 123 108  57  96 109]\n",
      "Feature importances: [0.03979341 0.02793968 0.02758033 0.02556865 0.02368717 0.01998843\n",
      " 0.01947537 0.01675318 0.01650051 0.01521515 0.01391107 0.01209299\n",
      " 0.01157108 0.01148276 0.01021131]\n",
      "Fold 11, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 134  33 106 127  35  32 129  31 123 108  57  96 109]\n",
      "Fold 11, VarNum 15, Method RF, Model LDA, Train Acc: 0.7303370786516854, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35 123  31  21  96  57 132  89]\n",
      "Feature importances: [0.04250741 0.03145967 0.02699476 0.02659428 0.02423571 0.01720357\n",
      " 0.01623954 0.01578945 0.01415767 0.01344477 0.01321165 0.01152826\n",
      " 0.01145633 0.01114404 0.01105053]\n",
      "Fold 12, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 106  33 134 127  32  35 123  31  21  96  57 132  89]\n",
      "Fold 12, VarNum 15, Method RF, Model LDA, Train Acc: 0.7253333333333334, Test Acc: 0.5909090909090909\n",
      "Selected features (RF): [ 30 107 106  33 134 127  32  35  21 123  57 129 109  94  22]\n",
      "Feature importances: [0.04251799 0.03071221 0.02732726 0.02672395 0.02108771 0.01754468\n",
      " 0.01747435 0.01446083 0.01321363 0.01310117 0.01289599 0.01222572\n",
      " 0.01219213 0.01106477 0.01089312]\n",
      "Fold 13, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 106  33 134 127  32  35  21 123  57 129 109  94  22]\n",
      "Fold 13, VarNum 15, Method RF, Model LDA, Train Acc: 0.7103274559193955, Test Acc: 0.6818181818181818\n",
      "Selected features (RF): [ 30 107 106  33 127  32 134  35  22 123 100 108  21 109  96]\n",
      "Feature importances: [0.04001411 0.03147164 0.02924278 0.02637217 0.01932319 0.01842883\n",
      " 0.01823597 0.01525073 0.01457077 0.01385602 0.01377492 0.01294186\n",
      " 0.01275241 0.01209829 0.01207726]\n",
      "Fold 14, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 106  33 127  32 134  35  22 123 100 108  21 109  96]\n",
      "Fold 14, VarNum 15, Method RF, Model LDA, Train Acc: 0.6992840095465394, Test Acc: 0.6\n",
      "Selected features (RF): [ 30 106 107  33  32 134  35 105 129 127 123 122  90  96  31]\n",
      "Feature importances: [0.03790317 0.02858349 0.02848051 0.02216483 0.01993972 0.01836004\n",
      " 0.01680213 0.01447181 0.01430088 0.0139538  0.01331619 0.01156336\n",
      " 0.01147893 0.01138128 0.01122854]\n",
      "Fold 15, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 106 107  33  32 134  35 105 129 127 123 122  90  96  31]\n",
      "Fold 15, VarNum 15, Method RF, Model LDA, Train Acc: 0.7425968109339408, Test Acc: 0.7391304347826086\n",
      "Selected features (RF): [ 30 106  32 107  33  35 127 134 105 122  21 123  22 129  31]\n",
      "Feature importances: [0.03810158 0.02611623 0.02548277 0.02478636 0.02221613 0.01873923\n",
      " 0.01618912 0.01571743 0.01457547 0.01430517 0.01297532 0.01275019\n",
      " 0.01272992 0.01222445 0.01137464]\n",
      "Fold 16, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 106  32 107  33  35 127 134 105 122  21 123  22 129  31]\n",
      "Fold 16, VarNum 15, Method RF, Model LDA, Train Acc: 0.7380952380952381, Test Acc: 0.75\n",
      "Selected features (RF): [ 30 107 106  33  32 134  35 109 127  21 100 105 122  22 123]\n",
      "Feature importances: [0.03858583 0.02883702 0.02458557 0.02198017 0.02139276 0.01956944\n",
      " 0.01569444 0.01428796 0.01372584 0.01366784 0.01229067 0.01214229\n",
      " 0.01193667 0.011559   0.01086449]\n",
      "Fold 17, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 106  33  32 134  35 109 127  21 100 105 122  22 123]\n",
      "Fold 17, VarNum 15, Method RF, Model LDA, Train Acc: 0.7095435684647303, Test Acc: 0.65\n",
      "Selected features (RF): [ 30 107 134 106  32  33  21  35 127 123 109 105 100 122  25]\n",
      "Feature importances: [0.03705865 0.0293689  0.02382771 0.02369186 0.02182419 0.02063558\n",
      " 0.01696546 0.01649607 0.01265368 0.01217766 0.01185532 0.01174153\n",
      " 0.01132177 0.01129399 0.01109355]\n",
      "Fold 18, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 134 106  32  33  21  35 127 123 109 105 100 122  25]\n",
      "Fold 18, VarNum 15, Method RF, Model LDA, Train Acc: 0.7111553784860558, Test Acc: 0.5454545454545454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (RF): [ 30 107 106  33  32  35  21 127 134 109 100  31 123 131  36]\n",
      "Feature importances: [0.03275134 0.02515434 0.0251478  0.0220639  0.01916018 0.01912931\n",
      " 0.01576826 0.01519539 0.01477722 0.0138591  0.01338353 0.01234779\n",
      " 0.01193372 0.01166622 0.01078146]\n",
      "Fold 19, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107 106  33  32  35  21 127 134 109 100  31 123 131  36]\n",
      "Fold 19, VarNum 15, Method RF, Model LDA, Train Acc: 0.7404580152671756, Test Acc: 0.631578947368421\n",
      "Selected features (RF): [ 30 107  33  35  32 106  21 127 123 134 109 100 129 122 131]\n",
      "Feature importances: [0.03378237 0.02339782 0.02170517 0.02139228 0.01985176 0.01980564\n",
      " 0.01624184 0.01587182 0.0137815  0.01343143 0.01305402 0.01254294\n",
      " 0.01184195 0.01181257 0.01122953]\n",
      "Fold 20, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107  33  35  32 106  21 127 123 134 109 100 129 122 131]\n",
      "Fold 20, VarNum 15, Method RF, Model LDA, Train Acc: 0.6906077348066298, Test Acc: 0.7142857142857143\n",
      "Selected features (RF): [ 30 107  32  33  35 106  21 109 127 134 123 105 122 110  90]\n",
      "Feature importances: [0.04267411 0.02503233 0.0220347  0.02141431 0.02134627 0.01809367\n",
      " 0.01624739 0.01549509 0.0149973  0.01417063 0.01319223 0.01289244\n",
      " 0.01200101 0.01124886 0.0108701 ]\n",
      "Fold 21, VarNum 15, Method RF, Model LDA, Selected Features: [ 30 107  32  33  35 106  21 109 127 134 123 105 122 110  90]\n",
      "Fold 21, VarNum 15, Method RF, Model LDA, Train Acc: 0.7021276595744681, Test Acc: 0.6363636363636364\n",
      "[15:27:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594 0.02863571 0.02685573\n",
      " 0.02517813 0.02097734 0.01903709]\n",
      "Fold 0, VarNum 15, Method XGB, Model CART, Selected Features: [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Fold 0, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:27:27] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100  99 133 134  15 135  34  95  87 121   5]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001 0.03824509\n",
      " 0.03144442 0.03067115 0.02586752 0.02579053 0.0241793  0.02340247\n",
      " 0.02311029 0.02206623 0.02039631]\n",
      "Fold 1, VarNum 15, Method XGB, Model CART, Selected Features: [  1 114  92  79 100  99 133 134  15 135  34  95  87 121   5]\n",
      "Fold 1, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "[15:27:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119 118  83  97 134  89 106   0  32  92  30]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364  0.03260981\n",
      " 0.02879154 0.02842961 0.02837311 0.02583595 0.02269325 0.02135309\n",
      " 0.01880933 0.01693355 0.01669514]\n",
      "Fold 2, VarNum 15, Method XGB, Model CART, Selected Features: [ 26  72  77  46 119 118  83  97 134  89 106   0  32  92  30]\n",
      "Fold 2, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:27:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 59  25  46   0  61 106  99 134  30  97 113  32 118 133  89]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437 0.02968394\n",
      " 0.02835124 0.02730615 0.02658134 0.02494756 0.02389958 0.0223331\n",
      " 0.02197508 0.02195759 0.02179354]\n",
      "Fold 3, VarNum 15, Method XGB, Model CART, Selected Features: [ 59  25  46   0  61 106  99 134  30  97 113  32 118 133  89]\n",
      "Fold 3, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6086956521739131\n",
      "[15:27:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25  32 107 114  64  35  99 125  72 115  97]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888 0.02571377\n",
      " 0.02553768 0.02345947 0.02330734 0.02305957 0.02195241 0.01955295\n",
      " 0.01902484 0.01853873 0.01843479]\n",
      "Fold 4, VarNum 15, Method XGB, Model CART, Selected Features: [ 91 106  11 134  25  32 107 114  64  35  99 125  72 115  97]\n",
      "Fold 4, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:27:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45  47  25 106  32 135  89  52  68  34  42]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451 0.03064233\n",
      " 0.02763383 0.02525128 0.02185246 0.02143421 0.02086745 0.0201348\n",
      " 0.01876054 0.01809891 0.01793046]\n",
      "Fold 5, VarNum 15, Method XGB, Model CART, Selected Features: [ 74  99 134 119  45  47  25 106  32 135  89  52  68  34  42]\n",
      "Fold 5, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:27:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45  64  95 134 106  92 116 130 127  74  32]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231 0.02633443\n",
      " 0.02538574 0.02404596 0.02323602 0.0198393  0.01787091 0.01744338\n",
      " 0.01704738 0.01699063 0.01666635]\n",
      "Fold 6, VarNum 15, Method XGB, Model CART, Selected Features: [107  99  17  21  45  64  95 134 106  92 116 130 127  74  32]\n",
      "Fold 6, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.36363636363636365\n",
      "[15:27:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40 134 127  45  25   1 113  22 118  28  30]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901 0.0272389\n",
      " 0.02550623 0.02285499 0.02245414 0.02209218 0.02101223 0.02079713\n",
      " 0.01824612 0.01747868 0.01745014]\n",
      "Fold 7, VarNum 15, Method XGB, Model CART, Selected Features: [107  99  66 130  40 134 127  45  25   1 113  22 118  28  30]\n",
      "Fold 7, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "[15:27:28] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30  18 112  85 134 135  44 123  63  98 130]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053  0.0235923\n",
      " 0.02342531 0.02282099 0.02216369 0.02056672 0.01886323 0.01829717\n",
      " 0.01805966 0.01743061 0.01719207]\n",
      "Fold 8, VarNum 15, Method XGB, Model CART, Selected Features: [107  22  65  60  30  18 112  85 134 135  44 123  63  98 130]\n",
      "Fold 8, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "[15:27:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54  12  18  30  68  43  57  70  63 134  69]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297  0.02258516\n",
      " 0.02081037 0.02071864 0.01912442 0.01802449 0.01777671 0.01766494\n",
      " 0.01684872 0.01670655 0.01649962]\n",
      "Fold 9, VarNum 15, Method XGB, Model CART, Selected Features: [107  99   0  21  54  12  18  30  68  43  57  70  63 134  69]\n",
      "Fold 9, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "[15:27:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 70 107  85  87 110  30  63  45 117 134  21  99  49  93 121]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609 0.02446463\n",
      " 0.02375421 0.02277838 0.01852702 0.01821284 0.01779496 0.01685031\n",
      " 0.01667458 0.0164859  0.01594789]\n",
      "Fold 10, VarNum 15, Method XGB, Model CART, Selected Features: [ 70 107  85  87 110  30  63  45 117 134  21  99  49  93 121]\n",
      "Fold 10, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6363636363636364\n",
      "[15:27:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85  39  49  30  15  59  75  21  83  38  82]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574  0.02708106\n",
      " 0.02700011 0.0267045  0.02186688 0.01906396 0.01798938 0.01762597\n",
      " 0.01718468 0.01664497 0.01624968]\n",
      "Fold 11, VarNum 15, Method XGB, Model CART, Selected Features: [ 99  12 106  93  85  39  49  30  15  59  75  21  83  38  82]\n",
      "Fold 11, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:27:29] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20  38  99 124  93  44  56  92  94 136  85]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579 0.02196426\n",
      " 0.02097165 0.02021421 0.02020423 0.01939055 0.0182912  0.0177101\n",
      " 0.0172697  0.01662818 0.01616009]\n",
      "Fold 12, VarNum 15, Method XGB, Model CART, Selected Features: [ 27 107  74  30  20  38  99 124  93  44  56  92  94 136  85]\n",
      "Fold 12, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:27:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30  21  50  63  62 131  72  76  69  99  89]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133 0.02203631\n",
      " 0.02164648 0.01945004 0.01928781 0.01914054 0.01795315 0.01788961\n",
      " 0.0161291  0.01594377 0.01564425]\n",
      "Fold 13, VarNum 15, Method XGB, Model CART, Selected Features: [107  78  84  45  30  21  50  63  62 131  72  76  69  99  89]\n",
      "Fold 13, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:27:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94  99  30  71 132  24  22 116  65  68  72]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958 0.02117782\n",
      " 0.02106225 0.0206327  0.01638661 0.01580617 0.01546936 0.01492459\n",
      " 0.01461546 0.01459011 0.01432451]\n",
      "Fold 14, VarNum 15, Method XGB, Model CART, Selected Features: [107 106 112  51  94  99  30  71 132  24  22 116  65  68  72]\n",
      "Fold 14, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:27:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45  29  85  33   3 105  40 136 131  55   9]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043 0.02049493\n",
      " 0.02032613 0.01902149 0.01853412 0.01766204 0.01700879 0.01694755\n",
      " 0.01654983 0.015752   0.01557628]\n",
      "Fold 15, VarNum 15, Method XGB, Model CART, Selected Features: [106  93 119  30  45  29  85  33   3 105  40 136 131  55   9]\n",
      "Fold 15, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "[15:27:30] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22  93  40 114  63 107  35  94  23  45  42]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031 0.01834612\n",
      " 0.01832638 0.01660874 0.0163494  0.01607329 0.01531808 0.01473181\n",
      " 0.01466983 0.01447087 0.01388538]\n",
      "Fold 16, VarNum 15, Method XGB, Model CART, Selected Features: [ 28  30 106  85  22  93  40 114  63 107  35  94  23  45  42]\n",
      "Fold 16, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4\n",
      "[15:27:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93  84  30  26  67  49 108  38 113  86  94]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613 0.02621297\n",
      " 0.02436594 0.02368471 0.02286324 0.01924192 0.01689295 0.01632198\n",
      " 0.01574784 0.01483444 0.01414419]\n",
      "Fold 17, VarNum 15, Method XGB, Model CART, Selected Features: [106  45  40  18  93  84  30  26  67  49 108  38 113  86  94]\n",
      "Fold 17, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:27:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30  94  93   1  77  85  79 134  40 112 133]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267 0.02270854\n",
      " 0.02157388 0.02096329 0.0197053  0.01822887 0.01821537 0.0178174\n",
      " 0.01676521 0.01570135 0.01401956]\n",
      "Fold 18, VarNum 15, Method XGB, Model CART, Selected Features: [ 18  73 106 107  30  94  93   1  77  85  79 134  40 112 133]\n",
      "Fold 18, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "[15:27:31] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114  13  30  65  71  26 107 106 133  24 126]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243 0.02239063\n",
      " 0.02224347 0.02035736 0.02010566 0.01846889 0.01824656 0.01770644\n",
      " 0.01687778 0.01592533 0.01554673]\n",
      "Fold 19, VarNum 15, Method XGB, Model CART, Selected Features: [ 40  76  17  85 114  13  30  65  71  26 107 106 133  24 126]\n",
      "Fold 19, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.42105263157894735\n",
      "[15:27:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106  40  34  71 121  87  69  74 101  68  28]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375 0.01896119\n",
      " 0.01791704 0.01761022 0.01748885 0.01717754 0.01426151 0.01417943\n",
      " 0.01389764 0.01375816 0.01346007]\n",
      "Fold 20, VarNum 15, Method XGB, Model CART, Selected Features: [ 46  18 114  30 106  40  34  71 121  87  69  74 101  68  28]\n",
      "Fold 20, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.47619047619047616\n",
      "[15:27:32] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 76  40  64  30  36 114  85 106  32   7 121  45  42 107  55]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063 0.02041966\n",
      " 0.01861721 0.01584065 0.01534018 0.01502906 0.01482452 0.01470072\n",
      " 0.01435008 0.01427137 0.01418887]\n",
      "Fold 21, VarNum 15, Method XGB, Model CART, Selected Features: [ 76  40  64  30  36 114  85 106  32   7 121  45  42 107  55]\n",
      "Fold 21, VarNum 15, Method XGB, Model CART, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:27:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594 0.02863571 0.02685573\n",
      " 0.02517813 0.02097734 0.01903709]\n",
      "Fold 0, VarNum 15, Method XGB, Model RF, Selected Features: [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5714285714285714\n",
      "[15:27:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100  99 133 134  15 135  34  95  87 121   5]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001 0.03824509\n",
      " 0.03144442 0.03067115 0.02586752 0.02579053 0.0241793  0.02340247\n",
      " 0.02311029 0.02206623 0.02039631]\n",
      "Fold 1, VarNum 15, Method XGB, Model RF, Selected Features: [  1 114  92  79 100  99 133 134  15 135  34  95  87 121   5]\n",
      "Fold 1, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.4782608695652174\n",
      "[15:27:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119 118  83  97 134  89 106   0  32  92  30]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364  0.03260981\n",
      " 0.02879154 0.02842961 0.02837311 0.02583595 0.02269325 0.02135309\n",
      " 0.01880933 0.01693355 0.01669514]\n",
      "Fold 2, VarNum 15, Method XGB, Model RF, Selected Features: [ 26  72  77  46 119 118  83  97 134  89 106   0  32  92  30]\n",
      "Fold 2, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "[15:27:33] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 59  25  46   0  61 106  99 134  30  97 113  32 118 133  89]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437 0.02968394\n",
      " 0.02835124 0.02730615 0.02658134 0.02494756 0.02389958 0.0223331\n",
      " 0.02197508 0.02195759 0.02179354]\n",
      "Fold 3, VarNum 15, Method XGB, Model RF, Selected Features: [ 59  25  46   0  61 106  99 134  30  97 113  32 118 133  89]\n",
      "Fold 3, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.782608695652174\n",
      "[15:27:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25  32 107 114  64  35  99 125  72 115  97]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888 0.02571377\n",
      " 0.02553768 0.02345947 0.02330734 0.02305957 0.02195241 0.01955295\n",
      " 0.01902484 0.01853873 0.01843479]\n",
      "Fold 4, VarNum 15, Method XGB, Model RF, Selected Features: [ 91 106  11 134  25  32 107 114  64  35  99 125  72 115  97]\n",
      "Fold 4, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:27:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45  47  25 106  32 135  89  52  68  34  42]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451 0.03064233\n",
      " 0.02763383 0.02525128 0.02185246 0.02143421 0.02086745 0.0201348\n",
      " 0.01876054 0.01809891 0.01793046]\n",
      "Fold 5, VarNum 15, Method XGB, Model RF, Selected Features: [ 74  99 134 119  45  47  25 106  32 135  89  52  68  34  42]\n",
      "Fold 5, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:27:34] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45  64  95 134 106  92 116 130 127  74  32]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231 0.02633443\n",
      " 0.02538574 0.02404596 0.02323602 0.0198393  0.01787091 0.01744338\n",
      " 0.01704738 0.01699063 0.01666635]\n",
      "Fold 6, VarNum 15, Method XGB, Model RF, Selected Features: [107  99  17  21  45  64  95 134 106  92 116 130 127  74  32]\n",
      "Fold 6, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:27:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40 134 127  45  25   1 113  22 118  28  30]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901 0.0272389\n",
      " 0.02550623 0.02285499 0.02245414 0.02209218 0.02101223 0.02079713\n",
      " 0.01824612 0.01747868 0.01745014]\n",
      "Fold 7, VarNum 15, Method XGB, Model RF, Selected Features: [107  99  66 130  40 134 127  45  25   1 113  22 118  28  30]\n",
      "Fold 7, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:27:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30  18 112  85 134 135  44 123  63  98 130]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053  0.0235923\n",
      " 0.02342531 0.02282099 0.02216369 0.02056672 0.01886323 0.01829717\n",
      " 0.01805966 0.01743061 0.01719207]\n",
      "Fold 8, VarNum 15, Method XGB, Model RF, Selected Features: [107  22  65  60  30  18 112  85 134 135  44 123  63  98 130]\n",
      "Fold 8, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:27:35] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54  12  18  30  68  43  57  70  63 134  69]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297  0.02258516\n",
      " 0.02081037 0.02071864 0.01912442 0.01802449 0.01777671 0.01766494\n",
      " 0.01684872 0.01670655 0.01649962]\n",
      "Fold 9, VarNum 15, Method XGB, Model RF, Selected Features: [107  99   0  21  54  12  18  30  68  43  57  70  63 134  69]\n",
      "Fold 9, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6666666666666666\n",
      "[15:27:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110  30  63  45 117 134  21  99  49  93 121]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609 0.02446463\n",
      " 0.02375421 0.02277838 0.01852702 0.01821284 0.01779496 0.01685031\n",
      " 0.01667458 0.0164859  0.01594789]\n",
      "Fold 10, VarNum 15, Method XGB, Model RF, Selected Features: [ 70 107  85  87 110  30  63  45 117 134  21  99  49  93 121]\n",
      "Fold 10, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:27:36] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85  39  49  30  15  59  75  21  83  38  82]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574  0.02708106\n",
      " 0.02700011 0.0267045  0.02186688 0.01906396 0.01798938 0.01762597\n",
      " 0.01718468 0.01664497 0.01624968]\n",
      "Fold 11, VarNum 15, Method XGB, Model RF, Selected Features: [ 99  12 106  93  85  39  49  30  15  59  75  21  83  38  82]\n",
      "Fold 11, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:27:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20  38  99 124  93  44  56  92  94 136  85]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579 0.02196426\n",
      " 0.02097165 0.02021421 0.02020423 0.01939055 0.0182912  0.0177101\n",
      " 0.0172697  0.01662818 0.01616009]\n",
      "Fold 12, VarNum 15, Method XGB, Model RF, Selected Features: [ 27 107  74  30  20  38  99 124  93  44  56  92  94 136  85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 12, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5454545454545454\n",
      "[15:27:37] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30  21  50  63  62 131  72  76  69  99  89]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133 0.02203631\n",
      " 0.02164648 0.01945004 0.01928781 0.01914054 0.01795315 0.01788961\n",
      " 0.0161291  0.01594377 0.01564425]\n",
      "Fold 13, VarNum 15, Method XGB, Model RF, Selected Features: [107  78  84  45  30  21  50  63  62 131  72  76  69  99  89]\n",
      "Fold 13, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "[15:27:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94  99  30  71 132  24  22 116  65  68  72]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958 0.02117782\n",
      " 0.02106225 0.0206327  0.01638661 0.01580617 0.01546936 0.01492459\n",
      " 0.01461546 0.01459011 0.01432451]\n",
      "Fold 14, VarNum 15, Method XGB, Model RF, Selected Features: [107 106 112  51  94  99  30  71 132  24  22 116  65  68  72]\n",
      "Fold 14, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:27:38] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45  29  85  33   3 105  40 136 131  55   9]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043 0.02049493\n",
      " 0.02032613 0.01902149 0.01853412 0.01766204 0.01700879 0.01694755\n",
      " 0.01654983 0.015752   0.01557628]\n",
      "Fold 15, VarNum 15, Method XGB, Model RF, Selected Features: [106  93 119  30  45  29  85  33   3 105  40 136 131  55   9]\n",
      "Fold 15, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.7391304347826086\n",
      "[15:27:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22  93  40 114  63 107  35  94  23  45  42]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031 0.01834612\n",
      " 0.01832638 0.01660874 0.0163494  0.01607329 0.01531808 0.01473181\n",
      " 0.01466983 0.01447087 0.01388538]\n",
      "Fold 16, VarNum 15, Method XGB, Model RF, Selected Features: [ 28  30 106  85  22  93  40 114  63 107  35  94  23  45  42]\n",
      "Fold 16, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.6\n",
      "[15:27:39] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93  84  30  26  67  49 108  38 113  86  94]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613 0.02621297\n",
      " 0.02436594 0.02368471 0.02286324 0.01924192 0.01689295 0.01632198\n",
      " 0.01574784 0.01483444 0.01414419]\n",
      "Fold 17, VarNum 15, Method XGB, Model RF, Selected Features: [106  45  40  18  93  84  30  26  67  49 108  38 113  86  94]\n",
      "Fold 17, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:27:40] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30  94  93   1  77  85  79 134  40 112 133]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267 0.02270854\n",
      " 0.02157388 0.02096329 0.0197053  0.01822887 0.01821537 0.0178174\n",
      " 0.01676521 0.01570135 0.01401956]\n",
      "Fold 18, VarNum 15, Method XGB, Model RF, Selected Features: [ 18  73 106 107  30  94  93   1  77  85  79 134  40 112 133]\n",
      "Fold 18, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:27:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114  13  30  65  71  26 107 106 133  24 126]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243 0.02239063\n",
      " 0.02224347 0.02035736 0.02010566 0.01846889 0.01824656 0.01770644\n",
      " 0.01687778 0.01592533 0.01554673]\n",
      "Fold 19, VarNum 15, Method XGB, Model RF, Selected Features: [ 40  76  17  85 114  13  30  65  71  26 107 106 133  24 126]\n",
      "Fold 19, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.631578947368421\n",
      "[15:27:41] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106  40  34  71 121  87  69  74 101  68  28]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375 0.01896119\n",
      " 0.01791704 0.01761022 0.01748885 0.01717754 0.01426151 0.01417943\n",
      " 0.01389764 0.01375816 0.01346007]\n",
      "Fold 20, VarNum 15, Method XGB, Model RF, Selected Features: [ 46  18 114  30 106  40  34  71 121  87  69  74 101  68  28]\n",
      "Fold 20, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.42857142857142855\n",
      "[15:27:42] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 76  40  64  30  36 114  85 106  32   7 121  45  42 107  55]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063 0.02041966\n",
      " 0.01861721 0.01584065 0.01534018 0.01502906 0.01482452 0.01470072\n",
      " 0.01435008 0.01427137 0.01418887]\n",
      "Fold 21, VarNum 15, Method XGB, Model RF, Selected Features: [ 76  40  64  30  36 114  85 106  32   7 121  45  42 107  55]\n",
      "Fold 21, VarNum 15, Method XGB, Model RF, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "[15:27:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594 0.02863571 0.02685573\n",
      " 0.02517813 0.02097734 0.01903709]\n",
      "Fold 0, VarNum 15, Method XGB, Model XGB, Selected Features: [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Fold 0, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5238095238095238\n",
      "[15:27:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100  99 133 134  15 135  34  95  87 121   5]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001 0.03824509\n",
      " 0.03144442 0.03067115 0.02586752 0.02579053 0.0241793  0.02340247\n",
      " 0.02311029 0.02206623 0.02039631]\n",
      "Fold 1, VarNum 15, Method XGB, Model XGB, Selected Features: [  1 114  92  79 100  99 133 134  15 135  34  95  87 121   5]\n",
      "Fold 1, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5217391304347826\n",
      "[15:27:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119 118  83  97 134  89 106   0  32  92  30]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364  0.03260981\n",
      " 0.02879154 0.02842961 0.02837311 0.02583595 0.02269325 0.02135309\n",
      " 0.01880933 0.01693355 0.01669514]\n",
      "Fold 2, VarNum 15, Method XGB, Model XGB, Selected Features: [ 26  72  77  46 119 118  83  97 134  89 106   0  32  92  30]\n",
      "Fold 2, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.8421052631578947\n",
      "[15:27:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 59  25  46   0  61 106  99 134  30  97 113  32 118 133  89]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437 0.02968394\n",
      " 0.02835124 0.02730615 0.02658134 0.02494756 0.02389958 0.0223331\n",
      " 0.02197508 0.02195759 0.02179354]\n",
      "Fold 3, VarNum 15, Method XGB, Model XGB, Selected Features: [ 59  25  46   0  61 106  99 134  30  97 113  32 118 133  89]\n",
      "Fold 3, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6956521739130435\n",
      "[15:27:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25  32 107 114  64  35  99 125  72 115  97]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888 0.02571377\n",
      " 0.02553768 0.02345947 0.02330734 0.02305957 0.02195241 0.01955295\n",
      " 0.01902484 0.01853873 0.01843479]\n",
      "Fold 4, VarNum 15, Method XGB, Model XGB, Selected Features: [ 91 106  11 134  25  32 107 114  64  35  99 125  72 115  97]\n",
      "Fold 4, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:27:43] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45  47  25 106  32 135  89  52  68  34  42]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451 0.03064233\n",
      " 0.02763383 0.02525128 0.02185246 0.02143421 0.02086745 0.0201348\n",
      " 0.01876054 0.01809891 0.01793046]\n",
      "Fold 5, VarNum 15, Method XGB, Model XGB, Selected Features: [ 74  99 134 119  45  47  25 106  32 135  89  52  68  34  42]\n",
      "Fold 5, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5263157894736842\n",
      "[15:27:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  17  21  45  64  95 134 106  92 116 130 127  74  32]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231 0.02633443\n",
      " 0.02538574 0.02404596 0.02323602 0.0198393  0.01787091 0.01744338\n",
      " 0.01704738 0.01699063 0.01666635]\n",
      "Fold 6, VarNum 15, Method XGB, Model XGB, Selected Features: [107  99  17  21  45  64  95 134 106  92 116 130 127  74  32]\n",
      "Fold 6, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7727272727272727\n",
      "[15:27:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40 134 127  45  25   1 113  22 118  28  30]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901 0.0272389\n",
      " 0.02550623 0.02285499 0.02245414 0.02209218 0.02101223 0.02079713\n",
      " 0.01824612 0.01747868 0.01745014]\n",
      "Fold 7, VarNum 15, Method XGB, Model XGB, Selected Features: [107  99  66 130  40 134 127  45  25   1 113  22 118  28  30]\n",
      "Fold 7, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7894736842105263\n",
      "[15:27:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30  18 112  85 134 135  44 123  63  98 130]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053  0.0235923\n",
      " 0.02342531 0.02282099 0.02216369 0.02056672 0.01886323 0.01829717\n",
      " 0.01805966 0.01743061 0.01719207]\n",
      "Fold 8, VarNum 15, Method XGB, Model XGB, Selected Features: [107  22  65  60  30  18 112  85 134 135  44 123  63  98 130]\n",
      "Fold 8, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:27:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54  12  18  30  68  43  57  70  63 134  69]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297  0.02258516\n",
      " 0.02081037 0.02071864 0.01912442 0.01802449 0.01777671 0.01766494\n",
      " 0.01684872 0.01670655 0.01649962]\n",
      "Fold 9, VarNum 15, Method XGB, Model XGB, Selected Features: [107  99   0  21  54  12  18  30  68  43  57  70  63 134  69]\n",
      "Fold 9, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6190476190476191\n",
      "[15:27:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110  30  63  45 117 134  21  99  49  93 121]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609 0.02446463\n",
      " 0.02375421 0.02277838 0.01852702 0.01821284 0.01779496 0.01685031\n",
      " 0.01667458 0.0164859  0.01594789]\n",
      "Fold 10, VarNum 15, Method XGB, Model XGB, Selected Features: [ 70 107  85  87 110  30  63  45 117 134  21  99  49  93 121]\n",
      "Fold 10, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:27:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85  39  49  30  15  59  75  21  83  38  82]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574  0.02708106\n",
      " 0.02700011 0.0267045  0.02186688 0.01906396 0.01798938 0.01762597\n",
      " 0.01718468 0.01664497 0.01624968]\n",
      "Fold 11, VarNum 15, Method XGB, Model XGB, Selected Features: [ 99  12 106  93  85  39  49  30  15  59  75  21  83  38  82]\n",
      "Fold 11, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6842105263157895\n",
      "[15:27:45] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20  38  99 124  93  44  56  92  94 136  85]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579 0.02196426\n",
      " 0.02097165 0.02021421 0.02020423 0.01939055 0.0182912  0.0177101\n",
      " 0.0172697  0.01662818 0.01616009]\n",
      "Fold 12, VarNum 15, Method XGB, Model XGB, Selected Features: [ 27 107  74  30  20  38  99 124  93  44  56  92  94 136  85]\n",
      "Fold 12, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:27:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30  21  50  63  62 131  72  76  69  99  89]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133 0.02203631\n",
      " 0.02164648 0.01945004 0.01928781 0.01914054 0.01795315 0.01788961\n",
      " 0.0161291  0.01594377 0.01564425]\n",
      "Fold 13, VarNum 15, Method XGB, Model XGB, Selected Features: [107  78  84  45  30  21  50  63  62 131  72  76  69  99  89]\n",
      "Fold 13, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6818181818181818\n",
      "[15:27:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94  99  30  71 132  24  22 116  65  68  72]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958 0.02117782\n",
      " 0.02106225 0.0206327  0.01638661 0.01580617 0.01546936 0.01492459\n",
      " 0.01461546 0.01459011 0.01432451]\n",
      "Fold 14, VarNum 15, Method XGB, Model XGB, Selected Features: [107 106 112  51  94  99  30  71 132  24  22 116  65  68  72]\n",
      "Fold 14, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.65\n",
      "[15:27:46] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [106  93 119  30  45  29  85  33   3 105  40 136 131  55   9]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043 0.02049493\n",
      " 0.02032613 0.01902149 0.01853412 0.01766204 0.01700879 0.01694755\n",
      " 0.01654983 0.015752   0.01557628]\n",
      "Fold 15, VarNum 15, Method XGB, Model XGB, Selected Features: [106  93 119  30  45  29  85  33   3 105  40 136 131  55   9]\n",
      "Fold 15, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.6521739130434783\n",
      "[15:27:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22  93  40 114  63 107  35  94  23  45  42]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031 0.01834612\n",
      " 0.01832638 0.01660874 0.0163494  0.01607329 0.01531808 0.01473181\n",
      " 0.01466983 0.01447087 0.01388538]\n",
      "Fold 16, VarNum 15, Method XGB, Model XGB, Selected Features: [ 28  30 106  85  22  93  40 114  63 107  35  94  23  45  42]\n",
      "Fold 16, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.75\n",
      "[15:27:47] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93  84  30  26  67  49 108  38 113  86  94]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613 0.02621297\n",
      " 0.02436594 0.02368471 0.02286324 0.01924192 0.01689295 0.01632198\n",
      " 0.01574784 0.01483444 0.01414419]\n",
      "Fold 17, VarNum 15, Method XGB, Model XGB, Selected Features: [106  45  40  18  93  84  30  26  67  49 108  38 113  86  94]\n",
      "Fold 17, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.5\n",
      "[15:27:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 18  73 106 107  30  94  93   1  77  85  79 134  40 112 133]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267 0.02270854\n",
      " 0.02157388 0.02096329 0.0197053  0.01822887 0.01821537 0.0178174\n",
      " 0.01676521 0.01570135 0.01401956]\n",
      "Fold 18, VarNum 15, Method XGB, Model XGB, Selected Features: [ 18  73 106 107  30  94  93   1  77  85  79 134  40 112 133]\n",
      "Fold 18, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.4090909090909091\n",
      "[15:27:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114  13  30  65  71  26 107 106 133  24 126]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243 0.02239063\n",
      " 0.02224347 0.02035736 0.02010566 0.01846889 0.01824656 0.01770644\n",
      " 0.01687778 0.01592533 0.01554673]\n",
      "Fold 19, VarNum 15, Method XGB, Model XGB, Selected Features: [ 40  76  17  85 114  13  30  65  71  26 107 106 133  24 126]\n",
      "Fold 19, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.47368421052631576\n",
      "[15:27:48] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106  40  34  71 121  87  69  74 101  68  28]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375 0.01896119\n",
      " 0.01791704 0.01761022 0.01748885 0.01717754 0.01426151 0.01417943\n",
      " 0.01389764 0.01375816 0.01346007]\n",
      "Fold 20, VarNum 15, Method XGB, Model XGB, Selected Features: [ 46  18 114  30 106  40  34  71 121  87  69  74 101  68  28]\n",
      "Fold 20, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.7142857142857143\n",
      "[15:27:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 76  40  64  30  36 114  85 106  32   7 121  45  42 107  55]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063 0.02041966\n",
      " 0.01861721 0.01584065 0.01534018 0.01502906 0.01482452 0.01470072\n",
      " 0.01435008 0.01427137 0.01418887]\n",
      "Fold 21, VarNum 15, Method XGB, Model XGB, Selected Features: [ 76  40  64  30  36 114  85 106  32   7 121  45  42 107  55]\n",
      "Fold 21, VarNum 15, Method XGB, Model XGB, Train Acc: 1.0, Test Acc: 0.45454545454545453\n",
      "[15:27:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Feature importances: [0.05529401 0.04942611 0.04690247 0.03984741 0.03686202 0.03489007\n",
      " 0.03425908 0.03199412 0.03003381 0.02984594 0.02863571 0.02685573\n",
      " 0.02517813 0.02097734 0.01903709]\n",
      "Fold 0, VarNum 15, Method XGB, Model LDA, Selected Features: [ 54  92  67 105  46  76 134 111  66  57 106 133  78 100 109]\n",
      "Fold 0, VarNum 15, Method XGB, Model LDA, Train Acc: 0.68, Test Acc: 0.5714285714285714\n",
      "[15:27:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [  1 114  92  79 100  99 133 134  15 135  34  95  87 121   5]\n",
      "Feature importances: [0.07011802 0.05151211 0.04698886 0.04609953 0.04262001 0.03824509\n",
      " 0.03144442 0.03067115 0.02586752 0.02579053 0.0241793  0.02340247\n",
      " 0.02311029 0.02206623 0.02039631]\n",
      "Fold 1, VarNum 15, Method XGB, Model LDA, Selected Features: [  1 114  92  79 100  99 133 134  15 135  34  95  87 121   5]\n",
      "Fold 1, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6986301369863014, Test Acc: 0.6521739130434783\n",
      "[15:27:49] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 26  72  77  46 119 118  83  97 134  89 106   0  32  92  30]\n",
      "Feature importances: [0.06929518 0.06220025 0.04788743 0.04746341 0.0343364  0.03260981\n",
      " 0.02879154 0.02842961 0.02837311 0.02583595 0.02269325 0.02135309\n",
      " 0.01880933 0.01693355 0.01669514]\n",
      "Fold 2, VarNum 15, Method XGB, Model LDA, Selected Features: [ 26  72  77  46 119 118  83  97 134  89 106   0  32  92  30]\n",
      "Fold 2, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7041420118343196, Test Acc: 0.8421052631578947\n",
      "[15:27:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 59  25  46   0  61 106  99 134  30  97 113  32 118 133  89]\n",
      "Feature importances: [0.04915252 0.04541609 0.03848644 0.03071052 0.03060437 0.02968394\n",
      " 0.02835124 0.02730615 0.02658134 0.02494756 0.02389958 0.0223331\n",
      " 0.02197508 0.02195759 0.02179354]\n",
      "Fold 3, VarNum 15, Method XGB, Model LDA, Selected Features: [ 59  25  46   0  61 106  99 134  30  97 113  32 118 133  89]\n",
      "Fold 3, VarNum 15, Method XGB, Model LDA, Train Acc: 0.75, Test Acc: 0.7391304347826086\n",
      "[15:27:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 91 106  11 134  25  32 107 114  64  35  99 125  72 115  97]\n",
      "Feature importances: [0.04838402 0.04720221 0.03289761 0.03034894 0.02912888 0.02571377\n",
      " 0.02553768 0.02345947 0.02330734 0.02305957 0.02195241 0.01955295\n",
      " 0.01902484 0.01853873 0.01843479]\n",
      "Fold 4, VarNum 15, Method XGB, Model LDA, Selected Features: [ 91 106  11 134  25  32 107 114  64  35  99 125  72 115  97]\n",
      "Fold 4, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7393364928909952, Test Acc: 0.47619047619047616\n",
      "[15:27:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 74  99 134 119  45  47  25 106  32 135  89  52  68  34  42]\n",
      "Feature importances: [0.06361207 0.05064658 0.03814812 0.03285487 0.03097451 0.03064233\n",
      " 0.02763383 0.02525128 0.02185246 0.02143421 0.02086745 0.0201348\n",
      " 0.01876054 0.01809891 0.01793046]\n",
      "Fold 5, VarNum 15, Method XGB, Model LDA, Selected Features: [ 74  99 134 119  45  47  25 106  32 135  89  52  68  34  42]\n",
      "Fold 5, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7198275862068966, Test Acc: 0.5789473684210527\n",
      "[15:27:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [107  99  17  21  45  64  95 134 106  92 116 130 127  74  32]\n",
      "Feature importances: [0.05859797 0.05590509 0.05015673 0.02674374 0.02638231 0.02633443\n",
      " 0.02538574 0.02404596 0.02323602 0.0198393  0.01787091 0.01744338\n",
      " 0.01704738 0.01699063 0.01666635]\n",
      "Fold 6, VarNum 15, Method XGB, Model LDA, Selected Features: [107  99  17  21  45  64  95 134 106  92 116 130 127  74  32]\n",
      "Fold 6, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7250996015936255, Test Acc: 0.36363636363636365\n",
      "[15:27:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99  66 130  40 134 127  45  25   1 113  22 118  28  30]\n",
      "Feature importances: [0.0674936  0.05236558 0.03716106 0.03085598 0.02897901 0.0272389\n",
      " 0.02550623 0.02285499 0.02245414 0.02209218 0.02101223 0.02079713\n",
      " 0.01824612 0.01747868 0.01745014]\n",
      "Fold 7, VarNum 15, Method XGB, Model LDA, Selected Features: [107  99  66 130  40 134 127  45  25   1 113  22 118  28  30]\n",
      "Fold 7, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7142857142857143, Test Acc: 0.7368421052631579\n",
      "[15:27:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  22  65  60  30  18 112  85 134 135  44 123  63  98 130]\n",
      "Feature importances: [0.0770492  0.04434527 0.04269333 0.02668799 0.0239053  0.0235923\n",
      " 0.02342531 0.02282099 0.02216369 0.02056672 0.01886323 0.01829717\n",
      " 0.01805966 0.01743061 0.01719207]\n",
      "Fold 8, VarNum 15, Method XGB, Model LDA, Selected Features: [107  22  65  60  30  18 112  85 134 135  44 123  63  98 130]\n",
      "Fold 8, VarNum 15, Method XGB, Model LDA, Train Acc: 0.726027397260274, Test Acc: 0.7142857142857143\n",
      "[15:27:50] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  99   0  21  54  12  18  30  68  43  57  70  63 134  69]\n",
      "Feature importances: [0.06011542 0.04494856 0.03253545 0.02751223 0.0263297  0.02258516\n",
      " 0.02081037 0.02071864 0.01912442 0.01802449 0.01777671 0.01766494\n",
      " 0.01684872 0.01670655 0.01649962]\n",
      "Fold 9, VarNum 15, Method XGB, Model LDA, Selected Features: [107  99   0  21  54  12  18  30  68  43  57  70  63 134  69]\n",
      "Fold 9, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7028753993610224, Test Acc: 0.6666666666666666\n",
      "[15:27:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 70 107  85  87 110  30  63  45 117 134  21  99  49  93 121]\n",
      "Feature importances: [0.04385455 0.03864514 0.02920155 0.02901106 0.02693609 0.02446463\n",
      " 0.02375421 0.02277838 0.01852702 0.01821284 0.01779496 0.01685031\n",
      " 0.01667458 0.0164859  0.01594789]\n",
      "Fold 10, VarNum 15, Method XGB, Model LDA, Selected Features: [ 70 107  85  87 110  30  63  45 117 134  21  99  49  93 121]\n",
      "Fold 10, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7005988023952096, Test Acc: 0.6818181818181818\n",
      "[15:27:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 99  12 106  93  85  39  49  30  15  59  75  21  83  38  82]\n",
      "Feature importances: [0.04358206 0.03767971 0.0332396  0.02802188 0.0277574  0.02708106\n",
      " 0.02700011 0.0267045  0.02186688 0.01906396 0.01798938 0.01762597\n",
      " 0.01718468 0.01664497 0.01624968]\n",
      "Fold 11, VarNum 15, Method XGB, Model LDA, Selected Features: [ 99  12 106  93  85  39  49  30  15  59  75  21  83  38  82]\n",
      "Fold 11, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7191011235955056, Test Acc: 0.5789473684210527\n",
      "[15:27:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 27 107  74  30  20  38  99 124  93  44  56  92  94 136  85]\n",
      "Feature importances: [0.03540662 0.03456968 0.02741739 0.02654268 0.02624579 0.02196426\n",
      " 0.02097165 0.02021421 0.02020423 0.01939055 0.0182912  0.0177101\n",
      " 0.0172697  0.01662818 0.01616009]\n",
      "Fold 12, VarNum 15, Method XGB, Model LDA, Selected Features: [ 27 107  74  30  20  38  99 124  93  44  56  92  94 136  85]\n",
      "Fold 12, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7493333333333333, Test Acc: 0.6363636363636364\n",
      "[15:27:51] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107  78  84  45  30  21  50  63  62 131  72  76  69  99  89]\n",
      "Feature importances: [0.05228566 0.02599358 0.02345364 0.02313786 0.02221133 0.02203631\n",
      " 0.02164648 0.01945004 0.01928781 0.01914054 0.01795315 0.01788961\n",
      " 0.0161291  0.01594377 0.01564425]\n",
      "Fold 13, VarNum 15, Method XGB, Model LDA, Selected Features: [107  78  84  45  30  21  50  63  62 131  72  76  69  99  89]\n",
      "Fold 13, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6851385390428212, Test Acc: 0.45454545454545453\n",
      "[15:27:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [107 106 112  51  94  99  30  71 132  24  22 116  65  68  72]\n",
      "Feature importances: [0.05568868 0.04905095 0.03195811 0.02766475 0.02238958 0.02117782\n",
      " 0.02106225 0.0206327  0.01638661 0.01580617 0.01546936 0.01492459\n",
      " 0.01461546 0.01459011 0.01432451]\n",
      "Fold 14, VarNum 15, Method XGB, Model LDA, Selected Features: [107 106 112  51  94  99  30  71 132  24  22 116  65  68  72]\n",
      "Fold 14, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7016706443914081, Test Acc: 0.6\n",
      "[15:27:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  93 119  30  45  29  85  33   3 105  40 136 131  55   9]\n",
      "Feature importances: [0.04404142 0.031406   0.02888564 0.02539387 0.02282043 0.02049493\n",
      " 0.02032613 0.01902149 0.01853412 0.01766204 0.01700879 0.01694755\n",
      " 0.01654983 0.015752   0.01557628]\n",
      "Fold 15, VarNum 15, Method XGB, Model LDA, Selected Features: [106  93 119  30  45  29  85  33   3 105  40 136 131  55   9]\n",
      "Fold 15, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6879271070615034, Test Acc: 0.7391304347826086\n",
      "[15:27:52] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 28  30 106  85  22  93  40 114  63 107  35  94  23  45  42]\n",
      "Feature importances: [0.04591639 0.02453089 0.02236762 0.02220122 0.02107031 0.01834612\n",
      " 0.01832638 0.01660874 0.0163494  0.01607329 0.01531808 0.01473181\n",
      " 0.01466983 0.01447087 0.01388538]\n",
      "Fold 16, VarNum 15, Method XGB, Model LDA, Selected Features: [ 28  30 106  85  22  93  40 114  63 107  35  94  23  45  42]\n",
      "Fold 16, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7034632034632035, Test Acc: 0.75\n",
      "[15:27:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [106  45  40  18  93  84  30  26  67  49 108  38 113  86  94]\n",
      "Feature importances: [0.03269581 0.030594   0.0302958  0.02941356 0.02845613 0.02621297\n",
      " 0.02436594 0.02368471 0.02286324 0.01924192 0.01689295 0.01632198\n",
      " 0.01574784 0.01483444 0.01414419]\n",
      "Fold 17, VarNum 15, Method XGB, Model LDA, Selected Features: [106  45  40  18  93  84  30  26  67  49 108  38 113  86  94]\n",
      "Fold 17, VarNum 15, Method XGB, Model LDA, Train Acc: 0.7199170124481328, Test Acc: 0.6\n",
      "[15:27:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected features (XGB): [ 18  73 106 107  30  94  93   1  77  85  79 134  40 112 133]\n",
      "Feature importances: [0.05220703 0.03001136 0.02681438 0.0255463  0.02420267 0.02270854\n",
      " 0.02157388 0.02096329 0.0197053  0.01822887 0.01821537 0.0178174\n",
      " 0.01676521 0.01570135 0.01401956]\n",
      "Fold 18, VarNum 15, Method XGB, Model LDA, Selected Features: [ 18  73 106 107  30  94  93   1  77  85  79 134  40 112 133]\n",
      "Fold 18, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6912350597609562, Test Acc: 0.5454545454545454\n",
      "[15:27:53] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 40  76  17  85 114  13  30  65  71  26 107 106 133  24 126]\n",
      "Feature importances: [0.03432887 0.03361512 0.02414164 0.02293952 0.02286243 0.02239063\n",
      " 0.02224347 0.02035736 0.02010566 0.01846889 0.01824656 0.01770644\n",
      " 0.01687778 0.01592533 0.01554673]\n",
      "Fold 19, VarNum 15, Method XGB, Model LDA, Selected Features: [ 40  76  17  85 114  13  30  65  71  26 107 106 133  24 126]\n",
      "Fold 19, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6870229007633588, Test Acc: 0.631578947368421\n",
      "[15:27:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 46  18 114  30 106  40  34  71 121  87  69  74 101  68  28]\n",
      "Feature importances: [0.03685965 0.02800523 0.02601543 0.02255109 0.01981375 0.01896119\n",
      " 0.01791704 0.01761022 0.01748885 0.01717754 0.01426151 0.01417943\n",
      " 0.01389764 0.01375816 0.01346007]\n",
      "Fold 20, VarNum 15, Method XGB, Model LDA, Selected Features: [ 46  18 114  30 106  40  34  71 121  87  69  74 101  68  28]\n",
      "Fold 20, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6813996316758748, Test Acc: 0.6190476190476191\n",
      "[15:27:54] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-07593ffd91cd9da33-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:767: \n",
      "Parameters: { \"n_estimator\" } are not used.\n",
      "\n",
      "Selected features (XGB): [ 76  40  64  30  36 114  85 106  32   7 121  45  42 107  55]\n",
      "Feature importances: [0.05807639 0.05106579 0.02760163 0.02753242 0.02322063 0.02041966\n",
      " 0.01861721 0.01584065 0.01534018 0.01502906 0.01482452 0.01470072\n",
      " 0.01435008 0.01427137 0.01418887]\n",
      "Fold 21, VarNum 15, Method XGB, Model LDA, Selected Features: [ 76  40  64  30  36 114  85 106  32   7 121  45  42 107  55]\n",
      "Fold 21, VarNum 15, Method XGB, Model LDA, Train Acc: 0.6914893617021277, Test Acc: 0.6363636363636364\n"
     ]
    }
   ],
   "source": [
    "data = data_main.drop(columns=['Target_BASE'])\n",
    "\n",
    "# Defining feature and target\n",
    "X = data.drop(columns=['Target'])\n",
    "Y = data['Target']\n",
    "\n",
    "# Normalization of data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Feature selection functions\n",
    "\n",
    "def select_features_corr(X_train, Y_train, varnum):\n",
    "    corr_matrix = data.corr()\n",
    "\n",
    "    corr_with_target = corr_matrix.iloc[:-1, -1]\n",
    "\n",
    "    sorted_corr = corr_with_target.abs().sort_values(ascending=False)\n",
    "\n",
    "    selected_features_name = sorted_corr.head(varnum).index\n",
    "\n",
    "    selected_features = [X.columns.get_loc(feature) for feature in selected_features_name]\n",
    "    print(f\"Selected features (CorrelationFS): {selected_features}\")\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "\n",
    "def select_features_chi_square(X_train, Y_train, varnum):\n",
    "    selector = SelectKBest(chi2, k=varnum).fit(X_train, Y_train)\n",
    "\n",
    "    selected_features = selector.get_support(indices=True)\n",
    "\n",
    "    print(f\"Selected features (ChiSquareFS): {selected_features}\")\n",
    "    return selected_features\n",
    "\n",
    "def select_features_rf(X_train, Y_train, varnumn_estimators=50):\n",
    "    rf = RandomForestClassifier(random_state=42)\n",
    "    rf.fit(X_train, Y_train)\n",
    "    model_selector = SelectFromModel(rf, max_features=varnum, prefit=True)\n",
    "    selected_features = model_selector.get_support(indices=True)\n",
    "    feature_importances = rf.feature_importances_\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "\n",
    "    # Select the most important 'varnum' number feature\n",
    "    selected_features = sorted_indices[:varnum]\n",
    "    print(f\"Selected features (RF): {selected_features}\")\n",
    "    print(f\"Feature importances: {feature_importances[selected_features]}\")\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "def select_features_xgb(X_train, Y_train, varnum):\n",
    "    xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42,n_estimator=100,max_depth=6,learning_rate=0.3)\n",
    "    xgb.fit(X_train, Y_train)\n",
    "    model_selector = SelectFromModel(xgb, max_features=varnum, prefit=True)\n",
    "    selected_features = model_selector.get_support(indices=True)\n",
    "    feature_importances = xgb.feature_importances_\n",
    "    \n",
    "    # Select the most important 'varnum' number feature\n",
    "    \n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    selected_features = sorted_indices[:varnum]\n",
    "    print(f\"Selected features (XGB): {selected_features}\")\n",
    "    print(f\"Feature importances: {feature_importances[selected_features]}\")\n",
    "\n",
    "    return selected_features\n",
    "\n",
    "# Model functions\n",
    "def create_cart_model():\n",
    "    return DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "def create_rf_model():\n",
    "    return RandomForestClassifier(random_state=42)\n",
    "\n",
    "def create_xgb_model():\n",
    "    return XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "\n",
    "def create_lda_model():\n",
    "    return LinearDiscriminantAnalysis()\n",
    "\n",
    "\n",
    "# Custom time series splitting function\n",
    "def custom_time_series_split(data, train_period_months=6, test_period_months=1):\n",
    "    periods = []\n",
    "    start_date = data.index.min()\n",
    "    end_date = data.index.max()\n",
    "\n",
    "    current_date = start_date\n",
    "\n",
    "    while current_date + relativedelta(months=train_period_months + test_period_months) <= end_date:\n",
    "        train_start = current_date\n",
    "        train_end = train_start + relativedelta(months=train_period_months) - pd.DateOffset(days=1)\n",
    "        test_start = train_end + pd.DateOffset(days=1)\n",
    "        test_end = test_start + relativedelta(months=test_period_months) - pd.DateOffset(days=1)\n",
    "\n",
    "        train_indices = data.loc[train_start:train_end].index\n",
    "        test_indices = data.loc[test_start:test_end].index\n",
    "\n",
    "        periods.append((train_indices, test_indices))\n",
    "\n",
    "        current_date = current_date + relativedelta(months=1)\n",
    "\n",
    "    return periods\n",
    "\n",
    "def expanding_time_series_split(data, initial_train_period_months=6, test_period_months=1):\n",
    "    periods = []\n",
    "    start_date = data.index.min()\n",
    "    end_date = data.index.max()\n",
    "\n",
    "    current_date = start_date + relativedelta(months=initial_train_period_months)\n",
    "\n",
    "    while current_date + relativedelta(months=test_period_months) <= end_date:\n",
    "        train_start = start_date\n",
    "        train_end = current_date - pd.DateOffset(days=1)\n",
    "        test_start = train_end + pd.DateOffset(days=1)\n",
    "        test_end = test_start + relativedelta(months=test_period_months) - pd.DateOffset(days=1)\n",
    "\n",
    "        train_indices = data.loc[train_start:train_end].index\n",
    "        test_indices = data.loc[test_start:test_end].index\n",
    "\n",
    "        periods.append((train_indices, test_indices))\n",
    "\n",
    "        current_date = current_date + relativedelta(months=1)\n",
    "\n",
    "    return periods\n",
    "\n",
    "# Return calculation function\n",
    "def calculate_returns(data_main, predictions, start_date, end_date):\n",
    "    period_data = data_main.loc[start_date:end_date]\n",
    "    period_returns = period_data['Target_BASE']\n",
    "\n",
    "    correct_predictions = (predictions == period_data['Target'])\n",
    "    incorrect_predictions = ~correct_predictions\n",
    "\n",
    "    returns = np.where(correct_predictions, abs(period_returns), -abs(period_returns))\n",
    "    return returns.sum()\n",
    "\n",
    "def calculate_metrics(data_main, predictions, start_date, end_date):\n",
    "    period_data = data_main.loc[start_date:end_date]\n",
    "    period_returns = period_data['Target_BASE']\n",
    "\n",
    "    correct_predictions = (predictions == period_data['Target'])\n",
    "    incorrect_predictions = ~correct_predictions\n",
    "\n",
    "    returns = np.where(correct_predictions, abs(period_returns), -abs(period_returns))\n",
    "\n",
    "    return_df = pd.DataFrame(returns, index=period_data.index)\n",
    "\n",
    "    return_monthly= return_df.resample('M').sum()\n",
    "    min_return_monthly =return_monthly.min().iloc[0]\n",
    "    neg_months = (return_monthly < 0).sum().iloc[0].sum()\n",
    "\n",
    "\n",
    "    return_weekly = return_df.resample('W').sum()\n",
    "    min_return_weekly = return_weekly.min().iloc[0]\n",
    "    neg_weeks = (return_weekly < 0).sum().iloc[0].sum()\n",
    "\n",
    "    long_preds=np.where(predictions > 0.5,1,0)\n",
    "    short_preds=np.where(predictions < 0.5,1,0)\n",
    "\n",
    "    long_f1=f1_score(period_data['Target'],long_preds)\n",
    "    short_f1=f1_score(period_data['Target'],short_preds)\n",
    "\n",
    "    return return_monthly,min_return_monthly,neg_months,return_weekly,min_return_weekly,neg_weeks,long_f1,short_f1\n",
    "\n",
    "\n",
    "# Trend calculation function\n",
    "def calculate_trend(values):\n",
    "    if len(values) < 2:\n",
    "        return np.nan\n",
    "    x = np.arange(len(values))\n",
    "    slope, _, _, _, _ = linregress(x, values)\n",
    "    return slope\n",
    "\n",
    "# Evaluation\n",
    "varnums = [5,10,15]\n",
    "selection_methods = {\n",
    "    'CORR': select_features_corr,\n",
    "     'CHI2': select_features_chi_square,\n",
    "     'RF': select_features_rf,\n",
    "     'XGB': select_features_xgb,\n",
    "}\n",
    "results_df_exp = pd.DataFrame(columns=['Fold', 'Model', 'Feature Selection Method', 'VarNum', 'Train Period', 'Test Period', 'Train Accuracy', 'Test Accuracy', 'Train Return Mean', 'Test Return Mean', 'Selected Features'])\n",
    "\n",
    "periods = expanding_time_series_split(data, initial_train_period_months=6, test_period_months=1)\n",
    "#periods = custom_time_series_split(data, train_period_months=6, test_period_months=1)\n",
    "\n",
    "fold = 0\n",
    "\n",
    "selected_features_count = {}\n",
    "\n",
    "for varnum in varnums:\n",
    "    for method_name, selection_method in selection_methods.items():\n",
    "        for model_name, create_model in [('CART', create_cart_model), ('RF', create_rf_model), ('XGB', create_xgb_model),('LDA', create_lda_model)]:\n",
    "            fold = 0\n",
    "            for train_indices, test_indices in periods:\n",
    "                train_indices = data.index.get_indexer(train_indices)\n",
    "                test_indices = data.index.get_indexer(test_indices)\n",
    "                                \n",
    "                X_train, X_test = X.iloc[train_indices], X.iloc[test_indices]\n",
    "                Y_train, Y_test = Y.iloc[train_indices], Y.iloc[test_indices]\n",
    "                X_train = scaler.fit_transform(X_train)\n",
    "                X_test = scaler.transform(X_test)\n",
    "                \n",
    "                selected_features = selection_method(X_train, Y_train, varnum)\n",
    "                print(f\"Fold {fold}, VarNum {varnum}, Method {method_name}, Model {model_name}, Selected Features: {selected_features}\")\n",
    "\n",
    "                X_train_selected = X_train[:, selected_features]\n",
    "                X_test_selected = X_test[:, selected_features]\n",
    "\n",
    "                selected_feature_names = X.columns[selected_features].tolist()\n",
    "\n",
    "                for feature in selected_feature_names:\n",
    "                    if feature not in selected_features_count:\n",
    "                        selected_features_count[feature] = 0\n",
    "                    selected_features_count[feature] += 1\n",
    "\n",
    "                model = create_model()\n",
    "                model.fit(X_train_selected, Y_train)\n",
    "\n",
    "                train_predictions = model.predict(X_train_selected)\n",
    "                test_predictions = model.predict(X_test_selected)\n",
    "\n",
    "                train_acc = np.mean(train_predictions == Y_train)\n",
    "                test_acc = np.mean(test_predictions == Y_test)\n",
    "\n",
    "                print(f\"Fold {fold}, VarNum {varnum}, Method {method_name}, Model {model_name}, Train Acc: {train_acc}, Test Acc: {test_acc}\")\n",
    "\n",
    "                train_start_date = data.index[train_indices].min()\n",
    "                train_end_date = data.index[train_indices].max()\n",
    "                test_start_date = data.index[test_indices].min()\n",
    "                test_end_date = data.index[test_indices].max()\n",
    "\n",
    "                train_return_mean = calculate_returns(data_main, train_predictions, train_start_date, train_end_date)\n",
    "                test_return_mean = calculate_returns(data_main, test_predictions, test_start_date, test_end_date)\n",
    "                return_monthly,min_return_monthly,neg_months,return_weekly,min_return_weekly,neg_weeks,long_f1,short_f1 = calculate_metrics(data_main, test_predictions, test_start_date, test_end_date)\n",
    "\n",
    "\n",
    "                fold += 1\n",
    "                results_df_exp = pd.concat([results_df_exp, pd.DataFrame({\n",
    "                    'Fold': [fold],\n",
    "                    'Model': [model_name],\n",
    "                    'Feature Selection Method': [method_name],\n",
    "                    'VarNum': [varnum],\n",
    "                    'Train Period': [f'{train_start_date.date()} - {train_end_date.date()}'],\n",
    "                    'Test Period': [f'{test_start_date.date()} - {test_end_date.date()}'],\n",
    "                    'Train Accuracy': [train_acc],\n",
    "                    'Test Accuracy': [test_acc],\n",
    "                    'Train Return Mean': [train_return_mean],\n",
    "                    'Test Return Mean': [test_return_mean],\n",
    "                    'Selected Features': [selected_feature_names],\n",
    "                    'Monthly Min Return':[min_return_monthly],\n",
    "                    'Negative Month Number':[neg_months],\n",
    "                    'Weekly Minimum Return':[min_return_weekly],\n",
    "                    'Negative Week Number':[neg_weeks],\n",
    "                    'Long F1':[long_f1],\n",
    "                    'Short F1':[short_f1]\n",
    "\n",
    "                })], ignore_index=True)\n",
    "\n",
    "# Calculate the number of negative return periods\n",
    "results_df_exp['Negative Train Return'] = results_df_exp['Train Return Mean'] < 0\n",
    "results_df_exp['Negative Test Return'] = results_df_exp['Test Return Mean'] < 0\n",
    "\n",
    "# Trend calculation\n",
    "results_df_exp['Test Accuracy Trend'] = results_df_exp.groupby(['Model', 'Feature Selection Method', 'VarNum'])['Test Accuracy'].transform(lambda x: calculate_trend(x[-25:]))\n",
    "results_df_exp['Test Return Mean Trend'] = results_df_exp.groupby(['Model', 'Feature Selection Method', 'VarNum'])['Test Return Mean'].transform(lambda x: calculate_trend(x[-25:]))\n",
    "\n",
    "# Calculate the mean and std of test accuracies\n",
    "mean_test_acc = results_df_exp.groupby(['Model', 'Feature Selection Method', 'VarNum'])['Test Accuracy'].mean()\n",
    "std_test_acc = results_df_exp.groupby(['Model', 'Feature Selection Method', 'VarNum'])['Test Accuracy'].std()\n",
    "\n",
    "# Create a pivot table for the results\n",
    "pivot_table_exp = results_df_exp.pivot_table(\n",
    "    index=['Model', 'Feature Selection Method', 'VarNum'],\n",
    "\n",
    "    values=['Test Accuracy', 'Test Return Mean', 'Negative Test Return', 'Test Accuracy Trend', 'Test Return Mean Trend','Monthly Min Return','Negative Month Number',\n",
    "            'Weekly Minimum Return','Negative Week Number','Long F1','Short F1'],\n",
    "    aggfunc={\n",
    "        'Test Accuracy': [np.mean, np.std, np.min],\n",
    "        'Test Return Mean': [np.mean, np.std],\n",
    "        'Negative Test Return': np.sum,\n",
    "        'Test Accuracy Trend': np.mean,\n",
    "        'Test Return Mean Trend': np.mean,\n",
    "        'Monthly Min Return':np.mean,\n",
    "        'Negative Month Number':np.mean,\n",
    "        'Weekly Minimum Return':np.mean,\n",
    "        'Negative Week Number':np.mean,\n",
    "        'Long F1':np.mean,\n",
    "        'Short F1':np.mean\n",
    "    }\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "\n",
    "# Flatten the multi-level columns\n",
    "pivot_table_exp.columns = [' '.join(col).strip() for col in pivot_table_exp.columns.values]\n",
    "pivot_table_exp['Periods'] = len(periods)\n",
    "\n",
    "# Create placeholder for Top Features\n",
    "pivot_table_exp[['Top Feature 1', 'Top Feature 2', 'Top Feature 3', 'Top Feature 4', 'Top Feature 5']] = ''\n",
    "\n",
    "# Find the 5 most frequently used variables in the result_df table and add them to the pivot_table\n",
    "for idx, row in pivot_table_exp.iterrows():\n",
    "    model_name = row['Model']\n",
    "    method_name = row['Feature Selection Method']\n",
    "    varnum = row['VarNum']\n",
    "\n",
    "    filtered_df = results_df_exp[(results_df_exp['Model'] == model_name) &\n",
    "                             (results_df_exp['Feature Selection Method'] == method_name) &\n",
    "                             (results_df_exp['VarNum'] == varnum)]\n",
    "\n",
    "    feature_counts = {}\n",
    "    for features in filtered_df['Selected Features']:\n",
    "        for feature in features:\n",
    "            if feature not in feature_counts:\n",
    "                feature_counts[feature] = 0\n",
    "            feature_counts[feature] += 1\n",
    "\n",
    "    top_features = pd.Series(feature_counts).nlargest(5).index.tolist()\n",
    "\n",
    "    for i, feature in enumerate(top_features):\n",
    "        pivot_table_exp.at[idx, f'Top Feature {i+1}'] = feature\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kIxtxw1ZUMku",
    "outputId": "86002bae-2240-4d68-f567-93ca7336f392"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Feature Selection Method</th>\n",
       "      <th>VarNum</th>\n",
       "      <th>Long F1 mean</th>\n",
       "      <th>Monthly Min Return mean</th>\n",
       "      <th>Negative Month Number mean</th>\n",
       "      <th>Negative Test Return sum</th>\n",
       "      <th>Negative Week Number mean</th>\n",
       "      <th>Short F1 mean</th>\n",
       "      <th>Test Accuracy amin</th>\n",
       "      <th>Test Accuracy mean</th>\n",
       "      <th>Test Accuracy std</th>\n",
       "      <th>Test Accuracy Trend mean</th>\n",
       "      <th>Test Return Mean mean</th>\n",
       "      <th>Test Return Mean std</th>\n",
       "      <th>Test Return Mean Trend mean</th>\n",
       "      <th>Weekly Minimum Return mean</th>\n",
       "      <th>Periods</th>\n",
       "      <th>Top Feature 1</th>\n",
       "      <th>Top Feature 2</th>\n",
       "      <th>Top Feature 3</th>\n",
       "      <th>Top Feature 4</th>\n",
       "      <th>Top Feature 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CART</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.631976</td>\n",
       "      <td>1.022727</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>2</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>0.349783</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.620416</td>\n",
       "      <td>0.109283</td>\n",
       "      <td>-0.007910</td>\n",
       "      <td>3.783183</td>\n",
       "      <td>3.378290</td>\n",
       "      <td>-0.121790</td>\n",
       "      <td>-0.806364</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>STOCH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CART</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.608907</td>\n",
       "      <td>1.282273</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>3</td>\n",
       "      <td>1.227273</td>\n",
       "      <td>0.373068</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.622587</td>\n",
       "      <td>0.092905</td>\n",
       "      <td>-0.005521</td>\n",
       "      <td>3.705000</td>\n",
       "      <td>2.724823</td>\n",
       "      <td>-0.108284</td>\n",
       "      <td>-0.530909</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CART</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.602030</td>\n",
       "      <td>0.320909</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>4</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>0.385031</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.595212</td>\n",
       "      <td>0.125675</td>\n",
       "      <td>-0.010707</td>\n",
       "      <td>1.966819</td>\n",
       "      <td>3.058830</td>\n",
       "      <td>-0.206782</td>\n",
       "      <td>-0.929545</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_^N225</td>\n",
       "      <td>STOCH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CART</td>\n",
       "      <td>CORR</td>\n",
       "      <td>5</td>\n",
       "      <td>0.549474</td>\n",
       "      <td>0.597727</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>3</td>\n",
       "      <td>1.590909</td>\n",
       "      <td>0.436962</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.562038</td>\n",
       "      <td>0.098837</td>\n",
       "      <td>-0.003754</td>\n",
       "      <td>2.414091</td>\n",
       "      <td>2.958483</td>\n",
       "      <td>0.015375</td>\n",
       "      <td>-0.964091</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CART</td>\n",
       "      <td>CORR</td>\n",
       "      <td>10</td>\n",
       "      <td>0.614191</td>\n",
       "      <td>0.512727</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>2</td>\n",
       "      <td>1.636364</td>\n",
       "      <td>0.381627</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.619080</td>\n",
       "      <td>0.102985</td>\n",
       "      <td>-0.003606</td>\n",
       "      <td>2.913183</td>\n",
       "      <td>2.856223</td>\n",
       "      <td>-0.026172</td>\n",
       "      <td>-0.825001</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>CART</td>\n",
       "      <td>CORR</td>\n",
       "      <td>15</td>\n",
       "      <td>0.562036</td>\n",
       "      <td>0.049545</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>4</td>\n",
       "      <td>1.590909</td>\n",
       "      <td>0.432296</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.569462</td>\n",
       "      <td>0.139839</td>\n",
       "      <td>-0.007654</td>\n",
       "      <td>1.886819</td>\n",
       "      <td>3.416658</td>\n",
       "      <td>-0.108554</td>\n",
       "      <td>-1.058637</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>CART</td>\n",
       "      <td>RF</td>\n",
       "      <td>5</td>\n",
       "      <td>0.579162</td>\n",
       "      <td>1.087271</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>3</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.408165</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.595541</td>\n",
       "      <td>0.094882</td>\n",
       "      <td>-0.004473</td>\n",
       "      <td>3.299544</td>\n",
       "      <td>2.894591</td>\n",
       "      <td>-0.049199</td>\n",
       "      <td>-0.784545</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>CCI_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>CART</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.568988</td>\n",
       "      <td>0.568637</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>5</td>\n",
       "      <td>1.681818</td>\n",
       "      <td>0.415383</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.580203</td>\n",
       "      <td>0.126445</td>\n",
       "      <td>-0.003550</td>\n",
       "      <td>2.488637</td>\n",
       "      <td>3.332970</td>\n",
       "      <td>-0.114789</td>\n",
       "      <td>-1.175909</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CART</td>\n",
       "      <td>RF</td>\n",
       "      <td>15</td>\n",
       "      <td>0.612755</td>\n",
       "      <td>0.824545</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.375904</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.603993</td>\n",
       "      <td>0.121130</td>\n",
       "      <td>-0.007179</td>\n",
       "      <td>2.883182</td>\n",
       "      <td>3.365338</td>\n",
       "      <td>-0.170869</td>\n",
       "      <td>-0.774546</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^KS11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CART</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.525231</td>\n",
       "      <td>-0.147728</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>6</td>\n",
       "      <td>2.136364</td>\n",
       "      <td>0.426082</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.545614</td>\n",
       "      <td>0.133173</td>\n",
       "      <td>-0.001877</td>\n",
       "      <td>1.671362</td>\n",
       "      <td>3.551844</td>\n",
       "      <td>0.031694</td>\n",
       "      <td>-1.501820</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_NVDA</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CART</td>\n",
       "      <td>XGB</td>\n",
       "      <td>10</td>\n",
       "      <td>0.573727</td>\n",
       "      <td>0.599545</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>7</td>\n",
       "      <td>1.818182</td>\n",
       "      <td>0.398543</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.589986</td>\n",
       "      <td>0.090459</td>\n",
       "      <td>-0.000171</td>\n",
       "      <td>2.071365</td>\n",
       "      <td>3.315604</td>\n",
       "      <td>0.058233</td>\n",
       "      <td>-1.246817</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_NVDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CART</td>\n",
       "      <td>XGB</td>\n",
       "      <td>15</td>\n",
       "      <td>0.569205</td>\n",
       "      <td>-0.217272</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>6</td>\n",
       "      <td>2.090909</td>\n",
       "      <td>0.418237</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.561759</td>\n",
       "      <td>0.117422</td>\n",
       "      <td>-0.005033</td>\n",
       "      <td>1.533184</td>\n",
       "      <td>3.070156</td>\n",
       "      <td>-0.055014</td>\n",
       "      <td>-1.286817</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_NVDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.671261</td>\n",
       "      <td>1.384999</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.295936</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.689126</td>\n",
       "      <td>0.111691</td>\n",
       "      <td>-0.007520</td>\n",
       "      <td>5.971363</td>\n",
       "      <td>2.567990</td>\n",
       "      <td>-0.030079</td>\n",
       "      <td>-0.083637</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>STOCH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.678136</td>\n",
       "      <td>1.384999</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.304586</td>\n",
       "      <td>0.476190</td>\n",
       "      <td>0.695632</td>\n",
       "      <td>0.099278</td>\n",
       "      <td>-0.005385</td>\n",
       "      <td>6.167727</td>\n",
       "      <td>2.438812</td>\n",
       "      <td>-0.003879</td>\n",
       "      <td>-0.132727</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.659340</td>\n",
       "      <td>1.300000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "      <td>0.772727</td>\n",
       "      <td>0.293601</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.660681</td>\n",
       "      <td>0.106648</td>\n",
       "      <td>-0.002376</td>\n",
       "      <td>5.673182</td>\n",
       "      <td>2.549364</td>\n",
       "      <td>0.112597</td>\n",
       "      <td>-0.228637</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_^N225</td>\n",
       "      <td>STOCH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CORR</td>\n",
       "      <td>5</td>\n",
       "      <td>0.618568</td>\n",
       "      <td>0.705908</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.322696</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.649047</td>\n",
       "      <td>0.088360</td>\n",
       "      <td>0.003891</td>\n",
       "      <td>4.488636</td>\n",
       "      <td>2.269571</td>\n",
       "      <td>0.181180</td>\n",
       "      <td>-0.448182</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CORR</td>\n",
       "      <td>10</td>\n",
       "      <td>0.681412</td>\n",
       "      <td>1.182271</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.278681</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.683828</td>\n",
       "      <td>0.082371</td>\n",
       "      <td>-0.002502</td>\n",
       "      <td>6.100453</td>\n",
       "      <td>2.124546</td>\n",
       "      <td>0.095364</td>\n",
       "      <td>-0.284092</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>LDA</td>\n",
       "      <td>CORR</td>\n",
       "      <td>15</td>\n",
       "      <td>0.669290</td>\n",
       "      <td>1.232272</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.681818</td>\n",
       "      <td>0.312617</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.673558</td>\n",
       "      <td>0.084872</td>\n",
       "      <td>-0.003817</td>\n",
       "      <td>5.960454</td>\n",
       "      <td>2.152256</td>\n",
       "      <td>0.031558</td>\n",
       "      <td>-0.227273</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>LDA</td>\n",
       "      <td>RF</td>\n",
       "      <td>5</td>\n",
       "      <td>0.673196</td>\n",
       "      <td>1.342272</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.294554</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>0.684191</td>\n",
       "      <td>0.098105</td>\n",
       "      <td>-0.001979</td>\n",
       "      <td>5.633179</td>\n",
       "      <td>2.239542</td>\n",
       "      <td>0.072123</td>\n",
       "      <td>-0.284546</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>CCI_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>LDA</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.648657</td>\n",
       "      <td>1.001363</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.317709</td>\n",
       "      <td>0.526316</td>\n",
       "      <td>0.659604</td>\n",
       "      <td>0.081598</td>\n",
       "      <td>-0.000666</td>\n",
       "      <td>5.326817</td>\n",
       "      <td>2.222570</td>\n",
       "      <td>0.087335</td>\n",
       "      <td>-0.308182</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>LDA</td>\n",
       "      <td>RF</td>\n",
       "      <td>15</td>\n",
       "      <td>0.649356</td>\n",
       "      <td>1.003181</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.318544</td>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.659146</td>\n",
       "      <td>0.076377</td>\n",
       "      <td>-0.002021</td>\n",
       "      <td>5.455909</td>\n",
       "      <td>2.165369</td>\n",
       "      <td>0.046025</td>\n",
       "      <td>-0.322273</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^KS11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>LDA</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.597615</td>\n",
       "      <td>0.474091</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>5</td>\n",
       "      <td>1.590909</td>\n",
       "      <td>0.322120</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.596076</td>\n",
       "      <td>0.118417</td>\n",
       "      <td>0.006600</td>\n",
       "      <td>3.019545</td>\n",
       "      <td>3.679115</td>\n",
       "      <td>0.236607</td>\n",
       "      <td>-0.784091</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_NVDA</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>LDA</td>\n",
       "      <td>XGB</td>\n",
       "      <td>10</td>\n",
       "      <td>0.635855</td>\n",
       "      <td>0.861363</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0</td>\n",
       "      <td>1.227273</td>\n",
       "      <td>0.274720</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.632674</td>\n",
       "      <td>0.083643</td>\n",
       "      <td>0.006971</td>\n",
       "      <td>4.404999</td>\n",
       "      <td>2.398242</td>\n",
       "      <td>0.167899</td>\n",
       "      <td>-0.465000</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_NVDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>LDA</td>\n",
       "      <td>XGB</td>\n",
       "      <td>15</td>\n",
       "      <td>0.617481</td>\n",
       "      <td>1.040455</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>2</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>0.290083</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.627939</td>\n",
       "      <td>0.108965</td>\n",
       "      <td>-0.000751</td>\n",
       "      <td>4.108636</td>\n",
       "      <td>2.721430</td>\n",
       "      <td>0.071095</td>\n",
       "      <td>-0.525454</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_NVDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>RF</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.684204</td>\n",
       "      <td>1.493181</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>1</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.288413</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.674292</td>\n",
       "      <td>0.116601</td>\n",
       "      <td>-0.008460</td>\n",
       "      <td>5.180455</td>\n",
       "      <td>2.546360</td>\n",
       "      <td>-0.104545</td>\n",
       "      <td>-0.600910</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>STOCH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>RF</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.661637</td>\n",
       "      <td>1.030909</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.289065</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.669177</td>\n",
       "      <td>0.107918</td>\n",
       "      <td>-0.005261</td>\n",
       "      <td>5.289545</td>\n",
       "      <td>3.019699</td>\n",
       "      <td>-0.032541</td>\n",
       "      <td>-0.442273</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>RF</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.662448</td>\n",
       "      <td>0.983636</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>0.304934</td>\n",
       "      <td>0.315789</td>\n",
       "      <td>0.655932</td>\n",
       "      <td>0.104010</td>\n",
       "      <td>-0.007361</td>\n",
       "      <td>4.896818</td>\n",
       "      <td>2.806543</td>\n",
       "      <td>-0.125799</td>\n",
       "      <td>-0.455000</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_^N225</td>\n",
       "      <td>STOCH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>RF</td>\n",
       "      <td>CORR</td>\n",
       "      <td>5</td>\n",
       "      <td>0.629125</td>\n",
       "      <td>0.893181</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>2</td>\n",
       "      <td>1.227273</td>\n",
       "      <td>0.321030</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.635709</td>\n",
       "      <td>0.097046</td>\n",
       "      <td>-0.000105</td>\n",
       "      <td>4.240455</td>\n",
       "      <td>3.358329</td>\n",
       "      <td>0.135274</td>\n",
       "      <td>-0.668183</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>RF</td>\n",
       "      <td>CORR</td>\n",
       "      <td>10</td>\n",
       "      <td>0.667524</td>\n",
       "      <td>1.278636</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.312112</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.666818</td>\n",
       "      <td>0.120691</td>\n",
       "      <td>-0.009692</td>\n",
       "      <td>4.986818</td>\n",
       "      <td>2.542632</td>\n",
       "      <td>-0.138910</td>\n",
       "      <td>-0.488637</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>RF</td>\n",
       "      <td>CORR</td>\n",
       "      <td>15</td>\n",
       "      <td>0.661659</td>\n",
       "      <td>1.366363</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.320036</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.667186</td>\n",
       "      <td>0.109522</td>\n",
       "      <td>-0.007989</td>\n",
       "      <td>5.163181</td>\n",
       "      <td>2.277552</td>\n",
       "      <td>-0.031558</td>\n",
       "      <td>-0.263182</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>5</td>\n",
       "      <td>0.655488</td>\n",
       "      <td>1.439545</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>2</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>0.302531</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.662769</td>\n",
       "      <td>0.124594</td>\n",
       "      <td>-0.010234</td>\n",
       "      <td>5.248636</td>\n",
       "      <td>3.197118</td>\n",
       "      <td>-0.156211</td>\n",
       "      <td>-0.307273</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>CCI_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.620367</td>\n",
       "      <td>0.985909</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.272727</td>\n",
       "      <td>0.342199</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.621220</td>\n",
       "      <td>0.101931</td>\n",
       "      <td>-0.008169</td>\n",
       "      <td>4.266817</td>\n",
       "      <td>2.986193</td>\n",
       "      <td>-0.062479</td>\n",
       "      <td>-0.688636</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>RF</td>\n",
       "      <td>RF</td>\n",
       "      <td>15</td>\n",
       "      <td>0.659008</td>\n",
       "      <td>0.967272</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>0.954545</td>\n",
       "      <td>0.313829</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.658887</td>\n",
       "      <td>0.097679</td>\n",
       "      <td>-0.005730</td>\n",
       "      <td>5.162271</td>\n",
       "      <td>2.677008</td>\n",
       "      <td>-0.033603</td>\n",
       "      <td>-0.469091</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^KS11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>RF</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.554426</td>\n",
       "      <td>0.937727</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>4</td>\n",
       "      <td>1.727273</td>\n",
       "      <td>0.361719</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.587233</td>\n",
       "      <td>0.113759</td>\n",
       "      <td>0.002416</td>\n",
       "      <td>3.263182</td>\n",
       "      <td>3.823460</td>\n",
       "      <td>0.135059</td>\n",
       "      <td>-0.933182</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_NVDA</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>RF</td>\n",
       "      <td>XGB</td>\n",
       "      <td>10</td>\n",
       "      <td>0.602919</td>\n",
       "      <td>1.029545</td>\n",
       "      <td>0.590909</td>\n",
       "      <td>3</td>\n",
       "      <td>1.409091</td>\n",
       "      <td>0.336699</td>\n",
       "      <td>0.450000</td>\n",
       "      <td>0.615946</td>\n",
       "      <td>0.086968</td>\n",
       "      <td>-0.004359</td>\n",
       "      <td>3.712272</td>\n",
       "      <td>3.277887</td>\n",
       "      <td>0.031005</td>\n",
       "      <td>-0.594091</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_NVDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>RF</td>\n",
       "      <td>XGB</td>\n",
       "      <td>15</td>\n",
       "      <td>0.584419</td>\n",
       "      <td>1.053636</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>2</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.369599</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.612168</td>\n",
       "      <td>0.112161</td>\n",
       "      <td>-0.005495</td>\n",
       "      <td>4.154091</td>\n",
       "      <td>3.386510</td>\n",
       "      <td>-0.036663</td>\n",
       "      <td>-0.499546</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_NVDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>5</td>\n",
       "      <td>0.647285</td>\n",
       "      <td>1.344545</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.330660</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.639500</td>\n",
       "      <td>0.078494</td>\n",
       "      <td>-0.005636</td>\n",
       "      <td>4.835001</td>\n",
       "      <td>2.048557</td>\n",
       "      <td>0.040096</td>\n",
       "      <td>-0.502728</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>STOCH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>10</td>\n",
       "      <td>0.639137</td>\n",
       "      <td>1.226363</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>1</td>\n",
       "      <td>1.045455</td>\n",
       "      <td>0.321002</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.648558</td>\n",
       "      <td>0.121441</td>\n",
       "      <td>-0.007923</td>\n",
       "      <td>4.617727</td>\n",
       "      <td>2.903388</td>\n",
       "      <td>-0.075036</td>\n",
       "      <td>-0.614091</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CHI2</td>\n",
       "      <td>15</td>\n",
       "      <td>0.667569</td>\n",
       "      <td>1.062727</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1.318182</td>\n",
       "      <td>0.308941</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.633259</td>\n",
       "      <td>0.143421</td>\n",
       "      <td>-0.009547</td>\n",
       "      <td>4.187729</td>\n",
       "      <td>3.130951</td>\n",
       "      <td>-0.174969</td>\n",
       "      <td>-0.839091</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>Return_^N225</td>\n",
       "      <td>STOCH_D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CORR</td>\n",
       "      <td>5</td>\n",
       "      <td>0.590400</td>\n",
       "      <td>0.331818</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>8</td>\n",
       "      <td>1.545455</td>\n",
       "      <td>0.376945</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.580298</td>\n",
       "      <td>0.097816</td>\n",
       "      <td>-0.001222</td>\n",
       "      <td>2.385000</td>\n",
       "      <td>3.342250</td>\n",
       "      <td>-0.001858</td>\n",
       "      <td>-1.241819</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CORR</td>\n",
       "      <td>10</td>\n",
       "      <td>0.637933</td>\n",
       "      <td>0.990454</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>0.351212</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.640542</td>\n",
       "      <td>0.113805</td>\n",
       "      <td>-0.008084</td>\n",
       "      <td>4.361366</td>\n",
       "      <td>2.689882</td>\n",
       "      <td>-0.080819</td>\n",
       "      <td>-0.685454</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>XGB</td>\n",
       "      <td>CORR</td>\n",
       "      <td>15</td>\n",
       "      <td>0.638203</td>\n",
       "      <td>1.048181</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>1</td>\n",
       "      <td>1.136364</td>\n",
       "      <td>0.335754</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.639951</td>\n",
       "      <td>0.131870</td>\n",
       "      <td>-0.009744</td>\n",
       "      <td>4.709545</td>\n",
       "      <td>3.167356</td>\n",
       "      <td>-0.101474</td>\n",
       "      <td>-0.582273</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>CCI_14</td>\n",
       "      <td>CCI_20</td>\n",
       "      <td>WilliamsR</td>\n",
       "      <td>RSI_14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>XGB</td>\n",
       "      <td>RF</td>\n",
       "      <td>5</td>\n",
       "      <td>0.645838</td>\n",
       "      <td>1.449544</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>1</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>0.338131</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.641534</td>\n",
       "      <td>0.093728</td>\n",
       "      <td>-0.002863</td>\n",
       "      <td>5.094999</td>\n",
       "      <td>3.395632</td>\n",
       "      <td>0.018865</td>\n",
       "      <td>-0.625455</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>CCI_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>XGB</td>\n",
       "      <td>RF</td>\n",
       "      <td>10</td>\n",
       "      <td>0.617270</td>\n",
       "      <td>1.064090</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>2</td>\n",
       "      <td>1.409091</td>\n",
       "      <td>0.353355</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.614092</td>\n",
       "      <td>0.111467</td>\n",
       "      <td>-0.010194</td>\n",
       "      <td>3.448636</td>\n",
       "      <td>2.824410</td>\n",
       "      <td>-0.168972</td>\n",
       "      <td>-1.005455</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^N225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>XGB</td>\n",
       "      <td>RF</td>\n",
       "      <td>15</td>\n",
       "      <td>0.638996</td>\n",
       "      <td>1.395909</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>1</td>\n",
       "      <td>1.090909</td>\n",
       "      <td>0.338113</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.635637</td>\n",
       "      <td>0.091152</td>\n",
       "      <td>-0.003971</td>\n",
       "      <td>4.584090</td>\n",
       "      <td>2.402018</td>\n",
       "      <td>0.049266</td>\n",
       "      <td>-0.542727</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^KS11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>XGB</td>\n",
       "      <td>XGB</td>\n",
       "      <td>5</td>\n",
       "      <td>0.527986</td>\n",
       "      <td>-0.128182</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>7</td>\n",
       "      <td>1.954545</td>\n",
       "      <td>0.422429</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.545380</td>\n",
       "      <td>0.122642</td>\n",
       "      <td>0.004858</td>\n",
       "      <td>1.963182</td>\n",
       "      <td>3.708327</td>\n",
       "      <td>0.229944</td>\n",
       "      <td>-1.177274</td>\n",
       "      <td>22</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_NVDA</td>\n",
       "      <td>AMD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>XGB</td>\n",
       "      <td>XGB</td>\n",
       "      <td>10</td>\n",
       "      <td>0.589231</td>\n",
       "      <td>0.658636</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>3</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>0.383365</td>\n",
       "      <td>0.260870</td>\n",
       "      <td>0.596375</td>\n",
       "      <td>0.122464</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>3.549547</td>\n",
       "      <td>3.715034</td>\n",
       "      <td>0.199012</td>\n",
       "      <td>-0.628182</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_NVDA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>XGB</td>\n",
       "      <td>XGB</td>\n",
       "      <td>15</td>\n",
       "      <td>0.629895</td>\n",
       "      <td>0.880908</td>\n",
       "      <td>0.545455</td>\n",
       "      <td>6</td>\n",
       "      <td>1.454545</td>\n",
       "      <td>0.315412</td>\n",
       "      <td>0.409091</td>\n",
       "      <td>0.630503</td>\n",
       "      <td>0.121831</td>\n",
       "      <td>-0.005948</td>\n",
       "      <td>4.003181</td>\n",
       "      <td>4.207464</td>\n",
       "      <td>-0.005855</td>\n",
       "      <td>-0.759546</td>\n",
       "      <td>22</td>\n",
       "      <td>STOCH_K</td>\n",
       "      <td>Return_^AORD</td>\n",
       "      <td>Return_^AXJO</td>\n",
       "      <td>Return_^TWII</td>\n",
       "      <td>Return_NVDA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Model Feature Selection Method  VarNum  Long F1 mean  \\\n",
       "0   CART                     CHI2       5      0.631976   \n",
       "1   CART                     CHI2      10      0.608907   \n",
       "2   CART                     CHI2      15      0.602030   \n",
       "3   CART                     CORR       5      0.549474   \n",
       "4   CART                     CORR      10      0.614191   \n",
       "5   CART                     CORR      15      0.562036   \n",
       "6   CART                       RF       5      0.579162   \n",
       "7   CART                       RF      10      0.568988   \n",
       "8   CART                       RF      15      0.612755   \n",
       "9   CART                      XGB       5      0.525231   \n",
       "10  CART                      XGB      10      0.573727   \n",
       "11  CART                      XGB      15      0.569205   \n",
       "12   LDA                     CHI2       5      0.671261   \n",
       "13   LDA                     CHI2      10      0.678136   \n",
       "14   LDA                     CHI2      15      0.659340   \n",
       "15   LDA                     CORR       5      0.618568   \n",
       "16   LDA                     CORR      10      0.681412   \n",
       "17   LDA                     CORR      15      0.669290   \n",
       "18   LDA                       RF       5      0.673196   \n",
       "19   LDA                       RF      10      0.648657   \n",
       "20   LDA                       RF      15      0.649356   \n",
       "21   LDA                      XGB       5      0.597615   \n",
       "22   LDA                      XGB      10      0.635855   \n",
       "23   LDA                      XGB      15      0.617481   \n",
       "24    RF                     CHI2       5      0.684204   \n",
       "25    RF                     CHI2      10      0.661637   \n",
       "26    RF                     CHI2      15      0.662448   \n",
       "27    RF                     CORR       5      0.629125   \n",
       "28    RF                     CORR      10      0.667524   \n",
       "29    RF                     CORR      15      0.661659   \n",
       "30    RF                       RF       5      0.655488   \n",
       "31    RF                       RF      10      0.620367   \n",
       "32    RF                       RF      15      0.659008   \n",
       "33    RF                      XGB       5      0.554426   \n",
       "34    RF                      XGB      10      0.602919   \n",
       "35    RF                      XGB      15      0.584419   \n",
       "36   XGB                     CHI2       5      0.647285   \n",
       "37   XGB                     CHI2      10      0.639137   \n",
       "38   XGB                     CHI2      15      0.667569   \n",
       "39   XGB                     CORR       5      0.590400   \n",
       "40   XGB                     CORR      10      0.637933   \n",
       "41   XGB                     CORR      15      0.638203   \n",
       "42   XGB                       RF       5      0.645838   \n",
       "43   XGB                       RF      10      0.617270   \n",
       "44   XGB                       RF      15      0.638996   \n",
       "45   XGB                      XGB       5      0.527986   \n",
       "46   XGB                      XGB      10      0.589231   \n",
       "47   XGB                      XGB      15      0.629895   \n",
       "\n",
       "    Monthly Min Return mean  Negative Month Number mean  \\\n",
       "0                  1.022727                    0.363636   \n",
       "1                  1.282273                    0.409091   \n",
       "2                  0.320909                    0.545455   \n",
       "3                  0.597727                    0.500000   \n",
       "4                  0.512727                    0.409091   \n",
       "5                  0.049545                    0.500000   \n",
       "6                  1.087271                    0.454545   \n",
       "7                  0.568637                    0.590909   \n",
       "8                  0.824545                    0.318182   \n",
       "9                 -0.147728                    0.727273   \n",
       "10                 0.599545                    0.681818   \n",
       "11                -0.217272                    0.727273   \n",
       "12                 1.384999                    0.272727   \n",
       "13                 1.384999                    0.272727   \n",
       "14                 1.300000                    0.363636   \n",
       "15                 0.705908                    0.363636   \n",
       "16                 1.182271                    0.272727   \n",
       "17                 1.232272                    0.272727   \n",
       "18                 1.342272                    0.363636   \n",
       "19                 1.001363                    0.409091   \n",
       "20                 1.003181                    0.409091   \n",
       "21                 0.474091                    0.545455   \n",
       "22                 0.861363                    0.363636   \n",
       "23                 1.040455                    0.409091   \n",
       "24                 1.493181                    0.318182   \n",
       "25                 1.030909                    0.363636   \n",
       "26                 0.983636                    0.409091   \n",
       "27                 0.893181                    0.363636   \n",
       "28                 1.278636                    0.363636   \n",
       "29                 1.366363                    0.272727   \n",
       "30                 1.439545                    0.454545   \n",
       "31                 0.985909                    0.500000   \n",
       "32                 0.967272                    0.363636   \n",
       "33                 0.937727                    0.636364   \n",
       "34                 1.029545                    0.590909   \n",
       "35                 1.053636                    0.454545   \n",
       "36                 1.344545                    0.272727   \n",
       "37                 1.226363                    0.318182   \n",
       "38                 1.062727                    0.363636   \n",
       "39                 0.331818                    0.727273   \n",
       "40                 0.990454                    0.363636   \n",
       "41                 1.048181                    0.272727   \n",
       "42                 1.449544                    0.363636   \n",
       "43                 1.064090                    0.545455   \n",
       "44                 1.395909                    0.409091   \n",
       "45                -0.128182                    0.727273   \n",
       "46                 0.658636                    0.636364   \n",
       "47                 0.880908                    0.545455   \n",
       "\n",
       "    Negative Test Return sum  Negative Week Number mean  Short F1 mean  \\\n",
       "0                          2                   1.318182       0.349783   \n",
       "1                          3                   1.227273       0.373068   \n",
       "2                          4                   1.454545       0.385031   \n",
       "3                          3                   1.590909       0.436962   \n",
       "4                          2                   1.636364       0.381627   \n",
       "5                          4                   1.590909       0.432296   \n",
       "6                          3                   1.500000       0.408165   \n",
       "7                          5                   1.681818       0.415383   \n",
       "8                          2                   1.500000       0.375904   \n",
       "9                          6                   2.136364       0.426082   \n",
       "10                         7                   1.818182       0.398543   \n",
       "11                         6                   2.090909       0.418237   \n",
       "12                         0                   0.681818       0.295936   \n",
       "13                         0                   0.500000       0.304586   \n",
       "14                         0                   0.772727       0.293601   \n",
       "15                         1                   0.909091       0.322696   \n",
       "16                         0                   0.681818       0.278681   \n",
       "17                         0                   0.681818       0.312617   \n",
       "18                         0                   0.909091       0.294554   \n",
       "19                         1                   1.000000       0.317709   \n",
       "20                         0                   0.909091       0.318544   \n",
       "21                         5                   1.590909       0.322120   \n",
       "22                         0                   1.227273       0.274720   \n",
       "23                         2                   1.318182       0.290083   \n",
       "24                         1                   0.909091       0.288413   \n",
       "25                         1                   1.000000       0.289065   \n",
       "26                         1                   1.136364       0.304934   \n",
       "27                         2                   1.227273       0.321030   \n",
       "28                         1                   1.045455       0.312112   \n",
       "29                         0                   0.954545       0.320036   \n",
       "30                         2                   1.136364       0.302531   \n",
       "31                         1                   1.272727       0.342199   \n",
       "32                         1                   0.954545       0.313829   \n",
       "33                         4                   1.727273       0.361719   \n",
       "34                         3                   1.409091       0.336699   \n",
       "35                         2                   1.500000       0.369599   \n",
       "36                         0                   1.045455       0.330660   \n",
       "37                         1                   1.045455       0.321002   \n",
       "38                         1                   1.318182       0.308941   \n",
       "39                         8                   1.545455       0.376945   \n",
       "40                         1                   1.090909       0.351212   \n",
       "41                         1                   1.136364       0.335754   \n",
       "42                         1                   1.090909       0.338131   \n",
       "43                         2                   1.409091       0.353355   \n",
       "44                         1                   1.090909       0.338113   \n",
       "45                         7                   1.954545       0.422429   \n",
       "46                         3                   1.500000       0.383365   \n",
       "47                         6                   1.454545       0.315412   \n",
       "\n",
       "    Test Accuracy amin  Test Accuracy mean  Test Accuracy std  \\\n",
       "0             0.454545            0.620416           0.109283   \n",
       "1             0.454545            0.622587           0.092905   \n",
       "2             0.350000            0.595212           0.125675   \n",
       "3             0.380952            0.562038           0.098837   \n",
       "4             0.400000            0.619080           0.102985   \n",
       "5             0.263158            0.569462           0.139839   \n",
       "6             0.450000            0.595541           0.094882   \n",
       "7             0.263158            0.580203           0.126445   \n",
       "8             0.350000            0.603993           0.121130   \n",
       "9             0.272727            0.545614           0.133173   \n",
       "10            0.409091            0.589986           0.090459   \n",
       "11            0.363636            0.561759           0.117422   \n",
       "12            0.523810            0.689126           0.111691   \n",
       "13            0.476190            0.695632           0.099278   \n",
       "14            0.421053            0.660681           0.106648   \n",
       "15            0.473684            0.649047           0.088360   \n",
       "16            0.523810            0.683828           0.082371   \n",
       "17            0.500000            0.673558           0.084872   \n",
       "18            0.545455            0.684191           0.098105   \n",
       "19            0.526316            0.659604           0.081598   \n",
       "20            0.523810            0.659146           0.076377   \n",
       "21            0.363636            0.596076           0.118417   \n",
       "22            0.454545            0.632674           0.083643   \n",
       "23            0.363636            0.627939           0.108965   \n",
       "24            0.409091            0.674292           0.116601   \n",
       "25            0.421053            0.669177           0.107918   \n",
       "26            0.315789            0.655932           0.104010   \n",
       "27            0.454545            0.635709           0.097046   \n",
       "28            0.409091            0.666818           0.120691   \n",
       "29            0.473684            0.667186           0.109522   \n",
       "30            0.368421            0.662769           0.124594   \n",
       "31            0.368421            0.621220           0.101931   \n",
       "32            0.473684            0.658887           0.097679   \n",
       "33            0.380952            0.587233           0.113759   \n",
       "34            0.450000            0.615946           0.086968   \n",
       "35            0.409091            0.612168           0.112161   \n",
       "36            0.454545            0.639500           0.078494   \n",
       "37            0.421053            0.648558           0.121441   \n",
       "38            0.263158            0.633259           0.143421   \n",
       "39            0.380952            0.580298           0.097816   \n",
       "40            0.368421            0.640542           0.113805   \n",
       "41            0.263158            0.639951           0.131870   \n",
       "42            0.454545            0.641534           0.093728   \n",
       "43            0.421053            0.614092           0.111467   \n",
       "44            0.428571            0.635637           0.091152   \n",
       "45            0.368421            0.545380           0.122642   \n",
       "46            0.260870            0.596375           0.122464   \n",
       "47            0.409091            0.630503           0.121831   \n",
       "\n",
       "    Test Accuracy Trend mean  Test Return Mean mean  Test Return Mean std  \\\n",
       "0                  -0.007910               3.783183              3.378290   \n",
       "1                  -0.005521               3.705000              2.724823   \n",
       "2                  -0.010707               1.966819              3.058830   \n",
       "3                  -0.003754               2.414091              2.958483   \n",
       "4                  -0.003606               2.913183              2.856223   \n",
       "5                  -0.007654               1.886819              3.416658   \n",
       "6                  -0.004473               3.299544              2.894591   \n",
       "7                  -0.003550               2.488637              3.332970   \n",
       "8                  -0.007179               2.883182              3.365338   \n",
       "9                  -0.001877               1.671362              3.551844   \n",
       "10                 -0.000171               2.071365              3.315604   \n",
       "11                 -0.005033               1.533184              3.070156   \n",
       "12                 -0.007520               5.971363              2.567990   \n",
       "13                 -0.005385               6.167727              2.438812   \n",
       "14                 -0.002376               5.673182              2.549364   \n",
       "15                  0.003891               4.488636              2.269571   \n",
       "16                 -0.002502               6.100453              2.124546   \n",
       "17                 -0.003817               5.960454              2.152256   \n",
       "18                 -0.001979               5.633179              2.239542   \n",
       "19                 -0.000666               5.326817              2.222570   \n",
       "20                 -0.002021               5.455909              2.165369   \n",
       "21                  0.006600               3.019545              3.679115   \n",
       "22                  0.006971               4.404999              2.398242   \n",
       "23                 -0.000751               4.108636              2.721430   \n",
       "24                 -0.008460               5.180455              2.546360   \n",
       "25                 -0.005261               5.289545              3.019699   \n",
       "26                 -0.007361               4.896818              2.806543   \n",
       "27                 -0.000105               4.240455              3.358329   \n",
       "28                 -0.009692               4.986818              2.542632   \n",
       "29                 -0.007989               5.163181              2.277552   \n",
       "30                 -0.010234               5.248636              3.197118   \n",
       "31                 -0.008169               4.266817              2.986193   \n",
       "32                 -0.005730               5.162271              2.677008   \n",
       "33                  0.002416               3.263182              3.823460   \n",
       "34                 -0.004359               3.712272              3.277887   \n",
       "35                 -0.005495               4.154091              3.386510   \n",
       "36                 -0.005636               4.835001              2.048557   \n",
       "37                 -0.007923               4.617727              2.903388   \n",
       "38                 -0.009547               4.187729              3.130951   \n",
       "39                 -0.001222               2.385000              3.342250   \n",
       "40                 -0.008084               4.361366              2.689882   \n",
       "41                 -0.009744               4.709545              3.167356   \n",
       "42                 -0.002863               5.094999              3.395632   \n",
       "43                 -0.010194               3.448636              2.824410   \n",
       "44                 -0.003971               4.584090              2.402018   \n",
       "45                  0.004858               1.963182              3.708327   \n",
       "46                  0.002825               3.549547              3.715034   \n",
       "47                 -0.005948               4.003181              4.207464   \n",
       "\n",
       "    Test Return Mean Trend mean  Weekly Minimum Return mean  Periods  \\\n",
       "0                     -0.121790                   -0.806364       22   \n",
       "1                     -0.108284                   -0.530909       22   \n",
       "2                     -0.206782                   -0.929545       22   \n",
       "3                      0.015375                   -0.964091       22   \n",
       "4                     -0.026172                   -0.825001       22   \n",
       "5                     -0.108554                   -1.058637       22   \n",
       "6                     -0.049199                   -0.784545       22   \n",
       "7                     -0.114789                   -1.175909       22   \n",
       "8                     -0.170869                   -0.774546       22   \n",
       "9                      0.031694                   -1.501820       22   \n",
       "10                     0.058233                   -1.246817       22   \n",
       "11                    -0.055014                   -1.286817       22   \n",
       "12                    -0.030079                   -0.083637       22   \n",
       "13                    -0.003879                   -0.132727       22   \n",
       "14                     0.112597                   -0.228637       22   \n",
       "15                     0.181180                   -0.448182       22   \n",
       "16                     0.095364                   -0.284092       22   \n",
       "17                     0.031558                   -0.227273       22   \n",
       "18                     0.072123                   -0.284546       22   \n",
       "19                     0.087335                   -0.308182       22   \n",
       "20                     0.046025                   -0.322273       22   \n",
       "21                     0.236607                   -0.784091       22   \n",
       "22                     0.167899                   -0.465000       22   \n",
       "23                     0.071095                   -0.525454       22   \n",
       "24                    -0.104545                   -0.600910       22   \n",
       "25                    -0.032541                   -0.442273       22   \n",
       "26                    -0.125799                   -0.455000       22   \n",
       "27                     0.135274                   -0.668183       22   \n",
       "28                    -0.138910                   -0.488637       22   \n",
       "29                    -0.031558                   -0.263182       22   \n",
       "30                    -0.156211                   -0.307273       22   \n",
       "31                    -0.062479                   -0.688636       22   \n",
       "32                    -0.033603                   -0.469091       22   \n",
       "33                     0.135059                   -0.933182       22   \n",
       "34                     0.031005                   -0.594091       22   \n",
       "35                    -0.036663                   -0.499546       22   \n",
       "36                     0.040096                   -0.502728       22   \n",
       "37                    -0.075036                   -0.614091       22   \n",
       "38                    -0.174969                   -0.839091       22   \n",
       "39                    -0.001858                   -1.241819       22   \n",
       "40                    -0.080819                   -0.685454       22   \n",
       "41                    -0.101474                   -0.582273       22   \n",
       "42                     0.018865                   -0.625455       22   \n",
       "43                    -0.168972                   -1.005455       22   \n",
       "44                     0.049266                   -0.542727       22   \n",
       "45                     0.229944                   -1.177274       22   \n",
       "46                     0.199012                   -0.628182       22   \n",
       "47                    -0.005855                   -0.759546       22   \n",
       "\n",
       "   Top Feature 1 Top Feature 2 Top Feature 3 Top Feature 4 Top Feature 5  \n",
       "0        STOCH_K     WilliamsR        CCI_20        CCI_14       STOCH_D  \n",
       "1        STOCH_K        CCI_20     WilliamsR        CCI_14  Return_^N225  \n",
       "2        STOCH_K        CCI_20     WilliamsR  Return_^N225       STOCH_D  \n",
       "3        STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "4        STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "5        STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "6        STOCH_K  Return_^AXJO  Return_^AORD  Return_^TWII        CCI_20  \n",
       "7   Return_^TWII  Return_^AORD       STOCH_K  Return_^AXJO  Return_^N225  \n",
       "8   Return_^TWII  Return_^AORD       STOCH_K  Return_^AXJO  Return_^KS11  \n",
       "9   Return_^AXJO  Return_^AORD       STOCH_K   Return_NVDA           AMD  \n",
       "10       STOCH_K  Return_^AORD  Return_^AXJO  Return_^TWII   Return_NVDA  \n",
       "11       STOCH_K  Return_^AORD  Return_^AXJO  Return_^TWII   Return_NVDA  \n",
       "12       STOCH_K     WilliamsR        CCI_20        CCI_14       STOCH_D  \n",
       "13       STOCH_K        CCI_20     WilliamsR        CCI_14  Return_^N225  \n",
       "14       STOCH_K        CCI_20     WilliamsR  Return_^N225       STOCH_D  \n",
       "15       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "16       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "17       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "18       STOCH_K  Return_^AXJO  Return_^AORD  Return_^TWII        CCI_20  \n",
       "19  Return_^TWII  Return_^AORD       STOCH_K  Return_^AXJO  Return_^N225  \n",
       "20  Return_^TWII  Return_^AORD       STOCH_K  Return_^AXJO  Return_^KS11  \n",
       "21  Return_^AXJO  Return_^AORD       STOCH_K   Return_NVDA           AMD  \n",
       "22       STOCH_K  Return_^AORD  Return_^AXJO  Return_^TWII   Return_NVDA  \n",
       "23       STOCH_K  Return_^AORD  Return_^AXJO  Return_^TWII   Return_NVDA  \n",
       "24       STOCH_K     WilliamsR        CCI_20        CCI_14       STOCH_D  \n",
       "25       STOCH_K        CCI_20     WilliamsR        CCI_14  Return_^N225  \n",
       "26       STOCH_K        CCI_20     WilliamsR  Return_^N225       STOCH_D  \n",
       "27       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "28       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "29       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "30       STOCH_K  Return_^AXJO  Return_^AORD  Return_^TWII        CCI_20  \n",
       "31  Return_^TWII  Return_^AORD       STOCH_K  Return_^AXJO  Return_^N225  \n",
       "32  Return_^TWII  Return_^AORD       STOCH_K  Return_^AXJO  Return_^KS11  \n",
       "33  Return_^AXJO  Return_^AORD       STOCH_K   Return_NVDA           AMD  \n",
       "34       STOCH_K  Return_^AORD  Return_^AXJO  Return_^TWII   Return_NVDA  \n",
       "35       STOCH_K  Return_^AORD  Return_^AXJO  Return_^TWII   Return_NVDA  \n",
       "36       STOCH_K     WilliamsR        CCI_20        CCI_14       STOCH_D  \n",
       "37       STOCH_K        CCI_20     WilliamsR        CCI_14  Return_^N225  \n",
       "38       STOCH_K        CCI_20     WilliamsR  Return_^N225       STOCH_D  \n",
       "39       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "40       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "41       STOCH_K        CCI_14        CCI_20     WilliamsR        RSI_14  \n",
       "42       STOCH_K  Return_^AXJO  Return_^AORD  Return_^TWII        CCI_20  \n",
       "43  Return_^TWII  Return_^AORD       STOCH_K  Return_^AXJO  Return_^N225  \n",
       "44  Return_^TWII  Return_^AORD       STOCH_K  Return_^AXJO  Return_^KS11  \n",
       "45  Return_^AXJO  Return_^AORD       STOCH_K   Return_NVDA           AMD  \n",
       "46       STOCH_K  Return_^AORD  Return_^AXJO  Return_^TWII   Return_NVDA  \n",
       "47       STOCH_K  Return_^AORD  Return_^AXJO  Return_^TWII   Return_NVDA  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot_table_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yfinance: 0.2.37\n",
    "talib: 0.4.24\n",
    "numpy: 1.24.4\n",
    "pandas: 2.0.3\n",
    "sklearn: 0.24.0\n",
    "xgboost: 1.7.5\n",
    "dateutil: 2.8.1\n",
    "scipy: 1.10.1"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
